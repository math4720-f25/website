[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 4720 (MSSC 5720) Statistical Methods Fall 2025",
    "section": "",
    "text": "The website is for both Section 101 2 - 3:15 PM and Section 102 3:30-4:45 PM.\n\nThis schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nSlides\nReading\nCode\nHomework\nQuiz\nProject\n\n\n\n1\nMon, Aug 25\nSyllabus/Overview of Statistics\n\nüñ•Ô∏è syllabusüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, Aug 27\nStatistical Studies and Data Collection; Data Type\nüñ•Ô∏è\n\n\n\n\n\n\n\n2\nMon, Sep 1\nNO CLASS: Labor Day\nüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, Sep 3\nR Software and Computation\n\n\n\n\n\n\n\n\n3\nMon, Sep 8\nSummarizing Data: Centrality\n\n\n\n‚úçÔ∏è HW1\n\n\n\n\n\nWed, Sep 10\nSummarizing Data: Variation\nüñ•Ô∏è\n\n\n\n\n\n\n\n4\nMon, Sep 15\nProbability Fundamentals\nüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, Sep 17\nRandom Variables\n\n\n\n\n\n\n\n\n5\nMon, Sep 22\nDiscrete Probability Distributions\nüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, Sep 24\nContinuous Probability Distributions\n\n\n\nHW 1 Due\n\n\n\n\n6\nMon, Sep 29\nSampling Distributions\nüñ•Ô∏è\n\n\n‚úçÔ∏è HW2\n\n\n\n\n\nWed, Oct 1\nCentral Limit Theorem\n\n\n\n\n\n‚úÖ Team up Due\n\n\n7\nMon, Oct 6\nExam 1\nüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, Oct 8\nInference about Population Means: Confidence Intervals\n\n\n\nHW 2 Due\n\n\n‚úÖ Proposal Due\n\n\n8\nMon, Oct 13\nInference about Population Means: Hypothesis Testing\nüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, Oct 15\nInference about Population Means: Hypothesis Testing\n\n\n\n‚úçÔ∏è HW3\n\n‚úÖ Materials Due\n\n\n9\nMon, Oct 20\nComparing Two Population Means\n\n\n\n\n\n\n\n\n\nWed, Oct 22\nComparing Two Population Means\n\n\n\n\n\n\n\n\n10\nMon, Oct 27\nInference about Population Variances\n\n\n\n\n\n\n\n\n\nWed, Oct 29\nInference about Population Variances\nüñ•Ô∏è\n\n\n\n\n\n\n\n11\nMon, Nov 3\nAnalysis of Variance (ANOVA)\nüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, Nov 5\nAnalysis of Variance (ANOVA)\n\n\n\n\n\n\n\n\n12\nMon, Nov 10\nExam 2\nüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, Nov 12\nMultiple Comparisons\n\n\n\nHW 3 Due\n\n\n\n\n13\nMon, Nov 17\nCorrelation\nüñ•Ô∏è\n\n\n\n\nProposal-2 Due\n\n\n\nWed, Nov 19\nSimple Linear Regression\n\n\n\n‚úçÔ∏è HW4\n\n\n\n\n14\nMon, Nov 24\nSimple Linear Regression\n\n\n\n\n\n‚úÖ Project-2 Guideline\n\n\n\nWed, Nov 26\nNO CLASS: Thanksgiving\n\n\n\n\n\n\n\n\n15\nMon, Dec 1\nInference for Categorical Data\nüñ•Ô∏è\n\n\n\n\n\n\n\n\nWed, Dec 3\nInference for Categorical Data\n\n\n\n\n\n\n\n\n16\n\nWed, Dec 10\n\nFinal Exam (Sec101-2PM)\n\n\n\nHW 4 Due\n\n\n\n\n\nFri, Dec 12\nFinal Exam (Sec102-3:30PM)\n\n\n\n\n\n‚úÖ Final project guideline\n\n\n\n\nI reserve the right to make changes to the schedule.",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "present-work-2.html",
    "href": "present-work-2.html",
    "title": "Midterm Project II Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(2025)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[2]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[3]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[4]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[5]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\"",
    "crumbs": [
      "Midterm Project II",
      "Topics and Works"
    ]
  },
  {
    "objectID": "present-work-2.html#presentation-order",
    "href": "present-work-2.html#presentation-order",
    "title": "Midterm Project II Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(2025)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[2]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[3]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[4]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[5]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\"",
    "crumbs": [
      "Midterm Project II",
      "Topics and Works"
    ]
  },
  {
    "objectID": "present-work-2.html#project-materials",
    "href": "present-work-2.html#project-materials",
    "title": "Midterm Project II Proposal and Presentation",
    "section": "Project Materials",
    "text": "Project Materials\n\nGroup 1 (Rakesh, Daniel, Jeremy): proposal\nGroup 2 (Violet, Vanessa, Michele): proposal\nGroup 3 (Sai, Rohith, Shristi) : proposal\nGroup 4 (Sajjad, Tanjina, Dewan): proposal\nGroup 5 (John, Jeremy, Praful): proposal\nGroup 6 (Ethan, Navid, Sylvester): proposal",
    "crumbs": [
      "Midterm Project II",
      "Topics and Works"
    ]
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Final Project Report Guidelines",
    "section": "",
    "text": "Please send me your entire work (written report, code, data, etc) by May 8, 2025 10 AM.\nYou receive 0 point if you miss the deadline.",
    "crumbs": [
      "Final Project",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "project-description.html#deadline",
    "href": "project-description.html#deadline",
    "title": "Final Project Report Guidelines",
    "section": "",
    "text": "Please send me your entire work (written report, code, data, etc) by May 8, 2025 10 AM.\nYou receive 0 point if you miss the deadline.",
    "crumbs": [
      "Final Project",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "project-description.html#project-writing",
    "href": "project-description.html#project-writing",
    "title": "Final Project Report Guidelines",
    "section": "Project Writing",
    "text": "Project Writing\nYour project can be in either of the following categories:\n\nData Analysis (DA) using one or more machine learning methods learned in class.\nIntroduce a new machine learning model/method/algorithm (ML) and compare it with the model/method/algorithms learned in class.\n\n\nStructure\nIf you choose to do DA, your paper should include the following sections:\n\nIntroduction: State why you think the questions you would like to answer are important or interesting, and why you think the method(s) you consider is an appropriate one to answer your questions.\nData: Describe the selected data set. Perform a thorough exploratory data analysis.\nAnalysis:\n\nExplain the chosen model/method.\nShow why the chosen model(s) is appropriate and better than others.\nAnswer your research questions by the analysis result.\n\nConclusion: Restate your research question, and summarize how you learn from data to answer your questions. What is the contribution of this project? Discuss any limitation of your model/method, and how it could be improved for better inference or prediction results.\nReferences/Bibliography: Include a detailed list of references, including papers, books, websites, code, and any idea/work that is not produced by yourself.\n\nIf you choose to do ML, your paper should include the following sections:\n\nIntroduction: State why you choose to learn this new method. Provide an overview and little history of the method. Describe the intuition and idea of the method. What are the pros and cons of the method?\nModel/Method: Provide the mathematical expression of the model. Explain the model and its properties, and how we do the supervised or unsupervised learning with the model.\nSimulation: Do a simulation study, and compare the chosen method with other methods learned in class. Determine which method performs better under what conditions.\nDiscussion: Based on the simulation results, discuss the advantages and disadvantages of the chosen method. Discuss any variants of the chosen method.\nReferences/Bibliography: Include a detailed list of references, including papers, books, websites, code, and any idea/work that is not produced by yourself.\n\n\n\nFormat and Layout\n\nYour project paper is saved as one PDF.\nYour paper should have your project title and your name on the first page. Date, Abstract, Keywords are optional.\nExcept the first title page, the margins should be no larger than one inch.\nExcept the project title and section title, the font size is 12 pt.\nPlease use 1.5 or double line spacing.\nYour report, including everything, should be at least 12 pages long, but no more than 15 pages.\nYour code should NOT be printed in the paper.\n\n\n\nCode\n\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the report, including the source of the raw data (where you find and load the data) if the project is about data analysis.",
    "crumbs": [
      "Final Project",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "project-description.html#project-evaluation",
    "href": "project-description.html#project-evaluation",
    "title": "Final Project Report Guidelines",
    "section": "Project Evaluation",
    "text": "Project Evaluation\nYour project will be evaluated soley by Dr.¬†Yu based on\n\nContent:\n\nThe quality of research question and relevancy of data to those questions? For example, the relationship between human height and weight is a BAD question. An elementary-school height and weight data set is a BAD data set.\nThe quality of the chosen model. For example, one-way ANOVA is a BAD model.\n\nCorrectness, Completeness and Complexity:\n\nAre machine learning methods carried out and explained correctly?\nDoes project include rigorous analysis and models? Simple linear regression model lacks complexity.\n\nWriting: What is the quality of the machine learning model/method presentation, visualization, writing, and explanations.\nFormat: Does the report follow the required format?\nCreativity and Critical Thought: Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\nReproducibility: Can your code reproduce what you show in the paper?\nReference: Do you cite others work properly?",
    "crumbs": [
      "Final Project",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "documents/project_slides/group5_Statistical ML Project 1.html",
    "href": "documents/project_slides/group5_Statistical ML Project 1.html",
    "title": "MATH 4720 (MSSC 5720) - Fall 2025",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n\ndata = pd.read_csv(\"C:\\\\Users\\\\anish\\\\Downloads\\\\train.csv\")\n\n\ndata.isna().sum().tolist()\n\n[0,\n 0,\n 0,\n 259,\n 0,\n 0,\n 1369,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 872,\n 8,\n 0,\n 0,\n 0,\n 37,\n 37,\n 38,\n 37,\n 0,\n 38,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 690,\n 81,\n 81,\n 81,\n 0,\n 0,\n 81,\n 81,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1453,\n 1179,\n 1406,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0]\n\n\n\ndata = pd.read_csv(\"C:\\\\Users\\\\anish\\\\Downloads\\\\train.csv\")\n\n# Drop columns with missing values\ncolumns_to_drop = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'MasVnrArea', 'FireplaceQu']\ndata = data.drop(columns=columns_to_drop)\n\n# Replace missing values for garage-related columns\ngarage_cols = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ndata[garage_cols] = data[garage_cols].fillna('NoGarage')\n\n# Replace missing values for basement-related columns\nbasement_cols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ndata[basement_cols] = data[basement_cols].fillna('NoBasement')\n\n# Calculate the mean of the 'LotFrontage' column (excluding missing values)\nmean_lotfrontage = data['LotFrontage'].mean()\n\n# Replace missing values with the mean\ndata['LotFrontage'].fillna(mean_lotfrontage, inplace=True)\n\n# Calculate the mode of the 'LotFrontage' column (excluding missing values)\nm_Electrical = data['Electrical'].mode()\n\n# Replace missing values with the mode\ndata['Electrical'].fillna(m_Electrical, inplace=True)\n\n\nimport numpy as np\nfrom scipy.stats import zscore\n\n# Select numerical columns, excluding 'GarageYrBlt' for the Z-score calculation\nnumerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n\n# Remove 'GarageYrBlt' from numerical columns for outlier detection\nnumerical_cols.remove('GarageYrBlt')\n\n# Calculate Z-scores for the remaining numerical columns\nz_scores = np.abs(zscore(data[numerical_cols]))\n\n# Set a threshold for Z-scores \nthreshold = 3\n\n# Keep only rows --&gt; where the Z-score is below the threshold for numerical columns\ndata = data[(z_scores &lt; threshold).all(axis=1)]\n\n# Print the number of rows before and after outlier removal\nprint(f\"After outlier removal: {data.shape[0]}\")\n\nAfter outlier removal: 1028\n\n\n\nnumerical_cols\n\n['Id',\n 'MSSubClass',\n 'LotFrontage',\n 'LotArea',\n 'OverallQual',\n 'OverallCond',\n 'YearBuilt',\n 'YearRemodAdd',\n 'BsmtFinSF1',\n 'BsmtFinSF2',\n 'BsmtUnfSF',\n 'TotalBsmtSF',\n '1stFlrSF',\n '2ndFlrSF',\n 'LowQualFinSF',\n 'GrLivArea',\n 'BsmtFullBath',\n 'BsmtHalfBath',\n 'FullBath',\n 'HalfBath',\n 'BedroomAbvGr',\n 'KitchenAbvGr',\n 'TotRmsAbvGrd',\n 'Fireplaces',\n 'GarageCars',\n 'GarageArea',\n 'WoodDeckSF',\n 'OpenPorchSF',\n 'EnclosedPorch',\n '3SsnPorch',\n 'ScreenPorch',\n 'PoolArea',\n 'MiscVal',\n 'MoSold',\n 'YrSold',\n 'SalePrice']\n\n\n\n# Creating New Features\ndata['TotalArea'] = data['GrLivArea'] + data['TotalBsmtSF']\ndata['HouseAge'] = data['YrSold'] - data['YearBuilt']\ndata['Remodel'] = (data['YearBuilt'] != data['YearRemodAdd']).astype(int)\ndata['TotalBathrooms'] = data['FullBath'] + (0.5 * data['HalfBath']) + data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath'])\n\n\ndata['BsmtHalfBath'].dtype\ndata['KitchenAbvGr'].dtype\ndata['PoolArea'].dtype\n\ndtype('int64')\n\n\n\n# Select numerical columns\nnum = data.select_dtypes(include=['int64', 'float64'])  # Select only numerical columns\nnum = num.drop(columns=['BsmtHalfBath', 'KitchenAbvGr', 'PoolArea'])  # Drop unwanted columns\nprint(num)\n\n        Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\n0        1          60         65.0     8450            7            5   \n2        3          60         68.0    11250            7            5   \n4        5          60         84.0    14260            8            5   \n6        7          20         75.0    10084            8            5   \n10      11          20         70.0    11200            5            5   \n...    ...         ...          ...      ...          ...          ...   \n1452  1453         180         35.0     3675            5            5   \n1453  1454          20         90.0    17217            5            5   \n1454  1455          20         62.0     7500            7            5   \n1455  1456          60         62.0     7917            6            5   \n1456  1457          20         85.0    13175            6            6   \n\n      YearBuilt  YearRemodAdd  BsmtFinSF1  BsmtFinSF2  ...  EnclosedPorch  \\\n0          2003          2003         706           0  ...              0   \n2          2001          2002         486           0  ...              0   \n4          2000          2000         655           0  ...              0   \n6          2004          2005        1369           0  ...              0   \n10         1965          1965         906           0  ...              0   \n...         ...           ...         ...         ...  ...            ...   \n1452       2005          2005         547           0  ...              0   \n1453       2006          2006           0           0  ...              0   \n1454       2004          2005         410           0  ...              0   \n1455       1999          2000           0           0  ...              0   \n1456       1978          1988         790         163  ...              0   \n\n      3SsnPorch  ScreenPorch  MiscVal  MoSold  YrSold  SalePrice  TotalArea  \\\n0             0            0        0       2    2008     208500       2566   \n2             0            0        0       9    2008     223500       2706   \n4             0            0        0      12    2008     250000       3343   \n6             0            0        0       8    2007     307000       3380   \n10            0            0        0       2    2008     129500       2080   \n...         ...          ...      ...     ...     ...        ...        ...   \n1452          0            0        0       5    2006     145000       1619   \n1453          0            0        0       7    2006      84500       2280   \n1454          0            0        0      10    2009     185000       2442   \n1455          0            0        0       8    2007     175000       2600   \n1456          0            0        0       2    2010     210000       3615   \n\n      HouseAge  TotalBathrooms  \n0            5             3.5  \n2            7             3.5  \n4            8             3.5  \n6            3             3.0  \n10          43             2.0  \n...        ...             ...  \n1452         1             2.0  \n1453         0             1.0  \n1454         5             3.0  \n1455         8             2.5  \n1456        32             3.0  \n\n[1028 rows x 37 columns]\n\n\n\n# Compute correlation matrix\ncorr = num.corr()\n\n# Plot the correlation matrix\nplt.figure(figsize=(20, 15))\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, linecolor=\"black\")\nplt.title(\"Correlation Matrix\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# StandardScaler\nscaler = StandardScaler()\n\n# Select numeric columns for scaling \nnumeric_cols = data.select_dtypes(include=['number']).columns\n\n# Apply StandardScaler\ndata[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n\n# Verify scaling\nprint(data.head())\n\n          Id  MSSubClass MSZoning  LotFrontage   LotArea Street LotShape  \\\n0  -1.735247    0.144020       RL    -0.142721 -0.194159   Pave      Reg   \n2  -1.730520    0.144020       RL     0.020674  0.576120   Pave      IR1   \n4  -1.725793    0.144020       RL     0.892110  1.404170   Pave      IR1   \n6  -1.721066   -0.846299       RL     0.401927  0.255354   Pave      Reg   \n10 -1.711611   -0.846299       RL     0.129603  0.562365   Pave      Reg   \n\n   LandContour Utilities LotConfig  ...   MiscVal    MoSold    YrSold  \\\n0          Lvl    AllPub    Inside  ... -0.144857 -1.592606  0.138594   \n2          Lvl    AllPub    Inside  ... -0.144857  1.026314  0.138594   \n4          Lvl    AllPub       FR2  ... -0.144857  2.148708  0.138594   \n6          Lvl    AllPub    Inside  ... -0.144857  0.652182 -0.607345   \n10         Lvl    AllPub    Inside  ... -0.144857 -1.592606  0.138594   \n\n   SaleType SaleCondition SalePrice  TotalArea  HouseAge   Remodel  \\\n0        WD        Normal  0.534157   0.180386 -0.945796 -0.885845   \n2        WD        Normal  0.766241   0.392149 -0.876887  1.128865   \n4        WD        Normal  1.176255   1.355673 -0.842432 -0.885845   \n6        WD        Normal  2.058171   1.411639 -1.014706  1.128865   \n10       WD        Normal -0.688148  -0.554736  0.363484 -0.885845   \n\n    TotalBathrooms  \n0         1.763476  \n2         1.763476  \n4         1.763476  \n6         1.109016  \n10       -0.199903  \n\n[5 rows x 78 columns]\n\n\n\n# List of nominal variables to one-hot encode\nnominal_vars = [\n    'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n    'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', \n    'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', \n    'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n    'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', \n    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', \n    'SaleCondition'\n]\n\n# List of ordinal variables to numeric encode\nordinal_vars = [\n    'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n    'HeatingQC', 'KitchenQual', 'Functional', 'GarageQual', 'GarageCond'\n]\n\n# Replace missing values for garage-related columns\ngarage_cols = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ndata[garage_cols] = data[garage_cols].fillna('NoGarage')\n\n# Replace missing values for basement-related columns\nbasement_cols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ndata[basement_cols] = data[basement_cols].fillna('NoBasement')\n\n# Ordinal Mapping Dictionary\nordinal_mapping = {\n    'OverallQual': {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10},\n    'OverallCond': {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10},\n    'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoBasement': 0},\n    'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n    'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoBasement': 0},\n    'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoBasement': 0},\n    'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n    'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n    'Functional': {'Typ': 5, 'Min1': 4, 'Min2': 3, 'Mod': 2, 'Maj1': 1, 'Maj2': 0, 'Sev': -1, 'Sal': -2},\n    'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoGarage': 0},\n    'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoGarage': 0}\n}\n\n# Apply the ordinal mappings to the original columns before encoding\nfor var in ordinal_vars:\n    if var in data.columns:\n        data[var] = data[var].map(ordinal_mapping.get(var, {}))\n\n# Apply one-hot encoding for nominal variables\ndata = pd.get_dummies(data, columns=nominal_vars, drop_first=True)\n\n\n# Number of nominal variables\nnum_nominal = len(nominal_vars)\n\n# Number of ordinal variables\nnum_ordinal = len(ordinal_vars)\n\n# Print the count\nprint(f\"Number of nominal variables: {num_nominal}\")\nprint(f\"Number of ordinal variables: {num_ordinal}\")\n\nNumber of nominal variables: 37\nNumber of ordinal variables: 9\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1028 entries, 0 to 1456\nColumns: 220 entries, Id to SaleCondition_Partial\ndtypes: bool(179), float64(41)\nmemory usage: 517.0 KB\n\n\n\nmissing_values = data.isna().sum()\nmissing_columns = missing_values[missing_values &gt; 0]  # Select only columns with missing values\n\nprint(missing_columns)\n\nGarageYrBlt    46\ndtype: int64\n\n\n\ndata = data.drop(columns=['GarageYrBlt'])\n\n\n# Target and features\nX = data.drop(columns=[\"SalePrice\"])  # Take all features except SalePrice\ny = data[\"SalePrice\"]\n\n# Handle categorical variables by one-hot encoding (if necessary)\nX = pd.get_dummies(X, drop_first=True)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Standardize features for better performance\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# Ridge Regression\nridge = Ridge(alpha=100, max_iter=1000, random_state= 43)  # Adjust alpha for regularization strength\nridge.fit(X_train_scaled, y_train)\nridge_preds = ridge.predict(X_test_scaled)\nridge_mse = mean_squared_error(y_test, ridge_preds)\nprint(f\"Ridge Regression MSE: {ridge_mse:.4f}\")\n\n# Lasso Regression\nlasso = Lasso(alpha=0.1, max_iter=1000, random_state= 43)  # Adjust alpha for feature selection strength\nlasso.fit(X_train_scaled, y_train)\nlasso_preds = lasso.predict(X_test_scaled)\nlasso_mse = mean_squared_error(y_test, lasso_preds)\nprint(f\"Lasso Regression MSE: {lasso_mse:.4f}\")\n\nRidge Regression MSE: 0.0879\nLasso Regression MSE: 0.1505\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\nimport warnings\nfrom tabulate import tabulate\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Target and features\nX = data.drop(columns=[\"SalePrice\"])  # Take all features except SalePrice\ny = data[\"SalePrice\"]\n\n# Handle categorical variables by one-hot encoding \nX = pd.get_dummies(X, drop_first=True)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize features for better performance \nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Ridge Regression\nridge = Ridge(alpha=100, max_iter=1000, random_state= 43)\nridge.fit(X_train_scaled, y_train)\nridge_preds = ridge.predict(X_test_scaled)\nridge_mse = mean_squared_error(y_test, ridge_preds)\nprint(f\"Ridge Regression MSE: {ridge_mse:.4f}\")\n\n# Lasso Regression\nlasso = Lasso(alpha=0.1, max_iter=1000, random_state= 43)\nlasso.fit(X_train_scaled, y_train)\nlasso_preds = lasso.predict(X_test_scaled)\nlasso_mse = mean_squared_error(y_test, lasso_preds)\nprint(f\"Lasso Regression MSE: {lasso_mse:.4f}\")\n\n# R¬≤ for both models\nridge_r2 = r2_score(y_test, ridge_preds)\nlasso_r2 = r2_score(y_test, lasso_preds)\nprint(f\"Ridge R¬≤: {ridge_r2:.4f}\")\nprint(f\"Lasso R¬≤: {lasso_r2:.4f}\")\n\nRidge Regression MSE: 0.0879\nLasso Regression MSE: 0.1505\nRidge R¬≤: 0.9154\nLasso R¬≤: 0.8552\n\n\n\n# Function to calculate regression metrics\ndef regression_metrics(y_true, y_pred, X):\n    mse = mean_squared_error(y_true, y_pred)\n    # Root Mean Squared Error\n    rmse = np.sqrt(mse)  \n    # Mean Absolute Error\n    mae = mean_absolute_error(y_true, y_pred)  \n    # R¬≤ Score\n    r2 = r2_score(y_true, y_pred)  \n    explained_var = explained_variance_score(y_true, y_pred)  \n    n, p = X.shape  # Number of samples (n) and predictors (p)\n\n    return {\"RMSE\": rmse, \"MAE\": mae}\n\n# Ridge Regression Metrics\nridge_metrics = regression_metrics(y_test, ridge_preds, X_test)\n\n# Lasso Regression Metrics\nlasso_metrics = regression_metrics(y_test, lasso_preds, X_test)\n\n# Create a DataFrame to display the results \nmetrics_df = pd.DataFrame([ridge_metrics, lasso_metrics], index=[\"Ridge\", \"Lasso\"])\n\n# Add MSE and R¬≤ values to DataFrame\nmetrics_df[\"MSE\"] = [ridge_mse, lasso_mse]\nmetrics_df[\"R¬≤\"] = [ridge_r2, lasso_r2]\n\n# Reorder columns to the specified order\nmetrics_df = metrics_df[[\"MSE\", \"RMSE\", \"MAE\", \"R¬≤\"]]\n\n# Round values to 4 decimal places\nmetrics_df = metrics_df.applymap(lambda x: f\"{x:.4f}\")\n\n# Print the table with lines using tabulate\nprint(metrics_df)\n\n          MSE    RMSE     MAE      R¬≤\nRidge  0.0879  0.2964  0.2024  0.9154\nLasso  0.1505  0.3879  0.2759  0.8552\n\n\n\n# Display important features from Lasso\nlasso_coef = pd.Series(lasso.coef_, index=X.columns)\nprint(\"Lasso selected features: \\n\",lasso_coef[lasso_coef != 0])\n\nLasso selected features: \n LotArea             0.006018\nOverallQual         0.219580\nYearRemodAdd        0.020271\nGrLivArea           0.085451\nGarageCars          0.037253\nGarageArea          0.078726\nTotalArea           0.314569\nHouseAge           -0.026418\nTotalBathrooms      0.090354\nExterQual_3        -0.031297\nBsmtQual_5          0.068490\nBsmtFinType1_GLQ    0.010984\nKitchenQual_3      -0.003296\nKitchenQual_5       0.023717\ndtype: float64\n\n\n\nfrom sklearn.model_selection import GridSearchCV\nridge_cv = GridSearchCV(Lasso(), {'alpha': [0.01, 0.1, 1, 10, 100]}, cv=5)\nridge_cv.fit(X_train_scaled, y_train)\nprint(f\"Best Ridge Lasso: {ridge_cv.best_params_['alpha']}\")\n\nBest Ridge Lasso: 0.01\n\n\n\nfrom sklearn.model_selection import GridSearchCV\nridge_cv = GridSearchCV(Ridge(), {'alpha': [0.01, 0.1, 1, 10, 100]}, cv=5)\nridge_cv.fit(X_train_scaled, y_train)\nprint(f\"Best Ridge alpha: {ridge_cv.best_params_['alpha']}\")\n\nBest Ridge alpha: 100\n\n\n\ndata.shape\n\n(1028, 219)\n\n\n\n#OLS Regression\nimport statsmodels.api as sm\n\n# Add a constant term for the intercept\nX_train_ols = sm.add_constant(X_train_scaled)\nX_test_ols = sm.add_constant(X_test_scaled)\n\n# Fit OLS model\nols_model = sm.OLS(y_train, X_train_ols).fit()\n\n# Print summary\nprint(ols_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              SalePrice   R-squared:                       0.954\nModel:                            OLS   Adj. R-squared:                  0.940\nMethod:                 Least Squares   F-statistic:                     66.42\nDate:                Thu, 06 Mar 2025   Prob (F-statistic):               0.00\nTime:                        15:19:50   Log-Likelihood:                 105.99\nNo. Observations:                 822   AIC:                             182.0\nDf Residuals:                     625   BIC:                             1110.\nDf Model:                         196                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0226      0.009     -2.662      0.008      -0.039      -0.006\nx1             0.0007      0.010      0.069      0.945      -0.018       0.020\nx2            -0.0853      0.086     -0.993      0.321      -0.254       0.083\nx3            -0.0177      0.015     -1.184      0.237      -0.047       0.012\nx4             0.0660      0.016      4.013      0.000       0.034       0.098\nx5             0.0975      0.021      4.683      0.000       0.057       0.138\nx6             0.0903      0.015      6.157      0.000       0.062       0.119\nx7             0.0721      0.020      3.633      0.000       0.033       0.111\nx8             0.0192      0.020      0.944      0.346      -0.021       0.059\nx9             0.0710      0.014      4.976      0.000       0.043       0.099\nx10            0.0125      0.020      0.625      0.532      -0.027       0.052\nx11           -0.0230      0.013     -1.790      0.074      -0.048       0.002\nx12            0.0547      0.021      2.660      0.008       0.014       0.095\nx13            0.0220      0.025      0.867      0.386      -0.028       0.072\nx14            0.1899      0.023      8.165      0.000       0.144       0.236\nx15           -0.0025      0.010     -0.263      0.793      -0.021       0.016\nx16            0.1932      0.018     10.859      0.000       0.158       0.228\nx17            0.0068      0.012      0.561      0.575      -0.017       0.031\nx18        -1.486e-15    6.6e-16     -2.251      0.025   -2.78e-15   -1.89e-16\nx19           -0.0012      0.014     -0.085      0.932      -0.029       0.026\nx20           -0.0105      0.016     -0.654      0.513      -0.042       0.021\nx21           -0.0382      0.017     -2.202      0.028      -0.072      -0.004\nx22        -2.905e-15   1.44e-15     -2.022      0.044   -5.73e-15   -8.35e-17\nx23           -0.0040      0.022     -0.184      0.854      -0.047       0.039\nx24            0.0231      0.013      1.848      0.065      -0.001       0.048\nx25            0.0597      0.025      2.421      0.016       0.011       0.108\nx26            0.0605      0.024      2.532      0.012       0.014       0.107\nx27            0.0230      0.011      2.116      0.035       0.002       0.044\nx28            0.0184      0.011      1.649      0.100      -0.004       0.040\nx29            0.0134      0.012      1.134      0.257      -0.010       0.037\nx30           -0.0058      0.009     -0.619      0.536      -0.024       0.013\nx31            0.0182      0.010      1.802      0.072      -0.002       0.038\nx32         9.506e-16   6.79e-16      1.399      0.162   -3.83e-16    2.28e-15\nx33            0.0178      0.010      1.755      0.080      -0.002       0.038\nx34            0.0057      0.010      0.570      0.569      -0.014       0.025\nx35           -0.0050      0.010     -0.476      0.634      -0.026       0.016\nx36            0.1594      0.011     14.562      0.000       0.138       0.181\nx37           -0.0721      0.020     -3.655      0.000      -0.111      -0.033\nx38            0.0118      0.013      0.917      0.359      -0.013       0.037\nx39            0.0002      0.010      0.024      0.981      -0.019       0.020\nx40            0.0912      0.047      1.938      0.053      -0.001       0.184\nx41            0.0611      0.018      3.381      0.001       0.026       0.097\nx42            0.1646      0.077      2.147      0.032       0.014       0.315\nx43            0.1325      0.063      2.120      0.034       0.010       0.255\nx44           -0.0212      0.015     -1.404      0.161      -0.051       0.008\nx45         4.089e-05      0.010      0.004      0.997      -0.020       0.020\nx46           -0.0068      0.013     -0.540      0.590      -0.032       0.018\nx47            0.0057      0.012      0.492      0.623      -0.017       0.029\nx48            0.0117      0.016      0.737      0.462      -0.020       0.043\nx49           -0.0315      0.013     -2.380      0.018      -0.057      -0.006\nx50           -0.0149      0.018     -0.814      0.416      -0.051       0.021\nx51            0.0056      0.011      0.488      0.625      -0.017       0.028\nx52           -0.0270      0.011     -2.569      0.010      -0.048      -0.006\nx53           -0.0137      0.011     -1.285      0.199      -0.035       0.007\nx54           -0.0189      0.012     -1.569      0.117      -0.042       0.005\nx55            0.0089      0.013      0.692      0.489      -0.016       0.034\nx56            0.0017      0.009      0.177      0.859      -0.017       0.020\nx57           -0.0108      0.010     -1.121      0.263      -0.030       0.008\nx58            0.0058      0.016      0.367      0.714      -0.025       0.037\nx59           -0.0336      0.026     -1.273      0.203      -0.085       0.018\nx60           -0.0311      0.014     -2.159      0.031      -0.059      -0.003\nx61           -0.0972      0.031     -3.149      0.002      -0.158      -0.037\nx62            0.0217      0.021      1.024      0.306      -0.020       0.063\nx63           -0.1061      0.028     -3.731      0.000      -0.162      -0.050\nx64           -0.0688      0.025     -2.728      0.007      -0.118      -0.019\nx65           -0.0468      0.024     -1.915      0.056      -0.095       0.001\nx66           -0.0288      0.020     -1.439      0.151      -0.068       0.011\nx67           -0.0747      0.020     -3.781      0.000      -0.113      -0.036\nx68           -0.1283      0.037     -3.427      0.001      -0.202      -0.055\nx69           -0.0015      0.016     -0.096      0.924      -0.032       0.029\nx70           -0.0791      0.022     -3.551      0.000      -0.123      -0.035\nx71            0.0153      0.019      0.808      0.419      -0.022       0.053\nx72            0.0193      0.022      0.880      0.379      -0.024       0.062\nx73           -0.0856      0.034     -2.543      0.011      -0.152      -0.019\nx74           -0.0344      0.016     -2.170      0.030      -0.066      -0.003\nx75           -0.0513      0.024     -2.130      0.034      -0.099      -0.004\nx76           -0.0375      0.019     -1.927      0.054      -0.076       0.001\nx77           -0.0040      0.030     -0.132      0.895      -0.064       0.056\nx78            0.0475      0.014      3.378      0.001       0.020       0.075\nx79           -0.0513      0.018     -2.825      0.005      -0.087      -0.016\nx80           -0.0219      0.012     -1.803      0.072      -0.046       0.002\nx81            0.0150      0.017      0.860      0.390      -0.019       0.049\nx82            0.0677      0.022      3.052      0.002       0.024       0.111\nx83           -0.0031      0.011     -0.293      0.770      -0.024       0.018\nx84            0.0294      0.013      2.312      0.021       0.004       0.054\nx85           -0.0075      0.012     -0.633      0.527      -0.031       0.016\nx86            0.0117      0.015      0.779      0.436      -0.018       0.041\nx87        -5.337e-16   1.89e-16     -2.818      0.005   -9.06e-16   -1.62e-16\nx88            0.0143      0.014      0.998      0.319      -0.014       0.042\nx89            0.0132      0.017      0.754      0.451      -0.021       0.047\nx90            0.0083      0.014      0.611      0.542      -0.018       0.035\nx91           -0.0125      0.013     -0.934      0.351      -0.039       0.014\nx92           -0.0174      0.011     -1.534      0.126      -0.040       0.005\nx93           -0.0292      0.042     -0.702      0.483      -0.111       0.052\nx94           -0.0226      0.063     -0.362      0.718      -0.145       0.100\nx95            0.0165      0.013      1.298      0.195      -0.008       0.041\nx96            0.0754      0.041      1.849      0.065      -0.005       0.156\nx97           -0.0092      0.010     -0.935      0.350      -0.028       0.010\nx98            0.0090      0.030      0.302      0.763      -0.050       0.068\nx99            0.0257      0.019      1.350      0.178      -0.012       0.063\nx100           0.0398      0.024      1.682      0.093      -0.007       0.086\nx101           0.0006      0.005      0.107      0.915      -0.010       0.011\nx102          -0.0094      0.011     -0.825      0.410      -0.032       0.013\nx103           0.0008      0.006      0.135      0.893      -0.011       0.012\nx104           0.0112      0.012      0.900      0.369      -0.013       0.036\nx105          -0.0083      0.012     -0.716      0.474      -0.031       0.014\nx106          -0.0139      0.013     -1.084      0.279      -0.039       0.011\nx107        -3.68e-16   1.37e-16     -2.678      0.008   -6.38e-16   -9.81e-17\nx108          -0.0012      0.021     -0.055      0.956      -0.043       0.040\nx109          -0.0099      0.038     -0.261      0.794      -0.085       0.065\nx110          -0.0020      0.007     -0.296      0.767      -0.015       0.011\nx111          -0.1526      0.069     -2.204      0.028      -0.288      -0.017\nx112          -0.1478      0.094     -1.576      0.115      -0.332       0.036\nx113          -0.0161      0.014     -1.181      0.238      -0.043       0.011\nx114          -0.1525      0.098     -1.559      0.120      -0.345       0.040\nx115          -0.0895      0.060     -1.481      0.139      -0.208       0.029\nx116          -0.0363      0.033     -1.102      0.271      -0.101       0.028\nx117          -0.2611      0.138     -1.886      0.060      -0.533       0.011\nx118          -0.1737      0.085     -2.032      0.043      -0.342      -0.006\nx119          -0.0419      0.040     -1.037      0.300      -0.121       0.037\nx120           0.0205      0.025      0.808      0.419      -0.029       0.070\nx121           0.0150      0.028      0.542      0.588      -0.039       0.069\nx122          -0.0020      0.007     -0.296      0.767      -0.015       0.011\nx123           0.1372      0.067      2.038      0.042       0.005       0.269\nx124           0.1009      0.087      1.159      0.247      -0.070       0.272\nx125           0.0115      0.023      0.492      0.623      -0.034       0.057\nx126           0.1548      0.094      1.646      0.100      -0.030       0.340\nx127           0.0074      0.013      0.560      0.576      -0.019       0.033\nx128           0.0672      0.066      1.026      0.305      -0.061       0.196\nx129          -0.0002      0.013     -0.017      0.986      -0.025       0.025\nx130           0.0394      0.035      1.123      0.262      -0.030       0.108\nx131           0.2421      0.133      1.822      0.069      -0.019       0.503\nx132           0.1518      0.081      1.868      0.062      -0.008       0.311\nx133           0.0474      0.043      1.097      0.273      -0.037       0.132\nx134          -0.0291      0.117     -0.248      0.804      -0.260       0.201\nx135          -0.0004      0.117     -0.003      0.997      -0.231       0.230\nx136           0.0156      0.032      0.486      0.627      -0.047       0.079\nx137           0.0285      0.031      0.905      0.366      -0.033       0.090\nx138           0.0079      0.031      0.253      0.800      -0.054       0.069\nx139           0.0303      0.026      1.170      0.242      -0.021       0.081\nx140           0.0371      0.028      1.339      0.181      -0.017       0.092\nx141          -0.0119      0.019     -0.641      0.522      -0.048       0.024\nx142           0.0089      0.011      0.789      0.430      -0.013       0.031\nx143          2.5e-17   3.54e-17      0.706      0.480   -4.45e-17    9.46e-17\nx144           0.0101      0.012      0.858      0.391      -0.013       0.033\nx145          -0.0066      0.012     -0.545      0.586      -0.030       0.017\nx146          -0.0269      0.011     -2.548      0.011      -0.048      -0.006\nx147           0.0582      0.013      4.626      0.000       0.034       0.083\nx148          -0.0133      0.010     -1.376      0.169      -0.032       0.006\nx149           0.0098      0.006      1.716      0.087      -0.001       0.021\nx150          -0.0069      0.008     -0.879      0.380      -0.022       0.009\nx151           0.0477      0.012      4.120      0.000       0.025       0.070\nx152          -0.0116      0.012     -0.997      0.319      -0.034       0.011\nx153          -0.0357      0.015     -2.402      0.017      -0.065      -0.007\nx154           0.0054      0.011      0.510      0.610      -0.015       0.026\nx155          -0.0153      0.012     -1.235      0.217      -0.040       0.009\nx156           0.0432      0.017      2.480      0.013       0.009       0.077\nx157          -0.0232      0.012     -1.998      0.046      -0.046      -0.000\nx158           0.0054      0.011      0.510      0.610      -0.015       0.026\nx159          -0.0007      0.013     -0.059      0.953      -0.025       0.024\nx160           0.0097      0.021      0.466      0.641      -0.031       0.051\nx161          -0.0094      0.028     -0.332      0.740      -0.065       0.046\nx162           0.0109      0.013      0.844      0.399      -0.014       0.036\nx163          -0.0020      0.032     -0.061      0.951      -0.066       0.062\nx164           0.0054      0.011      0.510      0.610      -0.015       0.026\nx165          -0.0080      0.030     -0.268      0.789      -0.067       0.051\nx166          -0.0052      0.056     -0.091      0.927      -0.116       0.105\nx167           0.0125      0.038      0.328      0.743      -0.062       0.087\nx168           0.0109      0.027      0.409      0.682      -0.042       0.063\nx169           0.0015      0.029      0.051      0.960      -0.056       0.059\nx170           0.0288      0.016      1.765      0.078      -0.003       0.061\nx171          -0.0323      0.031     -1.035      0.301      -0.093       0.029\nx172          -0.0302      0.027     -1.120      0.263      -0.083       0.023\nx173          -0.0285      0.036     -0.782      0.435      -0.100       0.043\nx174          -0.0013      0.015     -0.086      0.932      -0.031       0.029\nx175           0.0007      0.013      0.058      0.954      -0.024       0.026\nx176          -0.0020      0.013     -0.157      0.875      -0.027       0.023\nx177           0.0001      0.012      0.011      0.991      -0.024       0.024\nx178          -0.0064      0.042     -0.153      0.879      -0.088       0.075\nx179          -0.0127      0.045     -0.283      0.777      -0.101       0.075\nx180           0.0565      0.023      2.500      0.013       0.012       0.101\nx181           0.0273      0.026      1.031      0.303      -0.025       0.079\nx182          -0.0169      0.027     -0.628      0.530      -0.070       0.036\nx183           0.0525      0.046      1.154      0.249      -0.037       0.142\nx184           0.0356      0.043      0.824      0.410      -0.049       0.120\nx185           0.1038      0.069      1.505      0.133      -0.032       0.239\nx186           0.0072      0.011      0.650      0.516      -0.015       0.029\nx187          -0.0024      0.012     -0.195      0.846      -0.027       0.022\nx188           0.0103      0.014      0.760      0.447      -0.016       0.037\nx189          -0.0096      0.015     -0.630      0.529      -0.040       0.020\nx190           0.0095      0.006      1.726      0.085      -0.001       0.020\nx191           0.0095      0.006      1.726      0.085      -0.001       0.020\nx192           0.0172      0.014      1.242      0.215      -0.010       0.044\nx193           0.0173      0.019      0.911      0.362      -0.020       0.055\nx194          -0.0189      0.014     -1.336      0.182      -0.047       0.009\nx195          -0.0215      0.010     -2.233      0.026      -0.040      -0.003\nx196           0.0067      0.007      1.018      0.309      -0.006       0.020\nx197           0.0077      0.009      0.815      0.416      -0.011       0.026\nx198           0.0024      0.005      0.484      0.629      -0.007       0.012\nx199           0.0139      0.018      0.781      0.435      -0.021       0.049\nx200          -0.0103      0.010     -1.023      0.307      -0.030       0.009\nx201          -0.0022      0.007     -0.323      0.747      -0.015       0.011\nx202          -0.0051      0.009     -0.544      0.586      -0.024       0.013\nx203           0.0024      0.005      0.484      0.629      -0.007       0.012\nx204          -0.0178      0.012     -1.438      0.151      -0.042       0.007\nx205          -0.0148      0.015     -1.007      0.314      -0.044       0.014\nx206           0.0123      0.010      1.276      0.203      -0.007       0.031\nx207                0          0        nan        nan           0           0\nx208          -0.0029      0.011     -0.257      0.798      -0.025       0.020\nx209          -0.0177      0.010     -1.792      0.074      -0.037       0.002\nx210        8.218e-05      0.014      0.006      0.995      -0.028       0.028\nx211          -0.0631      0.062     -1.017      0.310      -0.185       0.059\nx212           0.0084      0.010      0.873      0.383      -0.011       0.027\nx213           0.0004      0.022      0.020      0.984      -0.043       0.044\nx214           0.0242      0.012      1.959      0.051   -5.65e-05       0.048\nx215          -0.0402      0.018     -2.210      0.028      -0.076      -0.004\nx216           0.0041      0.011      0.391      0.696      -0.017       0.025\nx217           0.0563      0.016      3.436      0.001       0.024       0.088\nx218           0.1447      0.060      2.410      0.016       0.027       0.263\n==============================================================================\nOmnibus:                      122.167   Durbin-Watson:                   1.976\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1183.919\nSkew:                           0.300   Prob(JB):                    8.23e-258\nKurtosis:                       8.849   Cond. No.                     6.03e+16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 4.24e-30. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular."
  },
  {
    "objectID": "documents/project_slides/group4_win1.html",
    "href": "documents/project_slides/group4_win1.html",
    "title": "MATH 4720 (MSSC 5720) - Fall 2025",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\naba = pd.read_csv(\"C:/Users/Rakes/OneDrive/Desktop/stats/p1/abalone_dataset.csv\")\n\n\naba.head(5)\n\n\n\n\n\n\n\n\nSex\nLength\nDiameter\nHeight\nWhole_weight\nShucked_weight\nViscera_weight\nShell_weight\nRings\n\n\n\n\n0\nM\n0.455\n0.365\n0.095\n0.5140\n0.2245\n0.1010\n0.150\n15\n\n\n1\nM\n0.350\n0.265\n0.090\n0.2255\n0.0995\n0.0485\n0.070\n7\n\n\n2\nF\n0.530\n0.420\n0.135\n0.6770\n0.2565\n0.1415\n0.210\n9\n\n\n3\nM\n0.440\n0.365\n0.125\n0.5160\n0.2155\n0.1140\n0.155\n10\n\n\n4\nI\n0.330\n0.255\n0.080\n0.2050\n0.0895\n0.0395\n0.055\n7\n\n\n\n\n\n\n\n\naba.isnull().sum()\n\nSex               0\nLength            0\nDiameter          0\nHeight            0\nWhole_weight      0\nShucked_weight    0\nViscera_weight    0\nShell_weight      0\nRings             0\ndtype: int64\n\n\n\naba.shape\n\n(4177, 9)\n\n\n\naba.info\n\n&lt;bound method DataFrame.info of      Sex  Length  Diameter  Height  Whole_weight  Shucked_weight  \\\n0      M   0.455     0.365   0.095        0.5140          0.2245   \n1      M   0.350     0.265   0.090        0.2255          0.0995   \n2      F   0.530     0.420   0.135        0.6770          0.2565   \n3      M   0.440     0.365   0.125        0.5160          0.2155   \n4      I   0.330     0.255   0.080        0.2050          0.0895   \n...   ..     ...       ...     ...           ...             ...   \n4172   F   0.565     0.450   0.165        0.8870          0.3700   \n4173   M   0.590     0.440   0.135        0.9660          0.4390   \n4174   M   0.600     0.475   0.205        1.1760          0.5255   \n4175   F   0.625     0.485   0.150        1.0945          0.5310   \n4176   M   0.710     0.555   0.195        1.9485          0.9455   \n\n      Viscera_weight  Shell_weight  Rings  \n0             0.1010        0.1500     15  \n1             0.0485        0.0700      7  \n2             0.1415        0.2100      9  \n3             0.1140        0.1550     10  \n4             0.0395        0.0550      7  \n...              ...           ...    ...  \n4172          0.2390        0.2490     11  \n4173          0.2145        0.2605     10  \n4174          0.2875        0.3080      9  \n4175          0.2610        0.2960     10  \n4176          0.3765        0.4950     12  \n\n[4177 rows x 9 columns]&gt;\n\n\n\naba.dtypes\n\nSex                object\nLength            float64\nDiameter          float64\nHeight            float64\nWhole_weight      float64\nShucked_weight    float64\nViscera_weight    float64\nShell_weight      float64\nRings               int64\ndtype: object\n\n\n\nplt.figure(figsize=(6,4))\nsns.countplot(x=aba[\"Sex\"])\nplt.title(\"Distribution of Abalone Sex\")\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Box plots \ncols = [\"Sex\",\"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]\nplt.figure(figsize=(12,8))\nfor i, col in enumerate(cols, 1):\n    plt.subplot(3, 3, i)\n    sns.boxplot(y=aba[col])\n    plt.title(f\"Boxplot of {col}\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Relationships between features\nsns.pairplot(aba)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6,4))\nsns.histplot(aba[\"Height\"], bins=30, kde=True)\nplt.title(\"Distribution of Height\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Scatter plots to visualize growth\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.scatterplot(x=aba[\"Length\"], y=aba[\"Rings\"], hue=aba[\"Sex\"].astype(str), alpha=0.5)\nplt.title(\"Length vs. Rings\")\nplt.xlabel(\"Length\")\nplt.ylabel(\"Rings\")\n\nText(0, 0.5, 'Rings')\n\n\n\n\n\n\n\n\n\n\n# Impute zero height values with mean height\naba.loc[aba[\"Height\"] == 0, \"Height\"] = aba[\"Height\"].mean()\n\n\nfrom scipy.stats.mstats import winsorize\n# Winsorization: Cap extreme values at 5th and 95th percentile\nfor col in [\"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]:\n    aba[col] = winsorize(aba[col], limits=[0.05, 0.05])\n\n\naba = pd.get_dummies(aba, columns=[\"Sex\"])\n\n\n# Compute correlation matrix\ncorr_matrix = aba.corr()\n\n# Plot correlation heatmap\nplt.figure(figsize=(10,6))\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Splitting data\nX = aba.drop(columns=[\"Rings\"])\ny = aba[\"Rings\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardizing numerical features\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns, index=X_train.index)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X.columns, index=X_test.index)\n\nimport statsmodels.api as sm\n# Add constant for OLS\nX_train_ols = sm.add_constant(X_train_scaled)\nX_test_ols = sm.add_constant(X_test_scaled)\n# Fit OLS model\nols_model = sm.OLS(y_train, X_train_ols).fit()\nprint(ols_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Rings   R-squared:                       0.554\nModel:                            OLS   Adj. R-squared:                  0.553\nMethod:                 Least Squares   F-statistic:                     460.5\nDate:                Fri, 07 Mar 2025   Prob (F-statistic):               0.00\nTime:                        14:00:14   Log-Likelihood:                -6730.3\nNo. Observations:                3341   AIC:                         1.348e+04\nDf Residuals:                    3331   BIC:                         1.354e+04\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst              9.8743      0.031    314.165      0.000       9.813       9.936\nLength            -0.1197      0.209     -0.574      0.566      -0.529       0.290\nDiameter           0.9141      0.210      4.347      0.000       0.502       1.326\nHeight             0.7526      0.085      8.863      0.000       0.586       0.919\nWhole_weight       3.7527      0.342     10.961      0.000       3.081       4.424\nShucked_weight    -4.0647      0.179    -22.722      0.000      -4.415      -3.714\nViscera_weight    -0.8375      0.138     -6.087      0.000      -1.107      -0.568\nShell_weight       0.9626      0.165      5.835      0.000       0.639       1.286\nSex_F              0.0906      0.023      3.899      0.000       0.045       0.136\nSex_I             -0.2235      0.026     -8.542      0.000      -0.275      -0.172\nSex_M              0.1296      0.021      6.047      0.000       0.088       0.172\n==============================================================================\nOmnibus:                      294.355   Durbin-Watson:                   1.970\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              415.293\nSkew:                           0.706   Prob(JB):                     6.61e-91\nKurtosis:                       3.993   Cond. No.                     2.45e+15\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 3.99e-27. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n# Prediction\ny_pred = ols_model.predict(X_test_ols)\n# Model evaluation metrics\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"\\nModel Evaluation Metrics:\")\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"R-squared (R¬≤): {r2:.4f}\")\n\n\nModel Evaluation Metrics:\nMean Squared Error (MSE): 3.3736\nMean Absolute Error (MAE): 1.4127\nR-squared (R¬≤): 0.5496\n\n\n\n# Scatter plot for OLS Model\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)\nplt.xlabel(\"Actual Rings\")\nplt.ylabel(\"Predicted Rings\")\nplt.title(\"Actual vs. Predicted Rings (OLS Model)\")\nplt.text(min(y_test), max(y_pred) - 2, f\"MSE: {mse:.4f}\\nMAE: {mae:.4f}\\nR¬≤: {r2:.4f}\", fontsize=10, verticalalignment='top')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Ridge Regression\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\ny_ridge_pred = ridge.predict(X_test)\n\nridge_mse = mean_squared_error(y_test, y_ridge_pred)\nridge_mae = mean_absolute_error(y_test, y_ridge_pred)\nridge_r2 = r2_score(y_test, y_ridge_pred)\n\nprint(f\"\\nRidge Regression Evaluation Metrics:\")\nprint(f\"MSE: {ridge_mse:.4f}\")\nprint(f\"MAE: {ridge_mae:.4f}\")\nprint(f\"R¬≤: {ridge_r2:.4f}\")\n\n\nRidge Regression Evaluation Metrics:\nMSE: 3.3697\nMAE: 1.4200\nR¬≤: 0.5502\n\n\n\n# Coefficients Summary for Ridge Regression\nridge_coefficients = pd.Series(ridge.coef_, index=X.columns)\nprint(\"\\nRidge Regression Coefficients:\")\nprint(ridge_coefficients)\n\n\nRidge Regression Coefficients:\nLength             2.134166\nDiameter           5.892062\nHeight             9.001296\nWhole_weight       6.008200\nShucked_weight   -16.912315\nViscera_weight    -4.183926\nShell_weight      10.411092\nSex_F              0.245876\nSex_I             -0.530194\nSex_M              0.284318\ndtype: float64\n\n\n\n# Scatter plot for Ridge Model\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_ridge_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)\nplt.xlabel(\"Actual Rings\")\nplt.ylabel(\"Predicted Rings\")\nplt.title(\"Actual vs. Predicted Rings (Ridge Model)\")\nplt.text(min(y_test), max(y_ridge_pred) - 2, f\"MSE: {ridge_mse:.4f}\\nMAE: {ridge_mae:.4f}\\nR¬≤: {ridge_r2:.4f}\", fontsize=10, verticalalignment='top')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Lasso Regression\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\ny_lasso_pred = lasso.predict(X_test)\n\nlasso_mse = mean_squared_error(y_test, y_lasso_pred)\nlasso_mae = mean_absolute_error(y_test, y_lasso_pred)\nlasso_r2 = r2_score(y_test, y_lasso_pred)\n\nprint(f\"\\nLasso Regression Evaluation Metrics:\")\nprint(f\"MSE: {lasso_mse:.4f}\")\nprint(f\"MAE: {lasso_mae:.4f}\")\nprint(f\"R¬≤: {lasso_r2:.4f}\")\n\n# Coefficients Summary for Lasso Regression\nlasso_coefficients = pd.Series(lasso.coef_, index=X.columns)\nprint(\"\\nLasso Regression Coefficients:\")\nprint(lasso_coefficients)\n\n\nLasso Regression Evaluation Metrics:\nMSE: 4.8442\nMAE: 1.7291\nR¬≤: 0.3533\n\nLasso Regression Coefficients:\nLength            0.000000\nDiameter          0.000000\nHeight            0.000000\nWhole_weight      2.429523\nShucked_weight   -0.000000\nViscera_weight    0.000000\nShell_weight      0.000000\nSex_F             0.000000\nSex_I            -0.785001\nSex_M             0.000000\ndtype: float64\n\n\n\n# Scatter plot for Lasso Model\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_lasso_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)\nplt.xlabel(\"Actual Rings\")\nplt.ylabel(\"Predicted Rings\")\nplt.title(\"Actual vs. Predicted Rings (Lasso Model)\")\n\nplt.text(min(y_test) + 0.5, max(y_lasso_pred) + 1,\n         f\"MSE: {lasso_mse:.4f}\\nMAE: {lasso_mae:.4f}\\nR¬≤: {lasso_r2:.4f}\", \n         fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom patsy import dmatrix\n\n# Polynomial Regression\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_train_poly = poly.fit_transform(X_train_scaled)\nX_test_poly = poly.transform(X_test_scaled)\n\npoly_model = sm.OLS(y_train, sm.add_constant(X_train_poly)).fit()\nprint(\"\\nPolynomial Regression Summary:\")\nprint(poly_model.summary())\n\n# Predictions\ny_poly_pred = poly_model.predict(sm.add_constant(X_test_poly))\npoly_mse = mean_squared_error(y_test, y_poly_pred)\npoly_mae = mean_absolute_error(y_test, y_poly_pred)\npoly_r2 = r2_score(y_test, y_poly_pred)\nprint(f\"\\nPolynomial Regression Evaluation Metrics:\")\nprint(f\"MSE: {poly_mse:.4f}\")\nprint(f\"MAE: {poly_mae:.4f}\")\nprint(f\"R¬≤: {poly_r2:.4f}\")\n\n\nPolynomial Regression Summary:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Rings   R-squared:                       0.595\nModel:                            OLS   Adj. R-squared:                  0.589\nMethod:                 Least Squares   F-statistic:                     94.80\nDate:                Fri, 07 Mar 2025   Prob (F-statistic):               0.00\nTime:                        14:00:15   Log-Likelihood:                -6570.2\nNo. Observations:                3341   AIC:                         1.324e+04\nDf Residuals:                    3289   BIC:                         1.356e+04\nDf Model:                          51                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.2016      0.017    130.561      0.000       2.169       2.235\nx1            -0.8063      0.227     -3.547      0.000      -1.252      -0.361\nx2             0.3630      0.221      1.645      0.100      -0.070       0.796\nx3             0.3648      0.096      3.796      0.000       0.176       0.553\nx4             5.2842      0.514     10.290      0.000       4.277       6.291\nx5            -4.6651      0.258    -18.089      0.000      -5.171      -4.159\nx6            -0.5059      0.206     -2.450      0.014      -0.911      -0.101\nx7             1.3842      0.220      6.295      0.000       0.953       1.815\nx8            -0.1508      0.013    -11.292      0.000      -0.177      -0.125\nx9            -0.0544      0.018     -3.095      0.002      -0.089      -0.020\nx10            0.1980      0.014     14.041      0.000       0.170       0.226\nx11           -0.1722      0.707     -0.244      0.808      -1.558       1.213\nx12           -0.2377      1.095     -0.217      0.828      -2.385       1.910\nx13           -0.1767      0.558     -0.317      0.752      -1.271       0.918\nx14            4.7773      2.267      2.108      0.035       0.333       9.221\nx15           -0.8998      1.150     -0.783      0.434      -3.154       1.354\nx16           -2.9639      0.892     -3.323      0.001      -4.713      -1.215\nx17           -0.8647      1.099     -0.787      0.431      -3.019       1.290\nx18            0.1726      0.146      1.180      0.238      -0.114       0.459\nx19           -0.4246      0.173     -2.450      0.014      -0.764      -0.085\nx20            0.2458      0.140      1.751      0.080      -0.029       0.521\nx21           -0.8939      0.569     -1.572      0.116      -2.009       0.221\nx22           -0.2281      0.540     -0.422      0.673      -1.287       0.831\nx23           -1.9052      2.110     -0.903      0.367      -6.043       2.233\nx24            1.0809      1.097      0.985      0.325      -1.071       3.233\nx25            1.7421      0.864      2.017      0.044       0.049       3.436\nx26            1.2800      1.016      1.260      0.208      -0.711       3.271\nx27            0.0947      0.148      0.639      0.523      -0.196       0.386\nx28            0.0083      0.175      0.047      0.962      -0.334       0.350\nx29           -0.0992      0.137     -0.723      0.469      -0.368       0.170\nx30            0.4671      0.121      3.871      0.000       0.230       0.704\nx31            1.5358      0.804      1.911      0.056      -0.040       3.111\nx32           -1.0549      0.429     -2.460      0.014      -1.896      -0.214\nx33           -0.4355      0.326     -1.336      0.182      -1.075       0.204\nx34           -0.5845      0.362     -1.617      0.106      -1.293       0.124\nx35           -0.0777      0.060     -1.295      0.196      -0.195       0.040\nx36            0.1129      0.074      1.523      0.128      -0.032       0.258\nx37           -0.0347      0.056     -0.620      0.535      -0.145       0.075\nx38           -1.7600      0.997     -1.765      0.078      -3.715       0.195\nx39           -2.5752      1.202     -2.143      0.032      -4.931      -0.219\nx40            1.3901      0.885      1.571      0.116      -0.345       3.125\nx41           -0.2140      0.962     -0.223      0.824      -2.100       1.672\nx42           -0.1731      0.268     -0.645      0.519      -0.699       0.353\nx43            0.6471      0.392      1.649      0.099      -0.122       1.416\nx44           -0.4611      0.257     -1.797      0.072      -0.964       0.042\nx45            2.0029      0.467      4.288      0.000       1.087       2.919\nx46            0.1708      0.502      0.340      0.734      -0.814       1.155\nx47            0.8347      0.619      1.349      0.178      -0.379       2.048\nx48           -0.1964      0.137     -1.438      0.150      -0.464       0.071\nx49            0.1796      0.197      0.911      0.363      -0.207       0.566\nx50            0.0149      0.132      0.113      0.910      -0.243       0.273\nx51           -0.0739      0.272     -0.272      0.786      -0.608       0.460\nx52           -0.2264      0.505     -0.448      0.654      -1.217       0.764\nx53            0.1532      0.104      1.474      0.141      -0.051       0.357\nx54           -0.1080      0.156     -0.693      0.488      -0.414       0.198\nx55           -0.0427      0.102     -0.418      0.676      -0.243       0.158\nx56           -0.2462      0.293     -0.839      0.401      -0.821       0.329\nx57           -0.1714      0.128     -1.339      0.181      -0.422       0.080\nx58           -0.0696      0.175     -0.397      0.692      -0.414       0.274\nx59            0.2325      0.117      1.982      0.048       0.002       0.463\nx60            2.0798      0.018    117.067      0.000       2.045       2.115\nx61           -0.8824      0.013    -70.215      0.000      -0.907      -0.858\nx62           -1.1463      0.014    -83.750      0.000      -1.173      -1.119\nx63            2.1601      0.023     92.571      0.000       2.114       2.206\nx64           -1.2459      0.015    -84.626      0.000      -1.275      -1.217\nx65            2.3122      0.019    124.315      0.000       2.276       2.349\n==============================================================================\nOmnibus:                      230.353   Durbin-Watson:                   1.964\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              331.696\nSkew:                           0.578   Prob(JB):                     9.40e-73\nKurtosis:                       4.024   Cond. No.                     2.87e+16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.43e-28. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\nPolynomial Regression Evaluation Metrics:\nMSE: 3.1953\nMAE: 1.3357\nR¬≤: 0.5734\n\n\n\n# Compute training predictions\ny_poly_train_pred = poly_model.predict(sm.add_constant(X_train_poly))\n\n# Training vs Test R2\ntrain_r2_poly = r2_score(y_train, y_poly_train_pred)\ntest_r2_poly = r2_score(y_test, y_poly_pred)\n\n# Training vs Test MSE\ntrain_mse_poly = mean_squared_error(y_train, y_poly_train_pred)\ntest_mse_poly = mean_squared_error(y_test, y_poly_pred)\n\nprint(f\"\\nPolynomial Regression Overfitting Check:\")\nprint(f\"Training R2: {train_r2_poly:.4f}\")\nprint(f\"Test R2: {test_r2_poly:.4f}\")\nprint(f\"Training MSE: {train_mse_poly:.4f}\")\nprint(f\"Test MSE: {test_mse_poly:.4f}\")\n\n\nPolynomial Regression Overfitting Check:\nTraining R2: 0.5952\nTest R2: 0.5734\nTraining MSE: 2.9898\nTest MSE: 3.1953\n\n\n\n# Scatter plot for Polynomial Regression\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_poly_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)\nplt.xlabel(\"Actual Rings\")\nplt.ylabel(\"Predicted Rings\")\nplt.title(\"Actual vs. Predicted Rings (Polynomial Model)\")\n\nplt.text(min(y_test), max(y_poly_pred) - 2, \n         f\"MSE: {poly_mse:.4f}\\nMAE: {poly_mae:.4f}\\nR¬≤: {poly_r2:.4f}\", \n         fontsize=10, verticalalignment='top')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\ndegrees = [1, 2, 3, 4, 5]\n\npoly_results = {}\n\nfor degree in degrees:\n    print(f\"\\n Polynomial Degree: {degree}\")\n\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    X_train_poly = poly.fit_transform(X_train_scaled)\n    X_test_poly = poly.transform(X_test_scaled)\n\n    poly_model = sm.OLS(y_train, sm.add_constant(X_train_poly)).fit()\n\n    y_poly_pred = poly_model.predict(sm.add_constant(X_test_poly))\n\n    poly_mse = mean_squared_error(y_test, y_poly_pred)\n    poly_mae = mean_absolute_error(y_test, y_poly_pred)\n    poly_r2 = r2_score(y_test, y_poly_pred)\n\n    poly_results[degree] = {\n        \"MSE\": poly_mse,\n        \"MAE\": poly_mae,\n        \"R2\": poly_r2\n    }\n\n    print(f\" Degree {degree} - MSE: {poly_mse:.4f}, MAE: {poly_mae:.4f}, R¬≤: {poly_r2:.4f}\")\n\n\n Polynomial Degree: 1\n Degree 1 - MSE: 3.3736, MAE: 1.4127, R¬≤: 0.5496\n\n Polynomial Degree: 2\n Degree 2 - MSE: 3.1953, MAE: 1.3357, R¬≤: 0.5734\n\n Polynomial Degree: 3\n Degree 3 - MSE: 6.8021, MAE: 1.4638, R¬≤: 0.0919\n\n Polynomial Degree: 4\n Degree 4 - MSE: 175.1503, MAE: 2.5135, R¬≤: -22.3822\n\n Polynomial Degree: 5\n Degree 5 - MSE: 3170.7095, MAE: 8.2069, R¬≤: -422.2837\n\n\n\n# Plot MSE vs. Polynomial Degree\ndegrees = list(poly_results.keys())\nmse_values = [metrics[\"MSE\"] for metrics in poly_results.values()]\nplt.figure(figsize=(6, 4))\nplt.plot(degrees, mse_values, marker='o', linestyle='-', color='orange')\nplt.xlabel(\"Polynomial Degree\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE vs. Polynomial Degree\")\nplt.yscale(\"log\")\nplt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\nplt.show()"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "MATH 4720/5720 Fall 2025",
    "section": "",
    "text": "This course introduces the ideas of data summary and visualization, probability, and statistics with applications to natural and social sciences and daily life. Topics include but not limited to: random variables, discrete and continuous probability distributions, sampling distributions, confidence intervals, hypothesis testing, analysis of variance, and linear regression. This course builds a foundation for statistical inference, machine learning, and data modeling.\nThe course will assume facility with using the internet and a personal computer. A portion of the course involves  programming using RStudio or Posit Cloud, but prior coding experience is not required.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "code/13-svm-code.html",
    "href": "code/13-svm-code.html",
    "title": "13 - Support Vector Machine Code Demo",
    "section": "",
    "text": "Codelibrary(e1071)\nlibrary(kernlab)\n\n\n\nCodeset.seed(2025)\nn &lt;- 6\np &lt;- 2\nxneg &lt;- matrix(rnorm(n * p), n, p)\nxpos &lt;- matrix(rnorm(n * p, mean = 3), n, p)\nx &lt;- rbind(xpos, xneg)\ny &lt;- matrix(as.factor(c(rep(1, n), rep(-1, n))))\n\n\n\nCodesvm_fit &lt;- e1071::svm(y ~ ., data = data.frame(x, y), type = 'C-classification', \n                      kernel = 'linear', scale = FALSE, cost = 10000)\n\n\n\nCodesvm_fit2 &lt;- kernlab::ksvm(x, y, type = \"C-svc\", kernel = 'vanilladot', C = 10000)\n\n Setting default kernel parameters  \n\n\n\n\nCodeload(\"../data/ESL.mixture.rda\", verbose = TRUE)\n\nLoading objects:\n  ESL.mixture\n\nCodex &lt;- ESL.mixture$x\ny &lt;- ESL.mixture$y\ndat &lt;- data.frame(y = factor(y), x)\nfit &lt;- svm(y ~ ., data = dat, scale = FALSE, kernel = \"radial\", cost = 5)\n\n\n\nCodepx1 &lt;- ESL.mixture$px1\npx2 &lt;- ESL.mixture$px2\nxgrid &lt;- expand.grid(X1 = px1, X2 = px2)\nfunc &lt;- predict(fit, xgrid, decision.values = TRUE)\nfunc &lt;- attributes(func)$decision\n\n\n\nCodeygrid &lt;- predict(fit, xgrid)\nplot(xgrid, col = ifelse(ygrid == 1, 2, 4), \n     pch = 20, cex = 0.3, main=\"SVM with RBF kernal\")\npoints(x, col = ifelse(y == 1, 2, 4), pch = 19)\ncontour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE, lwd = 5)"
  },
  {
    "objectID": "code/13-svm-code.html#r-implementation",
    "href": "code/13-svm-code.html#r-implementation",
    "title": "13 - Support Vector Machine Code Demo",
    "section": "",
    "text": "Codelibrary(e1071)\nlibrary(kernlab)\n\n\n\nCodeset.seed(2025)\nn &lt;- 6\np &lt;- 2\nxneg &lt;- matrix(rnorm(n * p), n, p)\nxpos &lt;- matrix(rnorm(n * p, mean = 3), n, p)\nx &lt;- rbind(xpos, xneg)\ny &lt;- matrix(as.factor(c(rep(1, n), rep(-1, n))))\n\n\n\nCodesvm_fit &lt;- e1071::svm(y ~ ., data = data.frame(x, y), type = 'C-classification', \n                      kernel = 'linear', scale = FALSE, cost = 10000)\n\n\n\nCodesvm_fit2 &lt;- kernlab::ksvm(x, y, type = \"C-svc\", kernel = 'vanilladot', C = 10000)\n\n Setting default kernel parameters  \n\n\n\n\nCodeload(\"../data/ESL.mixture.rda\", verbose = TRUE)\n\nLoading objects:\n  ESL.mixture\n\nCodex &lt;- ESL.mixture$x\ny &lt;- ESL.mixture$y\ndat &lt;- data.frame(y = factor(y), x)\nfit &lt;- svm(y ~ ., data = dat, scale = FALSE, kernel = \"radial\", cost = 5)\n\n\n\nCodepx1 &lt;- ESL.mixture$px1\npx2 &lt;- ESL.mixture$px2\nxgrid &lt;- expand.grid(X1 = px1, X2 = px2)\nfunc &lt;- predict(fit, xgrid, decision.values = TRUE)\nfunc &lt;- attributes(func)$decision\n\n\n\nCodeygrid &lt;- predict(fit, xgrid)\nplot(xgrid, col = ifelse(ygrid == 1, 2, 4), \n     pch = 20, cex = 0.3, main=\"SVM with RBF kernal\")\npoints(x, col = ifelse(y == 1, 2, 4), pch = 19)\ncontour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE, lwd = 5)"
  },
  {
    "objectID": "code/13-svm-code.html#python-implementation",
    "href": "code/13-svm-code.html#python-implementation",
    "title": "13 - Support Vector Machine Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n\n\n2.1 Linear kernel\n\nCodenp.random.seed(1)\nn = 6\np = 2\nxneg = np.random.normal(size=(n, p))\nxpos = np.random.normal(loc=3, size=(n, p))\nx = np.vstack((xpos, xneg))\ny = np.array([1] * n + [-1] * n)\n\n\n\nCodesvm_fit = SVC(kernel=\"linear\", C=10000, probability=True)\nsvm_fit.fit(x, y)\n\n\n\n\nSVC(C=10000, kernel='linear', probability=True)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†SVC?Documentation for SVCiFittedSVC(C=10000, kernel='linear', probability=True) \n\n\n\n\n2.2 Radial basis (Gaussian) kernel\n\nCodeimport rdata\nmixture_example = rdata.read_rda('../data/ESL.mixture.rda')\n\n/Users/chenghanyu/.virtualenvs/r-reticulate/lib/python3.12/site-packages/rdata/conversion/_conversion.py:856: UserWarning: Missing constructor for R class \"matrix\". The underlying R object is returned instead.\n  warnings.warn(\n\nCodex = mixture_example['ESL.mixture']['x']\ny = mixture_example['ESL.mixture']['y']\n\n\n\nCodesvm_rbf = SVC(kernel = 'rbf', C = 5, probability = True)\nsvm_rbf.fit(x, y)\n\n\n\n\nSVC(C=5, probability=True)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†SVC?Documentation for SVCiFittedSVC(C=5, probability=True) \n\n\n\n\nCodepx1 = mixture_example['ESL.mixture']['px1']\npx2 = mixture_example['ESL.mixture']['px2']\nppx1, ppx2 = np.meshgrid(px1, px2)\nx_grid = np.c_[ppx1.ravel(), ppx2.ravel()]\ndecision_function = svm_rbf.decision_function(x_grid).reshape(ppx1.shape)\nprobability = svm_rbf.predict_proba(x_grid)[:, 1].reshape(ppx1.shape)\n\n\n\nCodeplt.figure()\nplt.contour(px1, px2, decision_function, levels=[0], colors=\"black\", linewidths=2)\nplt.scatter(x[:, 0], x[:, 1], c=y, cmap=\"bwr\", s=50, edgecolor=\"k\")\nplt.scatter(x_grid[:, 0], x_grid[:, 1], \n            c=np.where(probability.ravel() &gt;= 0.5, \"red\", \"blue\"), s=10, alpha=0.1)\nplt.title(\"SVM with RBF Kernel\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.show()"
  },
  {
    "objectID": "code/06-lasso-vs-code.html",
    "href": "code/06-lasso-vs-code.html",
    "title": "06-Lasso Code Demo",
    "section": "",
    "text": "Show/Hidelibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\n\n\nShow/Hideprostate &lt;- read.csv(\"../data/prostate.csv\")\nlasso_fit &lt;- cv.glmnet(x = data.matrix(prostate[, 1:8]), \n                       y = prostate$lpsa, nfolds = 10, \n                       alpha = 1)\n\n\n\nShow/Hidelasso_fit$lambda.min\n\n[1] 0.02961435\n\nShow/Hidelasso_fit$lambda.1se\n\n[1] 0.2089234\n\nShow/Hideplot(lasso_fit)\n\n\n\n\n\n\nShow/Hideplot(lasso_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nShow/Hidecoef(lasso_fit, s = \"lambda.min\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  0.169279155\nlcavol       0.509085165\nlweight      0.558643448\nage         -0.010256773\nlbph         0.067418867\nsvi          0.598979114\nlcp          .          \ngleason      0.008139093\npgg45        0.002387475\n\n\n\nShow/Hidecoef(lasso_fit, s = \"lambda.1se\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s1\n(Intercept) 0.7820637\nlcavol      0.4485189\nlweight     0.2804033\nage         .        \nlbph        .        \nsvi         0.3383490\nlcp         .        \ngleason     .        \npgg45       ."
  },
  {
    "objectID": "code/06-lasso-vs-code.html#r-implementation",
    "href": "code/06-lasso-vs-code.html#r-implementation",
    "title": "06-Lasso Code Demo",
    "section": "",
    "text": "Show/Hidelibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\n\n\nShow/Hideprostate &lt;- read.csv(\"../data/prostate.csv\")\nlasso_fit &lt;- cv.glmnet(x = data.matrix(prostate[, 1:8]), \n                       y = prostate$lpsa, nfolds = 10, \n                       alpha = 1)\n\n\n\nShow/Hidelasso_fit$lambda.min\n\n[1] 0.02961435\n\nShow/Hidelasso_fit$lambda.1se\n\n[1] 0.2089234\n\nShow/Hideplot(lasso_fit)\n\n\n\n\n\n\nShow/Hideplot(lasso_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nShow/Hidecoef(lasso_fit, s = \"lambda.min\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  0.169279155\nlcavol       0.509085165\nlweight      0.558643448\nage         -0.010256773\nlbph         0.067418867\nsvi          0.598979114\nlcp          .          \ngleason      0.008139093\npgg45        0.002387475\n\n\n\nShow/Hidecoef(lasso_fit, s = \"lambda.1se\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s1\n(Intercept) 0.7820637\nlcavol      0.4485189\nlweight     0.2804033\nage         .        \nlbph        .        \nsvi         0.3383490\nlcp         .        \ngleason     .        \npgg45       ."
  },
  {
    "objectID": "code/06-lasso-vs-code.html#python-implementation",
    "href": "code/06-lasso-vs-code.html#python-implementation",
    "title": "06-Lasso Code Demo",
    "section": "Python implementation",
    "text": "Python implementation\n\nShow/Hideimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso, LassoCV\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nShow/Hideprostate = pd.read_csv(\"../data/prostate.csv\")\nX = prostate.iloc[:, 0:8]\ny = prostate[\"lpsa\"]\n\n# Standardize predictors\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Perform Lasso regression with cross-validation\n# Automatically selects alphas\nlasso_cv = LassoCV(alphas=None, cv=10, random_state=2025)  \nlasso_cv.fit(X_scaled, y)\n\n\n\n\nLassoCV(cv=10, random_state=2025)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†LassoCV?Documentation for LassoCViFittedLassoCV(cv=10, random_state=2025) \n\n\nShow/Hide# Plot MSE across alphas\nmse_path = np.mean(lasso_cv.mse_path_, axis=1)\n\nplt.plot(np.log(lasso_cv.alphas_), mse_path, marker='o', label=\"Mean MSE\")\nplt.axvline(np.log(lasso_cv.alpha_), color=\"red\", linestyle=\"--\", \n            label=f\"Optimal Lambda: {lasso_cv.alpha_:.4f}\")\nplt.xlabel(\"Log(lambda)\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"Lasso Cross-Validation: MSE vs. Log(lambda)\", fontsize=14)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nShow/Hideoptimal_lambda = lasso_cv.alpha_\noptimal_lambda\n\n0.0008434274382607599\n\nShow/Hidelasso_cv.coef_\n\narray([ 0.65931859,  0.26435421, -0.15492328,  0.13834575,  0.31131745,\n       -0.14188085,  0.03451933,  0.12292875])\n\nShow/Hidelasso_cv.intercept_\n\n2.4783868783505154\n\n\n\nShow/Hide# Reverse standardization\nstd_devs = scaler.scale_  # Feature standard deviations\nmeans = scaler.mean_      # Feature means\ncoef_original = lasso_cv.coef_ / std_devs\ncoef_original\n\narray([ 0.56230244,  0.62026266, -0.02091681,  0.09585318,  0.75589027,\n       -0.10199745,  0.04805015,  0.00438119])\n\nShow/Hideintercept_original = lasso_cv.intercept_ - np.sum(lasso_cv.coef_ * means / std_devs)\nintercept_original\n\n0.18140520823512718\n\n\n\nShow/Hidelambdas = np.logspace(-6.5, 0.1, 100, base=np.exp(1))\n\ncoefficients = []\nfor lam in lambdas:\n    lasso = Lasso(alpha=lam, fit_intercept=True, max_iter=10000)\n    lasso.fit(X_scaled, y)\n    coefficients.append(lasso.coef_)\n\n\n\n\nLasso(alpha=1.1051709180756477, max_iter=10000)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†Lasso?Documentation for LassoiFittedLasso(alpha=1.1051709180756477, max_iter=10000) \n\n\nShow/Hidecoefficients = np.array(coefficients)\n\n# Plot coefficient paths\nfor i in range(coefficients.shape[1]):\n    plt.plot(np.log(lambdas), coefficients[:, i], label=f\"Feature {i+1}\")\n\nplt.xlabel(\"Log(lambda)\")\nplt.ylabel(\"Coefficients\")\nplt.title(\"Lasso Coefficient Paths vs. Log(lambdas)\")\nplt.legend(loc=\"upper right\", frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\nShow/Hidelasso = Lasso(alpha=0.21, fit_intercept=True)\nlasso.fit(X_scaled, y)\n\n\n\n\nLasso(alpha=0.21)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†Lasso?Documentation for LassoiFittedLasso(alpha=0.21) \n\n\nShow/Hidelasso.coef_\n\narray([0.52543798, 0.1186707 , 0.        , 0.        , 0.13865508,\n       0.        , 0.        , 0.        ])"
  },
  {
    "objectID": "code/16-clustering-code.html",
    "href": "code/16-clustering-code.html",
    "title": "16 - Clustering Code Demo",
    "section": "",
    "text": "Codehead(iris, 10)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\n\n\n\nCodedf_clust &lt;- iris[, 3:4]\n(iris_kmean &lt;- kmeans(df_clust, centers = 3, nstart = 20))\n\nK-means clustering with 3 clusters of sizes 50, 48, 52\n\nCluster means:\n  Petal.Length Petal.Width\n1     1.462000    0.246000\n2     5.595833    2.037500\n3     4.269231    1.342308\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [75] 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\nWithin cluster sum of squares by cluster:\n[1]  2.02200 16.29167 13.05769\n (between_SS / total_SS =  94.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nCodelibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nCodefviz_cluster(object = iris_kmean, data = df_clust, label = NA)\n\n\n\n\n\n\n\n\nCode## wss =  total within sum of squares\nfviz_nbclust(x = df_clust, FUNcluster = kmeans, method = \"wss\",  k.max = 6)\n\n\n\n\n\n\nCodefviz_nbclust(x = df_clust, FUNcluster = kmeans, method = \"silhouette\",  k.max = 6)\n\n\n\n\n\n\nCodefviz_nbclust(df_clust, kmeans, method = \"gap_stat\",  k.max = 6)\n\n\n\n\n\n\n\n\n\nCodelibrary(mclust)\n\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nCodegmm_clus &lt;- Mclust(df_clust)\npar(mar = c(4, 4, 0, 0))\nplot(gmm_clus, what = \"classification\")"
  },
  {
    "objectID": "code/16-clustering-code.html#k-means",
    "href": "code/16-clustering-code.html#k-means",
    "title": "16 - Clustering Code Demo",
    "section": "",
    "text": "Codedf_clust &lt;- iris[, 3:4]\n(iris_kmean &lt;- kmeans(df_clust, centers = 3, nstart = 20))\n\nK-means clustering with 3 clusters of sizes 50, 48, 52\n\nCluster means:\n  Petal.Length Petal.Width\n1     1.462000    0.246000\n2     5.595833    2.037500\n3     4.269231    1.342308\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [75] 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\nWithin cluster sum of squares by cluster:\n[1]  2.02200 16.29167 13.05769\n (between_SS / total_SS =  94.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nCodelibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nCodefviz_cluster(object = iris_kmean, data = df_clust, label = NA)\n\n\n\n\n\n\n\n\nCode## wss =  total within sum of squares\nfviz_nbclust(x = df_clust, FUNcluster = kmeans, method = \"wss\",  k.max = 6)\n\n\n\n\n\n\nCodefviz_nbclust(x = df_clust, FUNcluster = kmeans, method = \"silhouette\",  k.max = 6)\n\n\n\n\n\n\nCodefviz_nbclust(df_clust, kmeans, method = \"gap_stat\",  k.max = 6)"
  },
  {
    "objectID": "code/16-clustering-code.html#gaussian-mixture-model-gmm-based-clustering",
    "href": "code/16-clustering-code.html#gaussian-mixture-model-gmm-based-clustering",
    "title": "16 - Clustering Code Demo",
    "section": "",
    "text": "Codelibrary(mclust)\n\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nCodegmm_clus &lt;- Mclust(df_clust)\npar(mar = c(4, 4, 0, 0))\nplot(gmm_clus, what = \"classification\")"
  },
  {
    "objectID": "code/16-clustering-code.html#k-means-1",
    "href": "code/16-clustering-code.html#k-means-1",
    "title": "16 - Clustering Code Demo",
    "section": "\n2.1 K-Means",
    "text": "2.1 K-Means\n\nCodeimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nCodefrom sklearn.datasets import load_iris\niris = load_iris(as_frame=True)\n\n\n\nCodescaler = StandardScaler()\nX = scaler.fit_transform(iris.data.iloc[:, [2, 3]])\n\n\n\nCodekmeans = KMeans(n_clusters=3,  n_init=10).fit(X)\nkmeans.labels_[0:10]\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n\nCodekmeans.cluster_centers_\n\narray([[ 1.02799959,  1.12797813],\n       [-1.30498732, -1.25489349],\n       [ 0.3058728 ,  0.16541778]])\n\n\n\nCode# y_pred = KMeans(n_clusters=3,  n_init=10).fit_predict(X)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 6))\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.tight_layout()"
  },
  {
    "objectID": "code/16-clustering-code.html#gaussian-mixture-model-gmm-based-clustering-1",
    "href": "code/16-clustering-code.html#gaussian-mixture-model-gmm-based-clustering-1",
    "title": "16 - Clustering Code Demo",
    "section": "\n2.2 Gaussian-Mixture-Model (GMM) based Clustering",
    "text": "2.2 Gaussian-Mixture-Model (GMM) based Clustering\n\nCodefrom sklearn.mixture import GaussianMixture\n\nbic = []\nmodels = []\nfor k in range(1, 10):  # test up to 9 clusters\n    model = GaussianMixture(n_components=k, covariance_type='full', random_state=2025)\n    model.fit(X)\n    bic.append(model.bic(X))\n    models.append(model)\n\n\n\n\nGaussianMixture(n_components=9, random_state=2025)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†GaussianMixture?Documentation for GaussianMixtureiFittedGaussianMixture(n_components=9, random_state=2025) \n\n\nCode# Choose model with lowest BIC\nbest_model = models[np.argmin(bic)]\nbest_model\n\n\n\n\nGaussianMixture(n_components=3, random_state=2025)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†GaussianMixture?Documentation for GaussianMixtureiFittedGaussianMixture(n_components=3, random_state=2025) \n\n\nCodelabels = best_model.predict(X)\nlabels\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n\nCodeplt.figure(figsize=(8, 6))\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.tight_layout()"
  },
  {
    "objectID": "code/15-pca-code.html",
    "href": "code/15-pca-code.html",
    "title": "15 - Principal Component Analysis Code Demo",
    "section": "",
    "text": "1 R implementation\n\nCodeUSArrests\n\n               Murder Assault UrbanPop Rape\nAlabama          13.2     236       58 21.2\nAlaska           10.0     263       48 44.5\nArizona           8.1     294       80 31.0\nArkansas          8.8     190       50 19.5\nCalifornia        9.0     276       91 40.6\nColorado          7.9     204       78 38.7\nConnecticut       3.3     110       77 11.1\nDelaware          5.9     238       72 15.8\nFlorida          15.4     335       80 31.9\nGeorgia          17.4     211       60 25.8\nHawaii            5.3      46       83 20.2\nIdaho             2.6     120       54 14.2\nIllinois         10.4     249       83 24.0\nIndiana           7.2     113       65 21.0\nIowa              2.2      56       57 11.3\nKansas            6.0     115       66 18.0\nKentucky          9.7     109       52 16.3\nLouisiana        15.4     249       66 22.2\nMaine             2.1      83       51  7.8\nMaryland         11.3     300       67 27.8\nMassachusetts     4.4     149       85 16.3\nMichigan         12.1     255       74 35.1\nMinnesota         2.7      72       66 14.9\nMississippi      16.1     259       44 17.1\nMissouri          9.0     178       70 28.2\nMontana           6.0     109       53 16.4\nNebraska          4.3     102       62 16.5\nNevada           12.2     252       81 46.0\nNew Hampshire     2.1      57       56  9.5\nNew Jersey        7.4     159       89 18.8\nNew Mexico       11.4     285       70 32.1\nNew York         11.1     254       86 26.1\nNorth Carolina   13.0     337       45 16.1\nNorth Dakota      0.8      45       44  7.3\nOhio              7.3     120       75 21.4\nOklahoma          6.6     151       68 20.0\nOregon            4.9     159       67 29.3\nPennsylvania      6.3     106       72 14.9\nRhode Island      3.4     174       87  8.3\nSouth Carolina   14.4     279       48 22.5\nSouth Dakota      3.8      86       45 12.8\nTennessee        13.2     188       59 26.9\nTexas            12.7     201       80 25.5\nUtah              3.2     120       80 22.9\nVermont           2.2      48       32 11.2\nVirginia          8.5     156       63 20.7\nWashington        4.0     145       73 26.2\nWest Virginia     5.7      81       39  9.3\nWisconsin         2.6      53       66 10.8\nWyoming           6.8     161       60 15.6\n\n\n\nCodepca_output &lt;- prcomp(USArrests, scale = TRUE)\n\n\n\nCode(pca_output$rotation &lt;- -pca_output$rotation)\n\n               PC1        PC2        PC3         PC4\nMurder   0.5358995  0.4181809 -0.3412327 -0.64922780\nAssault  0.5831836  0.1879856 -0.2681484  0.74340748\nUrbanPop 0.2781909 -0.8728062 -0.3780158 -0.13387773\nRape     0.5434321 -0.1673186  0.8177779 -0.08902432\n\n\n\nCodepca_output$x\n\n                       PC1         PC2         PC3          PC4\nAlabama        -0.97566045 -1.12200121  0.43980366  0.154696581\nAlaska         -1.93053788 -1.06242692 -2.01950027 -0.434175454\nArizona        -1.74544285  0.73845954 -0.05423025 -0.826264240\nArkansas        0.13999894 -1.10854226 -0.11342217 -0.180973554\nCalifornia     -2.49861285  1.52742672 -0.59254100 -0.338559240\nColorado       -1.49934074  0.97762966 -1.08400162  0.001450164\nConnecticut     1.34499236  1.07798362  0.63679250 -0.117278736\nDelaware       -0.04722981  0.32208890  0.71141032 -0.873113315\nFlorida        -2.98275967 -0.03883425  0.57103206 -0.095317042\nGeorgia        -1.62280742 -1.26608838  0.33901818  1.065974459\nHawaii          0.90348448  1.55467609 -0.05027151  0.893733198\nIdaho           1.62331903 -0.20885253 -0.25719021 -0.494087852\nIllinois       -1.36505197  0.67498834  0.67068647 -0.120794916\nIndiana         0.50038122  0.15003926 -0.22576277  0.420397595\nIowa            2.23099579  0.10300828 -0.16291036  0.017379470\nKansas          0.78887206  0.26744941 -0.02529648  0.204421034\nKentucky        0.74331256 -0.94880748  0.02808429  0.663817237\nLouisiana      -1.54909076 -0.86230011  0.77560598  0.450157791\nMaine           2.37274014 -0.37260865  0.06502225 -0.327138529\nMaryland       -1.74564663 -0.42335704  0.15566968 -0.553450589\nMassachusetts   0.48128007  1.45967706  0.60337172 -0.177793902\nMichigan       -2.08725025  0.15383500 -0.38100046  0.101343128\nMinnesota       1.67566951  0.62590670 -0.15153200  0.066640316\nMississippi    -0.98647919 -2.36973712  0.73336290  0.213342049\nMissouri       -0.68978426  0.26070794 -0.37365033  0.223554811\nMontana         1.17353751 -0.53147851 -0.24440796  0.122498555\nNebraska        1.25291625  0.19200440 -0.17380930  0.015733156\nNevada         -2.84550542  0.76780502 -1.15168793  0.311354436\nNew Hampshire   2.35995585  0.01790055 -0.03648498 -0.032804291\nNew Jersey     -0.17974128  1.43493745  0.75677041  0.240936580\nNew Mexico     -1.96012351 -0.14141308 -0.18184598 -0.336121113\nNew York       -1.66566662  0.81491072  0.63661186 -0.013348844\nNorth Carolina -1.11208808 -2.20561081  0.85489245 -0.944789648\nNorth Dakota    2.96215223 -0.59309738 -0.29824930 -0.251434626\nOhio            0.22369436  0.73477837  0.03082616  0.469152817\nOklahoma        0.30864928  0.28496113  0.01515592  0.010228476\nOregon         -0.05852787  0.53596999 -0.93038718 -0.235390872\nPennsylvania    0.87948680  0.56536050  0.39660218  0.355452378\nRhode Island    0.85509072  1.47698328  1.35617705 -0.607402746\nSouth Carolina -1.30744986 -1.91397297  0.29751723 -0.130145378\nSouth Dakota    1.96779669 -0.81506822 -0.38538073 -0.108470512\nTennessee      -0.98969377 -0.85160534 -0.18619262  0.646302674\nTexas          -1.34151838  0.40833518  0.48712332  0.636731051\nUtah            0.54503180  1.45671524 -0.29077592 -0.081486749\nVermont         2.77325613 -1.38819435 -0.83280797 -0.143433697\nVirginia        0.09536670 -0.19772785 -0.01159482  0.209246429\nWashington      0.21472339  0.96037394 -0.61859067 -0.218628161\nWest Virginia   2.08739306 -1.41052627 -0.10372163  0.130583080\nWisconsin       2.05881199  0.60512507  0.13746933  0.182253407\nWyoming         0.62310061 -0.31778662  0.23824049 -0.164976866\n\n\n\nCodebiplot(pca_output, xlabs = state.abb, scale = 0,\n       col = c(\"blue\", \"red\"), las = 1,\n       xlab = \"PC1 score\", ylab = \"PC2 score\")\n\n\n\n\n\n\n\n\nCode(pc_var &lt;- pca_output$sdev ^ 2)\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\nCode(pc_var_prop &lt;- pc_var / sum(pc_var))\n\n[1] 0.62006039 0.24744129 0.08914080 0.04335752\n\n\n\n2 Python implementation\nThe biplot in Python needs extra work. We need to either write our own function or rely on some other packages such as pca.\n\nCodeimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nCodeUSArrests = pd.read_csv('../data/USArrests.csv')\n\n\n\nCodeUSArr = USArrests.drop(['rownames'], axis = 1)\nUSArr.index = USArrests['rownames']\nUSArr\n\n                Murder  Assault  UrbanPop  Rape\nrownames                                       \nAlabama           13.2      236        58  21.2\nAlaska            10.0      263        48  44.5\nArizona            8.1      294        80  31.0\nArkansas           8.8      190        50  19.5\nCalifornia         9.0      276        91  40.6\nColorado           7.9      204        78  38.7\nConnecticut        3.3      110        77  11.1\nDelaware           5.9      238        72  15.8\nFlorida           15.4      335        80  31.9\nGeorgia           17.4      211        60  25.8\nHawaii             5.3       46        83  20.2\nIdaho              2.6      120        54  14.2\nIllinois          10.4      249        83  24.0\nIndiana            7.2      113        65  21.0\nIowa               2.2       56        57  11.3\nKansas             6.0      115        66  18.0\nKentucky           9.7      109        52  16.3\nLouisiana         15.4      249        66  22.2\nMaine              2.1       83        51   7.8\nMaryland          11.3      300        67  27.8\nMassachusetts      4.4      149        85  16.3\nMichigan          12.1      255        74  35.1\nMinnesota          2.7       72        66  14.9\nMississippi       16.1      259        44  17.1\nMissouri           9.0      178        70  28.2\nMontana            6.0      109        53  16.4\nNebraska           4.3      102        62  16.5\nNevada            12.2      252        81  46.0\nNew Hampshire      2.1       57        56   9.5\nNew Jersey         7.4      159        89  18.8\nNew Mexico        11.4      285        70  32.1\nNew York          11.1      254        86  26.1\nNorth Carolina    13.0      337        45  16.1\nNorth Dakota       0.8       45        44   7.3\nOhio               7.3      120        75  21.4\nOklahoma           6.6      151        68  20.0\nOregon             4.9      159        67  29.3\nPennsylvania       6.3      106        72  14.9\nRhode Island       3.4      174        87   8.3\nSouth Carolina    14.4      279        48  22.5\nSouth Dakota       3.8       86        45  12.8\nTennessee         13.2      188        59  26.9\nTexas             12.7      201        80  25.5\nUtah               3.2      120        80  22.9\nVermont            2.2       48        32  11.2\nVirginia           8.5      156        63  20.7\nWashington         4.0      145        73  26.2\nWest Virginia      5.7       81        39   9.3\nWisconsin          2.6       53        66  10.8\nWyoming            6.8      161        60  15.6\n\n\n\nCodescaler = StandardScaler()\nX = scaler.fit_transform(USArr.values) ## Array\n\n\n\nCodepca = PCA(n_components=4)\npca.fit(X)\n\n\n\n\nPCA(n_components=4)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†PCA?Documentation for PCAiFittedPCA(n_components=4) \n\n\n\n\nCodepd.DataFrame(pca.components_.T, \n             columns=['PC1', 'PC2', 'PC3', 'PC4'],\n             index=USArr.columns)\n\n               PC1       PC2       PC3       PC4\nMurder    0.535899  0.418181 -0.341233  0.649228\nAssault   0.583184  0.187986 -0.268148 -0.743407\nUrbanPop  0.278191 -0.872806 -0.378016  0.133878\nRape      0.543432 -0.167319  0.817778  0.089024\n\n\n\nCodepd.DataFrame(pca.transform(X), \n             columns=['PC1', 'PC2', 'PC3', 'PC4'], \n             index=USArr.index)\n\n                     PC1       PC2       PC3       PC4\nrownames                                              \nAlabama         0.985566  1.133392 -0.444269  0.156267\nAlaska          1.950138  1.073213  2.040003 -0.438583\nArizona         1.763164 -0.745957  0.054781 -0.834653\nArkansas       -0.141420  1.119797  0.114574 -0.182811\nCalifornia      2.523980 -1.542934  0.598557 -0.341996\nColorado        1.514563 -0.987555  1.095007  0.001465\nConnecticut    -1.358647 -1.088928 -0.643258 -0.118469\nDelaware        0.047709 -0.325359 -0.718633 -0.881978\nFlorida         3.013042  0.039229 -0.576829 -0.096285\nGeorgia         1.639283  1.278942 -0.342460  1.076797\nHawaii         -0.912657 -1.570460  0.050782  0.902807\nIdaho          -1.639800  0.210973  0.259801 -0.499104\nIllinois        1.378911 -0.681841 -0.677496 -0.122021\nIndiana        -0.505461 -0.151563  0.228055  0.424666\nIowa           -2.253646 -0.104054  0.164564  0.017556\nKansas         -0.796881 -0.270165  0.025553  0.206496\nKentucky       -0.750859  0.958440 -0.028369  0.670557\nLouisiana       1.564818  0.871055 -0.783480  0.454728\nMaine          -2.396829  0.376392 -0.065682 -0.330460\nMaryland        1.763369  0.427655 -0.157250 -0.559070\nMassachusetts  -0.486166 -1.474496 -0.609497 -0.179599\nMichigan        2.108441 -0.155397  0.384869  0.102372\nMinnesota      -1.692682 -0.632261  0.153070  0.067317\nMississippi     0.996494  2.393796 -0.740808  0.215508\nMissouri        0.696787 -0.263355  0.377444  0.225824\nMontana        -1.185452  0.536874  0.246889  0.123742\nNebraska       -1.265637 -0.193954  0.175574  0.015893\nNevada          2.874395 -0.775600  1.163380  0.314515\nNew Hampshire  -2.383915 -0.018082  0.036855 -0.033137\nNew Jersey      0.181566 -1.449506 -0.764454  0.243383\nNew Mexico      1.980024  0.142849  0.183692 -0.339534\nNew York        1.682577 -0.823184 -0.643075 -0.013484\nNorth Carolina  1.123379  2.228003 -0.863572 -0.954382\nNorth Dakota   -2.992226  0.599119  0.301277 -0.253987\nOhio           -0.225965 -0.742238 -0.031139  0.473916\nOklahoma       -0.311783 -0.287854 -0.015310  0.010332\nOregon          0.059122 -0.541411  0.939833 -0.237781\nPennsylvania   -0.888416 -0.571100 -0.400629  0.359061\nRhode Island   -0.863772 -1.491978 -1.369946 -0.613569\nSouth Carolina  1.320724  1.933405 -0.300538 -0.131467\nSouth Dakota   -1.987775  0.823343  0.389293 -0.109572\nTennessee       0.999742  0.860251  0.188083  0.652864\nTexas           1.355138 -0.412481 -0.492069  0.643195\nUtah           -0.550565 -1.471505  0.293728 -0.082314\nVermont        -2.801412  1.402288  0.841263 -0.144890\nVirginia       -0.096335  0.199735  0.011713  0.211371\nWashington     -0.216903 -0.970124  0.624871 -0.220848\nWest Virginia  -2.108585  1.424847  0.104775  0.131909\nWisconsin      -2.079714 -0.611269 -0.138865  0.184104\nWyoming        -0.629427  0.321013 -0.240659 -0.166652\n\n\n\nCodepca.explained_variance_\n\narray([2.53085875, 1.00996444, 0.36383998, 0.17696948])"
  },
  {
    "objectID": "code/14-tree-code.html",
    "href": "code/14-tree-code.html",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Code# generate some data \nset.seed(2025)\nn &lt;- 1000\nx1 &lt;- runif(n, -1, 1)\nx2 &lt;- runif(n, -1, 1)\ny &lt;- rbinom(n, size = 1, prob = ifelse((x1 + x2 &gt; -0.5) & (x1 + x2 &lt; 0.5) , 0.8, 0.2))\nxgrid &lt;- expand.grid(x1 = seq(-1, 1, 0.01), x2 = seq(-1, 1, 0.01))\n\n\n\n\n\nCodelibrary(rpart)\nrpart_fit &lt;- rpart(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y))\npred &lt;- matrix(predict(rpart_fit, xgrid, type = \"class\") == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"CART\"))\n\n\n\n\n\n\n\n\nCodepar(mar = c(0.5, 0, 0.5, 0))\nplot(rpart_fit)\ntext(rpart_fit, cex = 0.8)\n\n\n\n\n\n\n\n\n\nrpart() uses the 10-fold CV (xval in rpart.control())\n\ncp is the complexity parameter\n\n\nCoderpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, \n              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,\n              surrogatestyle = 0, maxdepth = 30, ...)\n\n\n\nCoderpart_fit$cptable\n\n          CP nsplit rel error    xerror       xstd\n1 0.06485356      0 1.0000000 1.0000000 0.03304618\n2 0.05857741      3 0.7845188 0.9058577 0.03277990\n3 0.05439331      4 0.7259414 0.8284519 0.03235476\n4 0.03242678      5 0.6715481 0.7050209 0.03127115\n5 0.02719665      9 0.5418410 0.6401674 0.03048685\n6 0.01464435     10 0.5146444 0.6087866 0.03004981\n7 0.01359833     11 0.5000000 0.5732218 0.02950636\n8 0.01000000     13 0.4728033 0.5732218 0.02950636\n\n\n\nCodeplotcp(rpart_fit)\n\n\n\n\n\n\n\n\nCodeprunedtree &lt;- prune(rpart_fit, cp = 0.012)\nprunedtree\n\nn= 1000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 1000 478 0 (0.5220000 0.4780000)  \n    2) x1&gt;=0.8238083 93  26 0 (0.7204301 0.2795699)  \n      4) x2&gt;=-0.4195459 74  10 0 (0.8648649 0.1351351) *\n      5) x2&lt; -0.4195459 19   3 1 (0.1578947 0.8421053) *\n    3) x1&lt; 0.8238083 907 452 0 (0.5016538 0.4983462)  \n      6) x2&lt; -0.546795 203  69 0 (0.6600985 0.3399015)  \n       12) x1&lt; 0.3121958 145  27 0 (0.8137931 0.1862069) *\n       13) x1&gt;=0.3121958 58  16 1 (0.2758621 0.7241379) *\n      7) x2&gt;=-0.546795 704 321 1 (0.4559659 0.5440341)  \n       14) x2&gt;=0.5828916 189  74 0 (0.6084656 0.3915344)  \n         28) x1&gt;=-0.1670433 115  23 0 (0.8000000 0.2000000) *\n         29) x1&lt; -0.1670433 74  23 1 (0.3108108 0.6891892) *\n       15) x2&lt; 0.5828916 515 206 1 (0.4000000 0.6000000)  \n         30) x1&lt; -0.9438582 17   2 0 (0.8823529 0.1176471) *\n         31) x1&gt;=-0.9438582 498 191 1 (0.3835341 0.6164659)  \n           62) x1&gt;=0.192903 169  79 1 (0.4674556 0.5325444)  \n            124) x2&gt;=-0.1156255 97  33 0 (0.6597938 0.3402062) *\n            125) x2&lt; -0.1156255 72  15 1 (0.2083333 0.7916667) *\n           63) x1&lt; 0.192903 329 112 1 (0.3404255 0.6595745)  \n            126) x2&lt; -0.2736896 80  31 0 (0.6125000 0.3875000)  \n              252) x1&lt; -0.05781444 57  16 0 (0.7192982 0.2807018) *\n              253) x1&gt;=-0.05781444 23   8 1 (0.3478261 0.6521739) *\n            127) x2&gt;=-0.2736896 249  63 1 (0.2530120 0.7469880)  \n              254) x1&lt; -0.568942 82  34 1 (0.4146341 0.5853659)  \n                508) x2&lt; 0.2270988 39  13 0 (0.6666667 0.3333333) *\n                509) x2&gt;=0.2270988 43   8 1 (0.1860465 0.8139535) *\n              255) x1&gt;=-0.568942 167  29 1 (0.1736527 0.8263473) *\n\n\n\nCoderpart.plot::rpart.plot(prunedtree)\n\n\n\n\n\n\n\n\nRead ISLR Sec 8.3 for tree() demo.\n\n\nCodelibrary(ipred)\nbag_fit &lt;- bagging(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y), \n                   nbagg = 200, ns = 400)\npred &lt;- matrix(predict(prune(bag_fit), xgrid) == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\", \n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Bagging\"))\n\n\n\n\n\n\n\n\n\nCodelibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nCoderf_fit &lt;- randomForest(cbind(x1, x2), as.factor(y), ntree = 200, mtry = 1, \n                      nodesize = 20, sampsize = 400)\npred &lt;- matrix(predict(rf_fit, xgrid) == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt=\"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Random Forests\", cex = 2))\n\n\n\n\n\n\n\n\n\nCodelibrary(gbm)\n\nLoaded gbm 2.2.2\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nCodegbm_fit &lt;- gbm(y ~ ., data = data.frame(x1, x2, y), distribution = \"bernoulli\", \n               n.trees = 10000, shrinkage = 0.01, bag.fraction = 0.6, \n               interaction.depth = 2, cv.folds = 10)\nusetree &lt;- gbm.perf(gbm_fit, method = \"cv\", plot.it = FALSE)\nFx &lt;- predict(gbm_fit, xgrid, n.trees=usetree)\npred &lt;- matrix(1 / (1 + exp(-2 * Fx)) &gt; 0.5, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Boosting\"))"
  },
  {
    "objectID": "code/14-tree-code.html#cart",
    "href": "code/14-tree-code.html#cart",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Codelibrary(rpart)\nrpart_fit &lt;- rpart(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y))\npred &lt;- matrix(predict(rpart_fit, xgrid, type = \"class\") == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"CART\"))\n\n\n\n\n\n\n\n\nCodepar(mar = c(0.5, 0, 0.5, 0))\nplot(rpart_fit)\ntext(rpart_fit, cex = 0.8)\n\n\n\n\n\n\n\n\n\nrpart() uses the 10-fold CV (xval in rpart.control())\n\ncp is the complexity parameter\n\n\nCoderpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, \n              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,\n              surrogatestyle = 0, maxdepth = 30, ...)\n\n\n\nCoderpart_fit$cptable\n\n          CP nsplit rel error    xerror       xstd\n1 0.06485356      0 1.0000000 1.0000000 0.03304618\n2 0.05857741      3 0.7845188 0.9058577 0.03277990\n3 0.05439331      4 0.7259414 0.8284519 0.03235476\n4 0.03242678      5 0.6715481 0.7050209 0.03127115\n5 0.02719665      9 0.5418410 0.6401674 0.03048685\n6 0.01464435     10 0.5146444 0.6087866 0.03004981\n7 0.01359833     11 0.5000000 0.5732218 0.02950636\n8 0.01000000     13 0.4728033 0.5732218 0.02950636\n\n\n\nCodeplotcp(rpart_fit)\n\n\n\n\n\n\n\n\nCodeprunedtree &lt;- prune(rpart_fit, cp = 0.012)\nprunedtree\n\nn= 1000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 1000 478 0 (0.5220000 0.4780000)  \n    2) x1&gt;=0.8238083 93  26 0 (0.7204301 0.2795699)  \n      4) x2&gt;=-0.4195459 74  10 0 (0.8648649 0.1351351) *\n      5) x2&lt; -0.4195459 19   3 1 (0.1578947 0.8421053) *\n    3) x1&lt; 0.8238083 907 452 0 (0.5016538 0.4983462)  \n      6) x2&lt; -0.546795 203  69 0 (0.6600985 0.3399015)  \n       12) x1&lt; 0.3121958 145  27 0 (0.8137931 0.1862069) *\n       13) x1&gt;=0.3121958 58  16 1 (0.2758621 0.7241379) *\n      7) x2&gt;=-0.546795 704 321 1 (0.4559659 0.5440341)  \n       14) x2&gt;=0.5828916 189  74 0 (0.6084656 0.3915344)  \n         28) x1&gt;=-0.1670433 115  23 0 (0.8000000 0.2000000) *\n         29) x1&lt; -0.1670433 74  23 1 (0.3108108 0.6891892) *\n       15) x2&lt; 0.5828916 515 206 1 (0.4000000 0.6000000)  \n         30) x1&lt; -0.9438582 17   2 0 (0.8823529 0.1176471) *\n         31) x1&gt;=-0.9438582 498 191 1 (0.3835341 0.6164659)  \n           62) x1&gt;=0.192903 169  79 1 (0.4674556 0.5325444)  \n            124) x2&gt;=-0.1156255 97  33 0 (0.6597938 0.3402062) *\n            125) x2&lt; -0.1156255 72  15 1 (0.2083333 0.7916667) *\n           63) x1&lt; 0.192903 329 112 1 (0.3404255 0.6595745)  \n            126) x2&lt; -0.2736896 80  31 0 (0.6125000 0.3875000)  \n              252) x1&lt; -0.05781444 57  16 0 (0.7192982 0.2807018) *\n              253) x1&gt;=-0.05781444 23   8 1 (0.3478261 0.6521739) *\n            127) x2&gt;=-0.2736896 249  63 1 (0.2530120 0.7469880)  \n              254) x1&lt; -0.568942 82  34 1 (0.4146341 0.5853659)  \n                508) x2&lt; 0.2270988 39  13 0 (0.6666667 0.3333333) *\n                509) x2&gt;=0.2270988 43   8 1 (0.1860465 0.8139535) *\n              255) x1&gt;=-0.568942 167  29 1 (0.1736527 0.8263473) *\n\n\n\nCoderpart.plot::rpart.plot(prunedtree)\n\n\n\n\n\n\n\n\nRead ISLR Sec 8.3 for tree() demo."
  },
  {
    "objectID": "code/14-tree-code.html#bagging",
    "href": "code/14-tree-code.html#bagging",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Codelibrary(ipred)\nbag_fit &lt;- bagging(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y), \n                   nbagg = 200, ns = 400)\npred &lt;- matrix(predict(prune(bag_fit), xgrid) == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\", \n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Bagging\"))"
  },
  {
    "objectID": "code/14-tree-code.html#random-forests",
    "href": "code/14-tree-code.html#random-forests",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Codelibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nCoderf_fit &lt;- randomForest(cbind(x1, x2), as.factor(y), ntree = 200, mtry = 1, \n                      nodesize = 20, sampsize = 400)\npred &lt;- matrix(predict(rf_fit, xgrid) == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt=\"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Random Forests\", cex = 2))"
  },
  {
    "objectID": "code/14-tree-code.html#boosting",
    "href": "code/14-tree-code.html#boosting",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Codelibrary(gbm)\n\nLoaded gbm 2.2.2\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nCodegbm_fit &lt;- gbm(y ~ ., data = data.frame(x1, x2, y), distribution = \"bernoulli\", \n               n.trees = 10000, shrinkage = 0.01, bag.fraction = 0.6, \n               interaction.depth = 2, cv.folds = 10)\nusetree &lt;- gbm.perf(gbm_fit, method = \"cv\", plot.it = FALSE)\nFx &lt;- predict(gbm_fit, xgrid, n.trees=usetree)\npred &lt;- matrix(1 / (1 + exp(-2 * Fx)) &gt; 0.5, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Boosting\"))"
  },
  {
    "objectID": "code/14-tree-code.html#cart-1",
    "href": "code/14-tree-code.html#cart-1",
    "title": "14 - Tree Methods Code Demo",
    "section": "\n2.1 CART",
    "text": "2.1 CART\n\nCodedt = DecisionTreeClassifier(random_state=2025)\nparam_grid = {\n    \"max_depth\": [3, 5, 7, 10],  # Control tree depth\n    \"min_samples_split\": [2, 5, 10],  # Minimum samples to split a node\n    \"min_samples_leaf\": [1, 5, 10]  # Minimum samples in a leaf node\n}\ngrid_search = GridSearchCV(dt, param_grid, cv=10, scoring=\"accuracy\")\ngrid_search.fit(np.c_[x1, x2], y)\n\n\n\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=2025),\n             param_grid={'max_depth': [3, 5, 7, 10],\n                         'min_samples_leaf': [1, 5, 10],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=2025),\n             param_grid={'max_depth': [3, 5, 7, 10],\n                         'min_samples_leaf': [1, 5, 10],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy') \n\n\nestimator: DecisionTreeClassifierDecisionTreeClassifier(random_state=2025) \n\n¬†DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(random_state=2025) \n\n\n\n\nCodebest_dt = grid_search.best_estimator_\nbest_pred = best_dt.predict(xgrid).reshape(201, 201)\n\nplt.figure()\nplt.contourf(x1_grid, x2_grid, best_pred, cmap=\"coolwarm\", alpha=0.3)\nplt.scatter(x1, x2, c=y, cmap=\"bwr\", s=15, alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"CART\")\nplt.show()"
  },
  {
    "objectID": "code/14-tree-code.html#bagging-1",
    "href": "code/14-tree-code.html#bagging-1",
    "title": "14 - Tree Methods Code Demo",
    "section": "\n2.2 Bagging",
    "text": "2.2 Bagging\n\nCodefrom sklearn.ensemble import BaggingClassifier\n\n\n\nCodebagging_clf = BaggingClassifier(\n    estimator=DecisionTreeClassifier(),\n    n_estimators=200,  # Same as nbagg = 200 in R\n    max_samples=0.4,   # Equivalent to ns = 400 in R (approx)\n    random_state=2025\n)\nbagging_clf.fit(np.c_[x1, x2], y)\n\n\n\n\nBaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=0.4,\n                  n_estimators=200, random_state=2025)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n¬†¬†BaggingClassifier?Documentation for BaggingClassifieriFittedBaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=0.4,\n                  n_estimators=200, random_state=2025) \n\n\nestimator: DecisionTreeClassifierDecisionTreeClassifier() \n\n¬†DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n\nCodebagging_pred = bagging_clf.predict(xgrid).reshape(201, 201)\nplt.figure()\nplt.contourf(x1_grid, x2_grid, bagging_pred, cmap=\"coolwarm\", alpha=0.3)\nplt.scatter(x1, x2, c=y, cmap=\"bwr\", s=15, alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Bagging\")\nplt.show()"
  },
  {
    "objectID": "code/14-tree-code.html#random-forests-1",
    "href": "code/14-tree-code.html#random-forests-1",
    "title": "14 - Tree Methods Code Demo",
    "section": "\n2.3 Random Forests",
    "text": "2.3 Random Forests\n\nCodefrom sklearn.ensemble import RandomForestClassifier\n\n\n\nCoderf_clf = RandomForestClassifier(\n    n_estimators=200,  # Equivalent to ntree = 200 in R\n    max_features=1,    # Equivalent to mtry = 1 in R\n    min_samples_leaf=20,  # Equivalent to nodesize = 20 in R\n    max_samples=400,  # Equivalent to sampsize = 400 in R\n    random_state=2025\n)\nrf_clf.fit(np.c_[x1, x2], y)\n\n\n\n\nRandomForestClassifier(max_features=1, max_samples=400, min_samples_leaf=20,\n                       n_estimators=200, random_state=2025)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(max_features=1, max_samples=400, min_samples_leaf=20,\n                       n_estimators=200, random_state=2025) \n\n\nCoderf_pred = rf_clf.predict(xgrid).reshape(201, 201)\nplt.figure()\nplt.contourf(x1_grid, x2_grid, rf_pred, cmap=\"coolwarm\", alpha=0.3)\nplt.scatter(x1, x2, c=y, cmap=\"bwr\", s=15, alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Random Forests\")\nplt.show()"
  },
  {
    "objectID": "code/14-tree-code.html#boosting-1",
    "href": "code/14-tree-code.html#boosting-1",
    "title": "14 - Tree Methods Code Demo",
    "section": "\n2.4 Boosting",
    "text": "2.4 Boosting\n\nCodefrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\n\nCodegbm_clf = GradientBoostingClassifier(\n    n_estimators=10000,  # Equivalent to n.trees = 10000\n    learning_rate=0.01,  # Equivalent to shrinkage = 0.01\n    subsample=0.6,       # Equivalent to bag.fraction = 0.6\n    max_depth=2,         # Equivalent to interaction.depth = 2\n    random_state=2025\n)\ncv_scores = []\nfor n_trees in range(100, 2000, 100):  # Testing tree counts\n    gbm_clf.set_params(n_estimators=n_trees)\n    score = np.mean(cross_val_score(gbm_clf, np.c_[x1, x2], y, cv=10, scoring='accuracy'))\n    cv_scores.append((n_trees, score))\n\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=1900,\n                           random_state=2025, subsample=0.6)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†GradientBoostingClassifier?Documentation for GradientBoostingClassifieriNot fittedGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=1900,\n                           random_state=2025, subsample=0.6) \n\n\nCodebest_n_trees = max(cv_scores, key=lambda x: x[1])[0]\ngbm_clf.set_params(n_estimators=best_n_trees)\n\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=500,\n                           random_state=2025, subsample=0.6)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†GradientBoostingClassifier?Documentation for GradientBoostingClassifieriNot fittedGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=500,\n                           random_state=2025, subsample=0.6) \n\n\nCodegbm_clf.fit(np.c_[x1, x2], y)\n\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=500,\n                           random_state=2025, subsample=0.6)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=500,\n                           random_state=2025, subsample=0.6) \n\n\nCodeFx = gbm_clf.decision_function(xgrid)  # Raw output before sigmoid transformation\npred = (1 / (1 + np.exp(-2 * Fx)) &gt; 0.5).reshape(201, 201)\nplt.figure()\nplt.contourf(x1_grid, x2_grid, pred, cmap=\"coolwarm\", alpha=0.3)\nplt.scatter(x1, x2, c=y, cmap=\"bwr\", s=15, alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Gradient Boosting\")\nplt.show()"
  },
  {
    "objectID": "code/09-class-glm-code.html",
    "href": "code/09-class-glm-code.html",
    "title": "09- Logistic Regression Code Demo",
    "section": "",
    "text": "Codelibrary(ISLR2)\n\n\n\n\nCodedata(\"Default\")\nlogit_fit &lt;- glm(default ~ balance, data = Default, family = binomial)\ncoef(summary(logit_fit))\n\n                 Estimate   Std. Error   z value      Pr(&gt;|z|)\n(Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191\nbalance       0.005498917 0.0002203702  24.95309 1.976602e-137\n\n\n\nCodepi_hat &lt;- predict(logit_fit, type = \"response\")\neta_hat &lt;- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(balance = 2000), type = \"response\")\n\n        1 \n0.5857694 \n\n\n\nCodepred_prob &lt;- predict(logit_fit, type = \"response\")\ntable(pred_prob &gt; 0.5, Default$default)\n\n       \n          No  Yes\n  FALSE 9625  233\n  TRUE    42  100\n\n\n\nCodelibrary(ROCR)\n# create an object of class prediction \npred &lt;- ROCR::prediction(predictions = pred_prob, labels = Default$default)\n\n# calculates the ROC curve\nroc &lt;- ROCR::performance(prediction.obj = pred, measure = \"tpr\", x.measure = \"fpr\")\nplot(roc, colorize = TRUE, lwd = 3)\n\n\n\n\n\n\n\n\nCodeauc &lt;- ROCR::performance(prediction.obj = pred, measure = \"auc\")\nauc@y.values\n\n[[1]]\n[1] 0.9479785\n\n\n\n\nCodelibrary(foreign)\nmultino_data &lt;- foreign::read.dta(\"../data/hsbdemo.dta\")\n\n\n\nCodemultino_data$prog2 &lt;- relevel(multino_data$prog, ref = \"academic\")\nlevels(multino_data$prog2)\n\n[1] \"academic\" \"general\"  \"vocation\"\n\nCodelevels(multino_data$ses)\n\n[1] \"low\"    \"middle\" \"high\"  \n\n\n\nCodelibrary(nnet)\nmultino_fit &lt;- nnet::multinom(prog2 ~ ses + write, data = multino_data, trace = FALSE)\nsumm &lt;- summary(multino_fit)\nsumm$coefficients\n\n         (Intercept)  sesmiddle    seshigh      write\ngeneral     2.852198 -0.5332810 -1.1628226 -0.0579287\nvocation    5.218260  0.2913859 -0.9826649 -0.1136037\n\n\n\nCodehead(fitted(multino_fit))\n\n   academic   general  vocation\n1 0.1482764 0.3382454 0.5134781\n2 0.1202017 0.1806283 0.6991700\n3 0.4186747 0.2368082 0.3445171\n4 0.1726885 0.3508384 0.4764731\n5 0.1001231 0.1689374 0.7309395\n6 0.3533566 0.2377976 0.4088458\n\n\n\nCodedses &lt;- data.frame(ses = c(\"low\", \"middle\", \"high\"), \n                   write = mean(multino_data$write))\npredict(multino_fit, newdata = dses, type = \"probs\")\n\n   academic   general  vocation\n1 0.4396845 0.3581917 0.2021238\n2 0.4777488 0.2283353 0.2939159\n3 0.7009007 0.1784939 0.1206054\n\n\nWe can also use glmnet package.\n\nCodelibrary(fastDummies) # https://jacobkap.github.io/fastDummies/\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nCodeprog &lt;- multino_data$prog2\ndummy_dat &lt;- dummy_cols(multino_data, select_columns = c(\"ses\"))\nx &lt;- dummy_dat |&gt; dplyr::select(ses_middle, ses_high, write)\nfit &lt;- glmnet(x = x, y = prog, family = \"multinomial\", lambda = 0)\ncoef(fit)\n\n$academic\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n           -2.69052260\nses_middle  .         \nses_high    0.98278066\nwrite       0.05793834\n\n$general\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s0\n            0.1625919\nses_middle -0.5337675\nses_high   -0.1805596\nwrite       .        \n\n$vocation\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n            2.52793069\nses_middle  0.29119238\nses_high    .         \nwrite      -0.05566551\n\nCodenewx &lt;- x[1:3, ]\nnewx[, 3] &lt;- mean(multino_data$write)\npredict(fit, as.matrix(newx), type=\"response\")\n\n, , s0\n\n   academic   general  vocation\n1 0.4396037 0.3582719 0.2021245\n2 0.4777583 0.2283218 0.2939199\n3 0.7009084 0.1784762 0.1206154\n\nCode# model_mat &lt;- model.matrix(prog2~ses+write, data=multino_data)"
  },
  {
    "objectID": "code/09-class-glm-code.html#r-implementation",
    "href": "code/09-class-glm-code.html#r-implementation",
    "title": "09- Logistic Regression Code Demo",
    "section": "",
    "text": "Codelibrary(ISLR2)\n\n\n\n\nCodedata(\"Default\")\nlogit_fit &lt;- glm(default ~ balance, data = Default, family = binomial)\ncoef(summary(logit_fit))\n\n                 Estimate   Std. Error   z value      Pr(&gt;|z|)\n(Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191\nbalance       0.005498917 0.0002203702  24.95309 1.976602e-137\n\n\n\nCodepi_hat &lt;- predict(logit_fit, type = \"response\")\neta_hat &lt;- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(balance = 2000), type = \"response\")\n\n        1 \n0.5857694 \n\n\n\nCodepred_prob &lt;- predict(logit_fit, type = \"response\")\ntable(pred_prob &gt; 0.5, Default$default)\n\n       \n          No  Yes\n  FALSE 9625  233\n  TRUE    42  100\n\n\n\nCodelibrary(ROCR)\n# create an object of class prediction \npred &lt;- ROCR::prediction(predictions = pred_prob, labels = Default$default)\n\n# calculates the ROC curve\nroc &lt;- ROCR::performance(prediction.obj = pred, measure = \"tpr\", x.measure = \"fpr\")\nplot(roc, colorize = TRUE, lwd = 3)\n\n\n\n\n\n\n\n\nCodeauc &lt;- ROCR::performance(prediction.obj = pred, measure = \"auc\")\nauc@y.values\n\n[[1]]\n[1] 0.9479785\n\n\n\n\nCodelibrary(foreign)\nmultino_data &lt;- foreign::read.dta(\"../data/hsbdemo.dta\")\n\n\n\nCodemultino_data$prog2 &lt;- relevel(multino_data$prog, ref = \"academic\")\nlevels(multino_data$prog2)\n\n[1] \"academic\" \"general\"  \"vocation\"\n\nCodelevels(multino_data$ses)\n\n[1] \"low\"    \"middle\" \"high\"  \n\n\n\nCodelibrary(nnet)\nmultino_fit &lt;- nnet::multinom(prog2 ~ ses + write, data = multino_data, trace = FALSE)\nsumm &lt;- summary(multino_fit)\nsumm$coefficients\n\n         (Intercept)  sesmiddle    seshigh      write\ngeneral     2.852198 -0.5332810 -1.1628226 -0.0579287\nvocation    5.218260  0.2913859 -0.9826649 -0.1136037\n\n\n\nCodehead(fitted(multino_fit))\n\n   academic   general  vocation\n1 0.1482764 0.3382454 0.5134781\n2 0.1202017 0.1806283 0.6991700\n3 0.4186747 0.2368082 0.3445171\n4 0.1726885 0.3508384 0.4764731\n5 0.1001231 0.1689374 0.7309395\n6 0.3533566 0.2377976 0.4088458\n\n\n\nCodedses &lt;- data.frame(ses = c(\"low\", \"middle\", \"high\"), \n                   write = mean(multino_data$write))\npredict(multino_fit, newdata = dses, type = \"probs\")\n\n   academic   general  vocation\n1 0.4396845 0.3581917 0.2021238\n2 0.4777488 0.2283353 0.2939159\n3 0.7009007 0.1784939 0.1206054\n\n\nWe can also use glmnet package.\n\nCodelibrary(fastDummies) # https://jacobkap.github.io/fastDummies/\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nCodeprog &lt;- multino_data$prog2\ndummy_dat &lt;- dummy_cols(multino_data, select_columns = c(\"ses\"))\nx &lt;- dummy_dat |&gt; dplyr::select(ses_middle, ses_high, write)\nfit &lt;- glmnet(x = x, y = prog, family = \"multinomial\", lambda = 0)\ncoef(fit)\n\n$academic\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n           -2.69052260\nses_middle  .         \nses_high    0.98278066\nwrite       0.05793834\n\n$general\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s0\n            0.1625919\nses_middle -0.5337675\nses_high   -0.1805596\nwrite       .        \n\n$vocation\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n            2.52793069\nses_middle  0.29119238\nses_high    .         \nwrite      -0.05566551\n\nCodenewx &lt;- x[1:3, ]\nnewx[, 3] &lt;- mean(multino_data$write)\npredict(fit, as.matrix(newx), type=\"response\")\n\n, , s0\n\n   academic   general  vocation\n1 0.4396037 0.3582719 0.2021245\n2 0.4777583 0.2283218 0.2939199\n3 0.7009084 0.1784762 0.1206154\n\nCode# model_mat &lt;- model.matrix(prog2~ses+write, data=multino_data)"
  },
  {
    "objectID": "code/09-class-glm-code.html#python-implementation",
    "href": "code/09-class-glm-code.html#python-implementation",
    "title": "09- Logistic Regression Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\n2.1 Binary (Binomial) Logistic Regression\n\nCodeimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n\n\nCode# Load your dataset\nDefault = pd.read_csv(\"../data/Default.csv\")\n\n\n\nCodeDefault['default'] = Default['default'].map({'Yes': 1, 'No': 0})\nfrom statsmodels.formula.api import logit\nlogit_fit = logit(formula='default ~ balance', data=Default).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.079823\n         Iterations 10\n\nCodelogit_fit.summary2().tables[1]\n\n               Coef.  Std.Err.          z          P&gt;|z|     [0.025    0.975]\nIntercept -10.651331  0.361169 -29.491287  3.723665e-191 -11.359208 -9.943453\nbalance     0.005499  0.000220  24.952404  2.010855e-137   0.005067  0.005931\n\n\n\nCodepi_hat = logit_fit.predict(Default[['balance']])  # Type = \"response\" in R\neta_hat = logit_fit.predict(Default[['balance']], which=\"linear\")  # Type = \"link\" in R\nnew_data = pd.DataFrame({'balance': [2000]})\nlogit_fit.predict(new_data)\n\n0    0.585769\ndtype: float64\n\n\n\nCodefrom sklearn.metrics import confusion_matrix\npred_prob = logit_fit.predict(Default[['balance']])  # Type = \"response\" in R\n# Create predictions based on a 0.5 threshold\npred_class = (pred_prob &gt; 0.5).astype(int)  # Convert to binary class (0 or 1)\n## C00: true negatives; C10: false negatives; C01: false postives; C11: true positives\nconfusion_matrix(y_true=Default['default'], y_pred=pred_class)\n\narray([[9625,   42],\n       [ 233,  100]])\n\n\n\nCodefrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(Default['default'], pred_prob)\n\n# Calculate the AUC (Area Under the Curve)\nauc = roc_auc_score(Default['default'], pred_prob)\nauc\n\n0.9479784946837808\n\n\n\nCodeplt.figure()\nplt.plot(fpr, tpr, color='blue')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.show()\n\n\n\n\n\n\n\n\n2.2 Multinomial Logistic Regression\n\nCodemultino_data = pd.read_stata(\"../data/hsbdemo.dta\")\n\n\n\nCodemultino_data['prog2'] = multino_data['prog'].cat.reorder_categories(\n    ['academic'] + [cat for cat in multino_data['prog'].cat.categories if cat != 'academic'],\n    ordered=True\n)\nmultino_data['prog2'].unique()\n\n['vocation', 'general', 'academic']\nCategories (3, object): ['academic' &lt; 'general' &lt; 'vocation']\n\nCodemultino_data['ses'].unique()\n\n['low', 'middle', 'high']\nCategories (3, object): ['low' &lt; 'middle' &lt; 'high']\n\n\n\nCodemultino_data['prog_int'] = multino_data['prog2'].map({\n    'academic': 0,\n    'general': 1,\n    'vocation': 2\n})\nmultino_data['prog_int'] = multino_data['prog_int'].cat.codes\n\n\n\nCodefrom statsmodels.formula.api import mnlogit\nmultino_fit = mnlogit(\"prog_int ~ ses + write\", data=multino_data).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.899909\n         Iterations 6\n\nCodemultino_fit.params\n\n                      0         1\nIntercept      2.852186  5.218200\nses[T.middle] -0.533291  0.291393\nses[T.high]   -1.162832 -0.982670\nwrite         -0.057928 -0.113603\n\n\n\nCodefitted_df = pd.DataFrame(multino_fit.predict(), \n                         columns=['academic', 'general', 'vocation'])\nfitted_df.head()\n\n   academic   general  vocation\n0  0.148278  0.338249  0.513473\n1  0.120203  0.180629  0.699168\n2  0.418679  0.236808  0.344513\n3  0.172690  0.350841  0.476468\n4  0.100125  0.168938  0.730937\n\n\n\nCodedses = pd.DataFrame({\n    'ses': ['low', 'middle', 'high'],\n    'write': [multino_data['write'].mean()] * 3\n})\nmultino_fit.predict(dses)\n\n          0         1         2\n0  0.439684  0.358193  0.202123\n1  0.477749  0.228334  0.293917\n2  0.700902  0.178493  0.120605\n\n\nWe can also use sklearn package.\n\nCodefrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Separate features (X) and target (y)\nX = multino_data[['ses', 'write']]  # Independent variables\ny = multino_data['prog_int']          # Dependent variable (categorical target)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(drop='first'), ['ses']),  # One-hot encode 'ses'\n        ('num', 'passthrough', ['write'])              # Leave 'write' as is\n    ]\n)\n# Create a multinomial logistic regression model\nmodel = Pipeline([\n    ('preprocessor', preprocessor),  # Preprocessing step\n    ('classifier', LogisticRegression(multi_class='multinomial', penalty=None,\n                                      solver='lbfgs', max_iter=500))\n])\n\nmodel.fit(X, y)\n\n\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first'),\n                                                  ['ses']),\n                                                 ('num', 'passthrough',\n                                                  ['write'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=500, multi_class='multinomial',\n                                    penalty=None))])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n¬†¬†Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first'),\n                                                  ['ses']),\n                                                 ('num', 'passthrough',\n                                                  ['write'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=500, multi_class='multinomial',\n                                    penalty=None))]) \n\n\n\n¬†preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('cat', OneHotEncoder(drop='first'), ['ses']),\n                                ('num', 'passthrough', ['write'])]) \n\n\n\ncat['ses'] \n\n¬†OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='first') \n\n\n\nnum['write'] \n\npassthroughpassthrough \n\n\n\n\n¬†LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=500, multi_class='multinomial', penalty=None) \n\n\n\n\nCodemodel.named_steps.classifier.coef_\n\narray([[-0.71516627, -0.63453974,  0.05717724],\n       [ 0.4476491 , -0.0049987 , -0.00075151],\n       [ 0.26751717,  0.63953844, -0.05642573]])\n\nCodemodel.named_steps.classifier.intercept_\n\narray([-1.97496981, -0.28559419,  2.260564  ])\n\n\n\nCodedses = pd.DataFrame({\n    'ses': ['low', 'middle', 'high'],\n    'write': [multino_data['write'].mean()] * 3\n})\n\npd.DataFrame(model.predict_proba(dses),\n             columns=model.named_steps.classifier.classes_)\n\n          0         1         2\n0  0.439686  0.358190  0.202124\n1  0.477748  0.228334  0.293917\n2  0.700903  0.178494  0.120603"
  },
  {
    "objectID": "present-description-2.html",
    "href": "present-description-2.html",
    "title": "Midterm Presentation II - Classification",
    "section": "",
    "text": "Proposal. Please send me a one-page PDF describing what you are going to do for your project (no word limit) with your project title by Tuesday, 4/8 11:59 PM.\nPresentation. You will be presenting your project on Tuesday, 4/15 in class.\nMaterials. Please share your entire work (slides, code, data, etc) by Friday, 4/18 11:59 PM.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#proposal",
    "href": "present-description-2.html#proposal",
    "title": "Midterm Presentation II - Classification",
    "section": "Proposal",
    "text": "Proposal\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\n\nYour proposal (in PDF) should include three parts:\n\nProject title\n\nThe goal of your project. For example, what is the research question you‚Äôd like to answer? What machine learning method/model/algorithm you‚Äôd like to introduce? What data you‚Äôd like to use for analysis or demonstration? etc.\n\n\nAlthough it is risky, you can change your project topic after you submit your proposal if you decide to do something else.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#presentation",
    "href": "present-description-2.html#presentation",
    "title": "Midterm Presentation II - Classification",
    "section": "Presentation",
    "text": "Presentation\n\nEach group presentation should be between 10 and 11 minute long, followed by 1 to 2 minute Q&A. If your presentation is too short or too long, every one of you loses 5 points of your project grade.\nEvery group member has to present some part of the group work. The one who does not present receives no point.\nQuestions are REQUIRED during Q&A. Each group is required to ask as least one question. More questions are welcome. If you, as a group, don‚Äôt ask a question when you should, every one of you loses 5 points of your project grade.\n\n\n\n\nTeam Presenting\nTeam Asking Questions\n\n\n\nTeam 1\nTeam 6\n\n\nTeam 2\nTeam 1\n\n\nTeam 3\nTeam 2\n\n\nTeam 4\nTeam 3\n\n\nTeam 5\nTeam 4\n\n\nTeam 6\nTeam 5",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#materials",
    "href": "present-description-2.html#materials",
    "title": "Midterm Presentation II - Classification",
    "section": "Materials",
    "text": "Materials\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\nYou need to share your entire work, including slides, code, and data if applicable.\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the slides, including the source of the raw data (where you find and load the data) if the project is about data analysis.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#data-analytics",
    "href": "present-description-2.html#data-analytics",
    "title": "Midterm Presentation II - Classification",
    "section": "Data Analytics",
    "text": "Data Analytics\nFor your DA project, you need to\n\nDescribe the selected data set.\nExplain and show why the chosen model(s) is appropriate for answering your research questions and better than others.\nInterpret your analysis result.\n\nBelow are some data repositories you can start with, but you are encouraged to explore more and find your favorite one.\n\nTidyTuesday\nKaggle\nAwesome Public Datasets\nHarvard Dataverse\nUCI Machine Learning Repository\nFiveThirtyEight",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#modelalgorithm",
    "href": "present-description-2.html#modelalgorithm",
    "title": "Midterm Presentation II - Classification",
    "section": "Model/Algorithm",
    "text": "Model/Algorithm\nFor your MA project, you need to\n\nDescribe the intuition and idea of the method. What are the pros and cons of the method?\nProvide the mathematical expression of the model/algorithm. Explain the model and its properties, and how we do supervised learning with the model/algorithm.\nCompare the chosen method with other methods learned in class. Determine which method performs better under what conditions.\nDemo how to implement the method using a programming language for supervised learning.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#group-performance-evaluation",
    "href": "present-description-2.html#group-performance-evaluation",
    "title": "Midterm Presentation II - Classification",
    "section": "Group Performance Evaluation",
    "text": "Group Performance Evaluation\n\nYou will need to evaluate all group projects except the one you work on.\n\nYou evaluate group performance based on the rubric attached. Four evaluation criteria are considered:\n\nProject Content and Organization (8 pts)\nPresentation Material (Slides) Quality (4 pts)\nOral Presentation Skill and Delivery (4 pts)\nInteractions and Q&A (4 pts)\n\n\nThe total points of a project presentation is 20 points.\nEvaluation sheets will be provided on the presentation day.\n\nHow do you get the full points for each category? Check the rubric below. \n\n\nContent and Organization (DA)\n\nBeautiful visualization helps find out relationship of variables and specification of models\nAll questions are answered accurately by the models\nDiscuss how and why the models are chosen\nApply sophisticated models and detailed analysis\nAll ideas are presented in logical order\n\n\n\nContent and Organization (MA)\n\nExplain the method clearly and accuratly\nShow the pros and cons of the method, and compare with the methods learned in class.\nShow how the method can be implemednted for supervised learning.\nAll ideas are presented in logical order\n\n\n\nPresentation Material Quality\n\nPresentation material show code and output beautifully\nPresentation material clearly aid the speaker in telling a coherent story\nAll tables and graphics are informative and related to the topic and make it easier to understand\nAttractive design, layout, and neatness.\n\n\n\nOral Presentation Skill\n\nGood volume and energy\nProper pace and diction\nAvoidance of distracting gestures\n\n\n\nInteractions and Q&A\n\nGood eye contact with audience\nExcellent listening skills\nAnswers audience questions with authority and accuracy\n\n\nAfter you evaluate all group project presentations, you rank them from 1st to last based on their earned points.\nNo two groups receive the same ranking. If you give two or more groups some points, you still need to give them a different ranking, deciding which team deserves a higher ranking according to your preference.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "hw/hw-1.html#homework-problems",
    "href": "hw/hw-1.html#homework-problems",
    "title": "Homework 1",
    "section": "Homework Problems",
    "text": "Homework Problems\n\nISL Sec. 2.4: 1\nISL Sec. 2.4: 3\nISL Sec. 2.4: 5\nISL Sec. 3.7: 4\nISL Sec. 3.7: 6\nISL Sec. 3.7: 14\n\nSimulation of simple linear regression. Consider the model \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, ~~ \\epsilon_i \\stackrel{iid} \\sim N(0, \\sigma^2), ~~ i = 1, \\dots, n,\\) or equivalently, \\(p(y_i \\mid x, \\beta_0, \\beta_1) = N\\left(\\beta_0 + \\beta_1x_i, \\sigma^2 \\right).\\) Set \\(\\beta_0 = 2, \\beta_1 = -1.5, \\sigma = 1, n = 100, x_i = i.\\) Generate 1000 simulated data sets and fit the model to each data set.  The sampling distribution of the least-squares estimator \\(b_1\\) is \\[b_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right).\\]\n\nFind the average of the 1000 estimates \\(b_1\\). Is it close to its true expected value?\nFind the variance of 1000 estimates \\(b_1\\). Is it close to its true value?\nDraw the histogram of \\(b_1\\). Comment on your graph.\nObtain 1000 95% confidence interval for \\(\\beta_1\\). What is your pertentage of coverage for \\(\\beta_1\\)? Comment your result.\n\n\n\nSimulation of bias-variance tradeoff. Let \\(f(x) = x ^ 2\\) be the true regression function. Simulate the data using \\(y_i = f(x_i) + \\epsilon_i, i = 1, \\dots, 100\\), where \\(x_i = 0.01i\\) and \\(\\epsilon_i \\stackrel{iid} \\sim N(0, 0.3^2).\\)\n\nGenerate 250 training data sets.\nFor each data set, fit the following three models:\n\n\nModel 1: \\(y = \\beta_0+\\beta_1x+\\epsilon\\)\n\nModel 2: \\(y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\epsilon\\)\n\nModel 3: \\(y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\cdots + \\beta_9x^9 + \\epsilon\\)\n\n\n\nCalculate empirical MSE of \\(\\hat{f}\\), bias of \\(\\hat{f}\\) and variance of \\(\\hat{f}\\). Then show that \\[\\text{MSE}_{\\hat{f}} \\approx \\text{Bias}^2(\\hat{f}) + \\mathrm{Var}(\\hat{f}).\\] Specifically, for each value of \\(x_i\\), \\(i = 1, \\dots, 100\\),\n\n\\[\\text{MSE}_{\\hat{f}} =  \\frac{1}{250}\\sum_{k=1}^{250} \\left(\\hat{f}_k(x_i) - f(x_i) \\right) ^2,\\] \\[\\text{Bias}(\\hat{f}) = \\overline{\\hat{f}}(x_i) - f(x_i),\\] where \\(\\overline{\\hat{f}}(x_i) = \\frac{1}{250}\\sum_{k = 1}^{250}\\hat{f}_k(x_i)\\) is the sample mean of \\(\\hat{f}(x_i)\\) that approximates \\(\\mathrm{E}_{\\hat{f}}\\left(\\hat{f}(x_i)\\right).\\) \\[ \\mathrm{Var}(\\hat{f}) =  \\frac{1}{250}\\sum_{k=1}^{250} \\left(\\hat{f}_k(x_i) - \\overline{\\hat{f}}(x_i) \\right) ^2.\\] [Note:] If you calculate the variance using the built-in function such as var() in R, the identity holds only approximately because of the \\(250 - 1\\) term in the denominator in the sample variance formula. If instead \\(250\\) is used in the denominator, the identity holds exactly.\n\nFor each model, plot first ten estimated \\(f\\), \\(\\hat{f}_{1}(x), \\dots, \\hat{f}_{10}(x)\\), and the average of \\(\\hat{f}\\), \\(\\frac{1}{250}\\sum_{k=1}^{250}\\hat{f}_{k}(x)\\) in one figure, as Figure¬†1 below. What‚Äôs your finding?\nGenerate one more data set and use it as the test data. Calculate the overall training MSE (for training \\(y\\)) and overall test MSE (for test \\(y\\)) for each model. \\[MSE_\\texttt{Tr} =  \\frac{1}{250} \\sum_{k = 1}^{250} \\frac{1}{100} \\sum_{i=1}^{100} \\left(\\hat{f}_{k}(x_i) - y_{i}^k\\right)^2\\] \\[MSE_\\texttt{Te} =  \\frac{1}{250} \\sum_{k = 1}^{250} \\frac{1}{100} \\sum_{i=1}^{100} \\left(\\hat{f}_{k}(x_i) - y_{i}^{\\texttt{Test}}\\right)^2\\]\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Trained \\(f\\)\n\n\n\n\n\n\n\n(MSSC PhD) Stochastic gradient descent. Write your own stochastic gradient descent algorithm for linear regression. Implement it and compare with the built-in function such as optim() of R or scipy.optimize() of python to make sure that it converges correctly."
  },
  {
    "objectID": "hw/hw-3.html#homework-questions",
    "href": "hw/hw-3.html#homework-questions",
    "title": "Homework 3 - Bayesian Statistics, Logistic Regression, Generative Models, and K-Nearest Neighbors",
    "section": "Homework Questions",
    "text": "Homework Questions\n\nRequired\n\nISL Sec. 4.8: 2\nISL Sec. 4.8: 3\nISL Sec. 4.8: 5\nISL Sec. 4.8: 13\nMNIST Handwritten Digits Image\n\nLoad the prepared MNIST data mnist.csv. Print some images.\nUse the first 1000 observations as the training data and the second half as the test data.\nTraining with KNN and predicting on the test data with the best \\(K\\) selected from the training.\n\nCalculate the test error rate.\nGenerate the confusion matrix.\n\nTraining with multinomial logistic regression and predicting on the test data.\n\nCalculate the test error rate.\nGenerate the confusion matrix.\n\n\n(MSSC PhD) KNN Curse of Dimensionality\n\nGenerate Generate the covariates \\(x_1, x_2, \\dots, x_5\\) of \\(n = 1000\\) training data from independent standard normal distribution. Then, generate \\(Y\\) from \\[Y = X_1 + 0.5 X_2 - X_3 + \\epsilon,\\] where \\(\\epsilon \\sim N(0, 1).\\)\nUse the first 500 observations as the training data and the rest as the test data. Fit KNN regression, and report the test MSE of \\(y\\) with the optimal \\(K\\).\nAdd additional 95 noisy predictors as follows.\n\nCase 1: \\(x_6, x_7, \\dots, x_{100} \\overset{\\mathrm{iid}}{\\sim} N(0, 1)\\)\nCase 2: \\(XA\\) where \\(X_{1000 \\times 5} = [x_1 \\cdots x_5]\\) and \\(A_{5 \\times 95}\\) having entries from iid uniform(0, 1).\n\nFit KNN regression in both cases (with the total of 100 covariates) and select the best \\(K\\) value.\nFor both cases, what is the best K and the best mean squared error for prediction? Discuss the effect of adding 95 (unnecessary) covariates.\n\n\n\n\nDo one of the followings\n\n\nWatch the talk All About that Bayes: Probability, Statistics, and the Quest to Quantify Uncertainty by Dr.¬†Kristin Lennox. In 250 words, summarize your thoughts and what you learned from the talk.\n\n\n\nIn 250 words, summarize your thoughts and what you learned from the deep learning workshop."
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Generative Models"
  },
  {
    "objectID": "weeks/week-8.html#reading-and-resources",
    "href": "weeks/week-8.html#reading-and-resources",
    "title": "Week 8",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 3.5, 4.5"
  },
  {
    "objectID": "weeks/week-8.html#exercise",
    "href": "weeks/week-8.html#exercise",
    "title": "Week 8",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Tree-based Methods"
  },
  {
    "objectID": "weeks/week-13.html#reading",
    "href": "weeks/week-13.html#reading",
    "title": "Week 13",
    "section": "Reading",
    "text": "Reading\nüìñ ISL Ch 8"
  },
  {
    "objectID": "weeks/week-13.html#exercise",
    "href": "weeks/week-13.html#exercise",
    "title": "Week 13",
    "section": "Exercise",
    "text": "Exercise\n\nr fontawesome::fa(\"table\") iris\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-10.html#participate",
    "href": "weeks/week-10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - K-Nearest Neighbors\nr fontawesome::fa(\"table\") Mixture Simulation Data\nr fontawesome::fa(\"table\") ZIP code Training and Test data"
  },
  {
    "objectID": "weeks/week-10.html#reading-and-resources",
    "href": "weeks/week-10.html#reading-and-resources",
    "title": "Week 10",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ PMLI Ch 16.1"
  },
  {
    "objectID": "weeks/week-10.html#exercise",
    "href": "weeks/week-10.html#exercise",
    "title": "Week 10",
    "section": "Exercise",
    "text": "Exercise\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\nHomework 1 due Friday, Feb 14, 11:59 PM"
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Ridge Regression and Cross Validation\nr fontawesome::fa(\"table\") Data Set - mtcars.csv"
  },
  {
    "objectID": "weeks/week-3.html#reading-and-resources",
    "href": "weeks/week-3.html#reading-and-resources",
    "title": "Week 3",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 5.1, 6.2.1\nüìñ MML Ch 8.2.3 - 8.2.4"
  },
  {
    "objectID": "weeks/week-3.html#exercise",
    "href": "weeks/week-3.html#exercise",
    "title": "Week 3",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\nHomework 1 due Friday, Feb 9, 11:59 PM"
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Lasso\n\nr fontawesome::fa(\"table\") Data Set - Prostate.csv"
  },
  {
    "objectID": "weeks/week-4.html#reading-and-resources",
    "href": "weeks/week-4.html#reading-and-resources",
    "title": "Week 4",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 6.1, 6.2"
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\nHomework 2 due Friday, Feb 28, 11:59 PM."
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Logistic Regression\nr fontawesome::fa(\"table\") ISLR2::Default\nr fontawesome::fa(\"table\") multinomial data hsbdemo.csv"
  },
  {
    "objectID": "weeks/week-7.html#reading-and-resources",
    "href": "weeks/week-7.html#reading-and-resources",
    "title": "Week 7",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 4.1 - 4.3, 4.6"
  },
  {
    "objectID": "weeks/week-7.html#exercise",
    "href": "weeks/week-7.html#exercise",
    "title": "Week 7",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "present-work.html",
    "href": "present-work.html",
    "title": "Midterm Project I Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(6250)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[2]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[3]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[4]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[5]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\"",
    "crumbs": [
      "Midterm Project I",
      "Topics and Works"
    ]
  },
  {
    "objectID": "present-work.html#presentation-order",
    "href": "present-work.html#presentation-order",
    "title": "Midterm Project I Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(6250)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[2]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[3]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[4]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[5]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\"",
    "crumbs": [
      "Midterm Project I",
      "Topics and Works"
    ]
  },
  {
    "objectID": "present-work.html#project-materials",
    "href": "present-work.html#project-materials",
    "title": "Midterm Project I Proposal and Presentation",
    "section": "Project Materials",
    "text": "Project Materials\n\nGroup 1 (Sajjad, Tanjina, Dewan) Predicting Life Expectancy: proposal\nGroup 2 (John, Jeremy, Praful) Poisson Regression: proposal\nGroup 3 (Violet, Vanessa, Michele) Is Ridge Regression or LASSO a better model to predict the price of houses in Ames, Iowa: proposal\nGroup 4 (Rakesh, Daniel, Jeremy) Predicting Abalone Age Using Regression Models: proposal\nGroup 5 (Sai, Rohith, Shristi) Analyzing the Impact of Housing Features on Sale Prices: A Regression-Based Study on the Ames Housing Dataset : proposal\nGroup 6 (Ethan, Navid, Sylvester) Predicting Car Sticker Price Regression: proposal",
    "crumbs": [
      "Midterm Project I",
      "Topics and Works"
    ]
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor ‚Äì Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.\n\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "present-description.html",
    "href": "present-description.html",
    "title": "Midterm Presentation I - Regression",
    "section": "",
    "text": "Team up! You will be working as a group of 3. One of you, please email me your team member list by Friday, 2/21 11:59 PM\nProposal. Please send me a one-page PDF describing what you are going to do for your project (no word limit) with your project title by Friday, 2/28 11:59 PM.\nPresentation. You will be presenting your project on Thursday, 3/6 in class.\nMaterials. Please share your entire work (slides, code, data, etc) by Friday, 3/7 11:59 PM.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#team-up",
    "href": "present-description.html#team-up",
    "title": "Midterm Presentation I - Regression",
    "section": "Team up!",
    "text": "Team up!\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\nYou will be randomly assigned to a group if you do not belong to any group before the deadline.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#proposal",
    "href": "present-description.html#proposal",
    "title": "Midterm Presentation I - Regression",
    "section": "Proposal",
    "text": "Proposal\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\n\nYour proposal (in PDF) should include three parts:\n\nProject title\n\nThe goal of your project. For example, what is the research question you‚Äôd like to answer? What machine learning method/model/algorithm you‚Äôd like to introduce? What data you‚Äôd like to use for analysis or demonstration? etc.\n\n\nAlthough it is risky, you can change your project topic after you submit your proposal if you decide to do something else.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#presentation",
    "href": "present-description.html#presentation",
    "title": "Midterm Presentation I - Regression",
    "section": "Presentation",
    "text": "Presentation\n\nEach group presentation should be between 10 and 11 minute long, followed by 1 to 2 minute Q&A. If your presentation is too short or too long, every one of you loses 5 points of your project grade.\nEvery group member has to present some part of the group work. The one who does not present receives no point.\nQuestions are encouraged during Q&A. Everyone is welcome to ask any questions about the projects.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#materials",
    "href": "present-description.html#materials",
    "title": "Midterm Presentation I - Regression",
    "section": "Materials",
    "text": "Materials\n\nEach one of you loses 5 points of your project grade if you don‚Äôt meet the requirement or miss the deadline.\nYou need to share your entire work, including slides, code, and data if applicable.\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the slides, including the source of the raw data (where you find and load the data) if the project is about data analysis.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#data-analytics",
    "href": "present-description.html#data-analytics",
    "title": "Midterm Presentation I - Regression",
    "section": "Data Analytics",
    "text": "Data Analytics\nFor your DA project, you need to\n\nDescribe the selected data set.\nExplain and show why the chosen model(s) is appropriate for answering your research questions and better than others.\nInterpret your analysis result.\n\nBelow are some data repositories you can start with, but you are encouraged to explore more and find your favorite one.\n\nTidyTuesday\nKaggle\nAwesome Public Datasets\nHarvard Dataverse\nUCI Machine Learning Repository\nFiveThirtyEight",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#modelalgorithm",
    "href": "present-description.html#modelalgorithm",
    "title": "Midterm Presentation I - Regression",
    "section": "Model/Algorithm",
    "text": "Model/Algorithm\nFor your MA project, you need to\n\nDescribe the intuition and idea of the method. What are the pros and cons of the method?\nProvide the mathematical expression of the model/algorithm. Explain the model and its properties, and how we do supervised learning with the model/algorithm.\nCompare the chosen method with other methods learned in class. Determine which method performs better under what conditions.\nDemo how to implement the method using a programming language for supervised learning.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#group-performance-evaluation",
    "href": "present-description.html#group-performance-evaluation",
    "title": "Midterm Presentation I - Regression",
    "section": "Group Performance Evaluation",
    "text": "Group Performance Evaluation\n\nYou will need to evaluate all group projects except the one you work on.\n\nYou evaluate group performance based on the rubric attached. Four evaluation criteria are considered:\n\nProject Content and Organization (8 pts)\nPresentation Material (Slides) Quality (4 pts)\nOral Presentation Skill and Delivery (4 pts)\nInteractions and Q&A (4 pts)\n\n\nThe total points of a project presentation is 20 points.\nEvaluation sheets will be provided on the presentation day.\n\nHow do you get the full points for each category? Check the rubric below. \n\n\nContent and Organization (DA)\n\nBeautiful visualization helps find out relationship of variables and specification of models\nAll questions are answered accurately by the models\nDiscuss how and why the models are chosen\nApply sophisticated models and detailed analysis\nAll ideas are presented in logical order\n\n\n\nContent and Organization (MA)\n\nExplain the method clearly and accuratly\nShow the pros and cons of the method, and compare with the methods learned in class.\nShow how the method can be implemednted for supervised learning.\nAll ideas are presented in logical order\n\n\n\nPresentation Material Quality\n\nPresentation material show code and output beautifully\nPresentation material clearly aid the speaker in telling a coherent story\nAll tables and graphics are informative and related to the topic and make it easier to understand\nAttractive design, layout, and neatness.\n\n\n\nOral Presentation Skill\n\nGood volume and energy\nProper pace and diction\nAvoidance of distracting gestures\n\n\n\nInteractions and Q&A\n\nGood eye contact with audience\nExcellent listening skills\nAnswers audience questions with authority and accuracy\n\n\nAfter you evaluate all group project presentations, you rank them from 1st to last based on their earned points.\nNo two groups receive the same ranking. If you give two or more groups some points, you still need to give them a different ranking, deciding which team deserves a higher ranking according to your preference.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Bayesian Linear Regression\nr fontawesome::fa(\"table\") bayesrules::bikes"
  },
  {
    "objectID": "weeks/week-6.html#reading-and-resources",
    "href": "weeks/week-6.html#reading-and-resources",
    "title": "Week 6",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ Bayes Rules! An Introduction to Applied Bayesian Modeling\nüìñ Statistical Rethinking\nüìñ A Student‚Äôs Guide to Bayesian Statistics\nüìñ Bayesian Data Analysis"
  },
  {
    "objectID": "weeks/week-6.html#exercise",
    "href": "weeks/week-6.html#exercise",
    "title": "Week 6",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Splines and Generalized Additive Models\nr fontawesome::fa(\"table\") Data set - birthrates"
  },
  {
    "objectID": "weeks/week-5.html#reading-and-resources",
    "href": "weeks/week-5.html#reading-and-resources",
    "title": "Week 5",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 7"
  },
  {
    "objectID": "weeks/week-5.html#exercise",
    "href": "weeks/week-5.html#exercise",
    "title": "Week 5",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "üìñ Read the syllabus.\nüìñ Get your laptop and computing environment ready!\nüìñ Refresh your probability/statistics and linear algebra."
  },
  {
    "objectID": "weeks/week-1.html#prepare",
    "href": "weeks/week-1.html#prepare",
    "title": "Week 1",
    "section": "",
    "text": "üìñ Read the syllabus.\nüìñ Get your laptop and computing environment ready!\nüìñ Refresh your probability/statistics and linear algebra."
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Welcome to MSSC 6250\nüñ•Ô∏è Slides - Overview of Machine Learning\nüñ•Ô∏è Slides - Bias-variance Tradeoff"
  },
  {
    "objectID": "weeks/week-1.html#reading-and-resources",
    "href": "weeks/week-1.html#reading-and-resources",
    "title": "Week 1",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 1, 2\nüìñ Dr.¬†Yu MSSC 5780 Probability and Statistics Review\nüìñ Dr.¬†Yu MSSC 5780 Linear Algebra Review\nüìñ Check any math you need from MML Ch 2 - 6"
  },
  {
    "objectID": "weeks/week-1.html#perform",
    "href": "weeks/week-1.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\nüìã Decide whether this is the right course for you by the drop deadline 1/21.\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\nHomework 1 released."
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Linear Regression\nr fontawesome::fa(\"table\") Data Set - Advertising.csv"
  },
  {
    "objectID": "weeks/week-2.html#reading-and-resources",
    "href": "weeks/week-2.html#reading-and-resources",
    "title": "Week 2",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 3.1 - 3.4\nüìñ Dr.¬†Yu MSSC 5780 Regression Analysis Week 1 to 6\nüìñ MML Ch 7.1, 8.1, 8.2.1, 8.2.2"
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\nüìã Homework 1\n\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Gaussian Processes"
  },
  {
    "objectID": "weeks/week-11.html#reading",
    "href": "weeks/week-11.html#reading",
    "title": "Week 11",
    "section": "Reading",
    "text": "Reading\nüìñ Gaussian Processes for Machine Learning (GPML) Ch 2\nüìñ Surrogates Ch 5\nüìñ PMLI Ch 17.2"
  },
  {
    "objectID": "weeks/week-11.html#exercise",
    "href": "weeks/week-11.html#exercise",
    "title": "Week 11",
    "section": "Exercise",
    "text": "Exercise\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Support Vector Machines"
  },
  {
    "objectID": "weeks/week-12.html#reading",
    "href": "weeks/week-12.html#reading",
    "title": "Week 12",
    "section": "Reading",
    "text": "Reading\nüìñ ISL Ch 9"
  },
  {
    "objectID": "weeks/week-12.html#exercise",
    "href": "weeks/week-12.html#exercise",
    "title": "Week 12",
    "section": "Exercise",
    "text": "Exercise\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "hw/hw-4.html#homework-questions",
    "href": "hw/hw-4.html#homework-questions",
    "title": "Homework 4 - Support Vector Machines, Tree Methods, Unsupervised Learning",
    "section": "Homework Questions",
    "text": "Homework Questions\n\nISL Sec. 8.4: 12 (Don‚Äôt do BART)\nISL Sec. 9.7: 3\nISL Sec. 9.7: 5\nISL Sec. 12.6: 10"
  },
  {
    "objectID": "hw/hw-2.html#homework-questions",
    "href": "hw/hw-2.html#homework-questions",
    "title": "Homework 2 - Ridge, Lasso, and Splines",
    "section": "Homework Questions",
    "text": "Homework Questions\n\n\nISL Sec. 6.6: 4\n\n\n\nISL Sec. 6.6: 9 (a)-(d)\nISL Sec. 7.9: 9\n(MSSC PhD) ISL Sec. 7.9: 11\n[Special Case for Ridge and Lasso] Suppose for a multiple linear regression problem with \\(n = p\\), \\({\\bf X} = {\\bf I}\\) and no intercept, i.e., \\(y_i = \\beta_i + \\epsilon_i, ~~ \\epsilon_i \\stackrel{iid} N(0, \\sigma^2)\\). Show that\n\nShow that the least squares problem can be simplified to finding \\(\\beta_1,     \\dots, \\beta_p\\) that minimize \\(\\sum_{j=1}^p\\left(y_j - \\beta_j\\right)^2\\). What is least squares estimator \\(b_j\\)?\nShow that the ridge estimator is \\(\\hat{\\beta}_j^r = \\frac{y_j}{1+\\lambda} = \\underset{\\beta_j}{\\arg\\min} \\sum_{j=1}^p\\left(y_j - \\beta_j\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\\).\n\n\n\nShow that the lasso solution of \\(\\sum_{j=1}^p\\left(y_j - \\beta_j\\right)^2 + \\lambda \\sum_{j=1}^p | \\beta_j |\\) is \\[\\hat{\\beta}_j^l =\n  \\begin{cases}\ny_j - \\lambda/2       & \\quad \\text{if } y_j &gt; \\lambda/2\\\\\ny_j + \\lambda/2       & \\quad \\text{if } y_j &lt; -\\lambda/2 \\\\\n0  & \\quad \\text{if } |y_j| \\le \\lambda/2\n  \\end{cases}\\]\nDescribe the ridge and lasso shrinkage behavior.\n\n[Lasso with Correlated Variables]\nConsider the linear regression\n\\[ \\mathbf{y}= \\mathbf{X}\\boldsymbol \\beta+ \\boldsymbol \\epsilon\\]\nwhere \\(\\boldsymbol \\beta= (\\beta_1, \\beta_2, \\ldots, \\beta_{20})'\\) with \\(\\beta_1 = \\beta_{2} = \\beta_{3} = 0.5\\) and all other \\(\\beta_j = 0, j = 4, \\dots, 20\\). No intercept \\(\\beta_0\\). The input vector \\(\\mathbf{x}= (x_1, x_2, \\dots, x_{20})'\\) follows a multivariate Gaussian distribution\n\\[\\mathbf{x}\\sim N\\left(\\mathbf{0}, \\Sigma_{20 \\times 20}\\right)\\]\nIn \\(\\Sigma\\), all diagonal elements are 1, and all off-diagonal elements are \\(\\rho\\) which measures the correlation between any two predictors.\n\nGenerate training data of size 400 and test data of size 100 independently from the above model with \\(\\rho = 0.1\\) and \\(\\epsilon_i \\stackrel{iid} \\sim N(0, 1)\\).\nFit a Lasso model on the training data with 10-fold cross-validation (CV).\nCompute test MSE with the optimal \\(\\lambda\\) selected by CV in (b). Does Lasso select the correct variables?\nRepeat (a)-(c) 100 times. That is, generate 100 different training and test data sets. For each run, record the test MSE and whether or not the true model is correctly selected. Then compute the average test MSE and the proportion of runs where the correct model was selected.\nRedo (a)-(d) with \\(\\rho = 0.6\\). Compare the two average test MSEs and the proportions. Comment the result."
  },
  {
    "objectID": "code/10-generative-code.html",
    "href": "code/10-generative-code.html",
    "title": "10 - Generative Models for Classification Code Demo",
    "section": "",
    "text": "Codelibrary(ISLR2)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nCodelibrary(e1071)\ndata(\"Default\")\n\n\n\nCodelda_fit &lt;- MASS::lda(default ~ balance, data = Default)\nlda_pred &lt;- predict(lda_fit, data = Default)\ntable(lda_pred$class, Default$default, dnn = c(\"Predicted\", \"Actual\"))\n\n         Actual\nPredicted   No  Yes\n      No  9643  257\n      Yes   24   76\n\n\n\nCodeqda_fit &lt;- MASS::qda(default ~ balance, data = Default)\nqda_pred &lt;- predict(qda_fit, data = Default)\ntable(qda_pred$class, Default$default, dnn = c(\"Predicted\", \"Actual\"))\n\n         Actual\nPredicted   No  Yes\n      No  9639  246\n      Yes   28   87\n\n\n\nCodenb_fit &lt;- e1071::naiveBayes(default ~ balance, data = Default)\nnb_pred &lt;- predict(nb_fit, Default)\n(nb_conf &lt;- table(nb_pred, Default$default))\n\n       \nnb_pred   No  Yes\n    No  9639  246\n    Yes   28   87"
  },
  {
    "objectID": "code/10-generative-code.html#r-implementation",
    "href": "code/10-generative-code.html#r-implementation",
    "title": "10 - Generative Models for Classification Code Demo",
    "section": "",
    "text": "Codelibrary(ISLR2)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nCodelibrary(e1071)\ndata(\"Default\")\n\n\n\nCodelda_fit &lt;- MASS::lda(default ~ balance, data = Default)\nlda_pred &lt;- predict(lda_fit, data = Default)\ntable(lda_pred$class, Default$default, dnn = c(\"Predicted\", \"Actual\"))\n\n         Actual\nPredicted   No  Yes\n      No  9643  257\n      Yes   24   76\n\n\n\nCodeqda_fit &lt;- MASS::qda(default ~ balance, data = Default)\nqda_pred &lt;- predict(qda_fit, data = Default)\ntable(qda_pred$class, Default$default, dnn = c(\"Predicted\", \"Actual\"))\n\n         Actual\nPredicted   No  Yes\n      No  9639  246\n      Yes   28   87\n\n\n\nCodenb_fit &lt;- e1071::naiveBayes(default ~ balance, data = Default)\nnb_pred &lt;- predict(nb_fit, Default)\n(nb_conf &lt;- table(nb_pred, Default$default))\n\n       \nnb_pred   No  Yes\n    No  9639  246\n    Yes   28   87"
  },
  {
    "objectID": "code/10-generative-code.html#python-implementation",
    "href": "code/10-generative-code.html#python-implementation",
    "title": "10 - Generative Models for Classification Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport pandas as pd\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\n\n\n\nCodeDefault = pd.read_csv(\"../data/Default.csv\")\nX = Default[['balance']]\ny = Default['default']\n\n\n\nCodelda = LinearDiscriminantAnalysis()\nlda.fit(X, y)\n\n\n\n\nLinearDiscriminantAnalysis()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†LinearDiscriminantAnalysis?Documentation for LinearDiscriminantAnalysisiFittedLinearDiscriminantAnalysis() \n\n\nCodelda_pred = lda.predict(X)\nlda_conf_df = pd.DataFrame(\n    confusion_matrix(y_true=y, y_pred=lda_pred), \n    index=[f\"Actual: {cls}\" for cls in lda.classes_],\n    columns=[f\"Predicted: {cls}\" for cls in lda.classes_]\n)\nlda_conf_df\n\n             Predicted: No  Predicted: Yes\nActual: No            9643              24\nActual: Yes            257              76\n\n\n\nCodeqda = QuadraticDiscriminantAnalysis()\nqda.fit(X, y)\n\n\n\n\nQuadraticDiscriminantAnalysis()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†QuadraticDiscriminantAnalysis?Documentation for QuadraticDiscriminantAnalysisiFittedQuadraticDiscriminantAnalysis() \n\n\nCodeqda_pred = qda.predict(X)\nqda_conf_df = pd.DataFrame(\n    confusion_matrix(y, qda_pred), \n    index=[f\"Actual: {cls}\" for cls in qda.classes_],\n    columns=[f\"Predicted: {cls}\" for cls in qda.classes_]\n)\nqda_conf_df\n\n             Predicted: No  Predicted: Yes\nActual: No            9639              28\nActual: Yes            246              87\n\n\n\nCodenb = GaussianNB()\nnb.fit(X, y)\n\n\n\n\nGaussianNB()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†GaussianNB?Documentation for GaussianNBiFittedGaussianNB() \n\n\nCodenb_pred = nb.predict(X)\nnb_conf_df = pd.DataFrame(\n    confusion_matrix(y, nb_pred),\n    index=[f\"Actual: {cls}\" for cls in nb.classes_],\n    columns=[f\"Predicted: {cls}\" for cls in nb.classes_]\n)\nnb_conf_df\n\n             Predicted: No  Predicted: Yes\nActual: No            9639              28\nActual: Yes            246              87"
  },
  {
    "objectID": "code/08-bayes-code.html",
    "href": "code/08-bayes-code.html",
    "title": "08- Bayesian Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hidelibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(janitor)\nlibrary(broom.mixed)\n\n\n\nShow/Hidedata(bikes)\n\n\n\nShow/Hidebike_model &lt;- rstanarm::stan_glm(rides ~ temp_feel, data = bikes,\n                                 family = gaussian,\n                                 prior_intercept = normal(5000, 1000),\n                                 prior = normal(100, 40), \n                                 prior_aux = exponential(0.0008),\n                                 chains = 4, iter = 5000*2, seed = 2025)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000667 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 6.67 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.187 seconds (Warm-up)\nChain 1:                0.201 seconds (Sampling)\nChain 1:                0.388 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.182 seconds (Warm-up)\nChain 2:                0.206 seconds (Sampling)\nChain 2:                0.388 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.109 seconds (Warm-up)\nChain 3:                0.213 seconds (Sampling)\nChain 3:                0.322 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.136 seconds (Warm-up)\nChain 4:                0.211 seconds (Sampling)\nChain 4:                0.347 seconds (Total)\nChain 4: \n\n\n\nShow/Hidebayesplot::mcmc_trace(bike_model, size = 0.1)\n\n\n\n\n\n\n\n\nShow/Hidebayesplot::mcmc_dens_overlay(bike_model)\n\n\n\n\n\n\n\n\nShow/Hidebayesplot::neff_ratio(bike_model)\n\n(Intercept)   temp_feel       sigma \n    0.99465     0.99180     0.95030 \n\n\n\nShow/Hidebayesplot::rhat(bike_model)\n\n(Intercept)   temp_feel       sigma \n  1.0000752   1.0000909   0.9999646 \n\n\n\nShow/Hidetidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 4 √ó 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -2191.     355.    -2653.    -1735. \n2 temp_feel       82.1      5.07     75.7      88.7\n3 sigma         1282.      41.1    1231.     1336. \n4 mean_PPD      3487.      80.3    3385.     3590."
  },
  {
    "objectID": "code/08-bayes-code.html#r-implementation",
    "href": "code/08-bayes-code.html#r-implementation",
    "title": "08- Bayesian Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hidelibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(janitor)\nlibrary(broom.mixed)\n\n\n\nShow/Hidedata(bikes)\n\n\n\nShow/Hidebike_model &lt;- rstanarm::stan_glm(rides ~ temp_feel, data = bikes,\n                                 family = gaussian,\n                                 prior_intercept = normal(5000, 1000),\n                                 prior = normal(100, 40), \n                                 prior_aux = exponential(0.0008),\n                                 chains = 4, iter = 5000*2, seed = 2025)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000667 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 6.67 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.187 seconds (Warm-up)\nChain 1:                0.201 seconds (Sampling)\nChain 1:                0.388 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.182 seconds (Warm-up)\nChain 2:                0.206 seconds (Sampling)\nChain 2:                0.388 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.109 seconds (Warm-up)\nChain 3:                0.213 seconds (Sampling)\nChain 3:                0.322 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.136 seconds (Warm-up)\nChain 4:                0.211 seconds (Sampling)\nChain 4:                0.347 seconds (Total)\nChain 4: \n\n\n\nShow/Hidebayesplot::mcmc_trace(bike_model, size = 0.1)\n\n\n\n\n\n\n\n\nShow/Hidebayesplot::mcmc_dens_overlay(bike_model)\n\n\n\n\n\n\n\n\nShow/Hidebayesplot::neff_ratio(bike_model)\n\n(Intercept)   temp_feel       sigma \n    0.99465     0.99180     0.95030 \n\n\n\nShow/Hidebayesplot::rhat(bike_model)\n\n(Intercept)   temp_feel       sigma \n  1.0000752   1.0000909   0.9999646 \n\n\n\nShow/Hidetidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 4 √ó 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -2191.     355.    -2653.    -1735. \n2 temp_feel       82.1      5.07     75.7      88.7\n3 sigma         1282.      41.1    1231.     1336. \n4 mean_PPD      3487.      80.3    3385.     3590."
  },
  {
    "objectID": "code/08-bayes-code.html#python-implementation",
    "href": "code/08-bayes-code.html#python-implementation",
    "title": "08- Bayesian Linear Regression Code Demo",
    "section": "Python implementation",
    "text": "Python implementation\n\nCheck PyMC demo\nError occurs when rendering the quarto file. But it can be run in a Python script.\n\n\nShow/Hideimport pymc as pm\nimport numpy as np\nimport pandas as pd\nimport arviz as az\nimport xarray as xr\n\n\n\nShow/Hide# Load your dataset\nbikes = pd.read_csv(\"../data/bikes.csv\")\n# Assuming `rides` is the target variable and `temp_feel` is the predictor\nrides = bikes[\"rides\"].values\ntemp_feel = bikes[\"temp_feel\"].values\n\n\n\n\n\n\n\n\n\n\nShow/Hideimport pymc as pm\n# Define the Bayesian model\nwith pm.Model() as bike_model:\n    # Priors\n    intercept = pm.Normal(\"intercept\", mu=5000, sigma=1000)\n    slope = pm.Normal(\"slope\", mu=100, sigma=40)\n    # sigma = pm.Exponential(\"sigma\", lam=0.0008)\n    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n    # Linear model\n    mu = intercept + slope * temp_feel\n\n    # Likelihood\n    likelihood = pm.Normal(\"rides\", mu=mu, sigma=sigma, observed=rides)\n\n    # Sampling\n    draws = pm.sample(5000, tune=5000, chains=4, random_seed=2025)\n\n\n# pm.summary(draws)\n\n\n\nShow/Hidepm.summary(draws)\n\n\n\nShow/Hideaz.plot_trace(draws)\n\n\n\nShow/Hidetrace.posterior[\"y_model\"] = trace.posterior[\"Intercept\"] + trace.posterior[\"x\"] * xr.DataArray(x)\n\n\n\nShow/Hide_, ax = plt.subplots(figsize=(7, 7))\naz.plot_lm(trace=trace, y=\"y\", num_samples=100, axes=ax, y_model=\"y_model\")\nax.set_title(\"Posterior predictive regression lines\")\nax.set_xlabel(\"x\");"
  },
  {
    "objectID": "code/11-knn-code.html",
    "href": "code/11-knn-code.html",
    "title": "11 - K Nearest Neighbors Code Demo",
    "section": "",
    "text": "Codeload(\"../data/ESL.mixture.rda\", verbose = TRUE)\n\nLoading objects:\n  ESL.mixture\n\nCodex &lt;- ESL.mixture$x\ny &lt;- ESL.mixture$y\nlibrary(class)\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nCodeknn_fit &lt;- class::knn(train = x, test = x, cl = y, k = 15)\ncaret::confusionMatrix(table(knn_fit, y))\n\nConfusion Matrix and Statistics\n\n       y\nknn_fit  0  1\n      0 82 13\n      1 18 87\n                                          \n               Accuracy : 0.845           \n                 95% CI : (0.7873, 0.8922)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.69            \n                                          \n Mcnemar's Test P-Value : 0.4725          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8700          \n         Pos Pred Value : 0.8632          \n         Neg Pred Value : 0.8286          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4750          \n      Balanced Accuracy : 0.8450          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nCodeset.seed(2025)\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\nknn_cvfit &lt;- train(y ~ ., method = \"knn\", \n                   data = data.frame(\"x\" = x, \"y\" = as.factor(y)),\n                   tuneGrid = data.frame(k = seq(1, 40, 1)),\n                   trControl = control)\npar(mar = c(4, 4, 0, 0))\nplot(knn_cvfit$results$k, 1 - knn_cvfit$results$Accuracy,\n     xlab = \"K\", ylab = \"Classification Error\", type = \"b\",\n     pch = 19, col = 2)\n\n\n\n\n\n\n\n\nCodezip.train &lt;- read.csv(\"../data/zip.train.csv\")\nzip.test &lt;- read.csv(\"../data/zip.test.csv\")\n# fit 3nn model and calculate the error\nknn.fit &lt;- class::knn(zip.train[, 2:257], zip.test[, 2:257], zip.train[, 1], k = 3)\n# overall prediction error\nmean(knn.fit != zip.test[, 1])\n\n[1] 0.05530643\n\nCode# the confusion matrix\ntable(knn.fit, zip.test[, 1], dnn = c(\"pred\", \"true\"))\n\n    true\npred   0   1   2   3   4   5   6   7   8   9\n   0 355   0   6   2   0   4   3   0   4   1\n   1   0 257   0   0   2   0   0   1   0   0\n   2   2   0 184   2   0   3   1   1   2   0\n   3   0   0   2 153   0   3   0   1   5   0\n   4   0   3   1   0 182   0   2   4   0   3\n   5   0   0   0   6   2 145   1   0   1   1\n   6   0   2   0   0   2   0 163   0   0   0\n   7   1   2   2   1   2   0   0 138   1   4\n   8   0   0   3   0   1   1   0   1 151   0\n   9   1   0   0   2   9   4   0   1   2 168"
  },
  {
    "objectID": "code/11-knn-code.html#r-implementation",
    "href": "code/11-knn-code.html#r-implementation",
    "title": "11 - K Nearest Neighbors Code Demo",
    "section": "",
    "text": "Codeload(\"../data/ESL.mixture.rda\", verbose = TRUE)\n\nLoading objects:\n  ESL.mixture\n\nCodex &lt;- ESL.mixture$x\ny &lt;- ESL.mixture$y\nlibrary(class)\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nCodeknn_fit &lt;- class::knn(train = x, test = x, cl = y, k = 15)\ncaret::confusionMatrix(table(knn_fit, y))\n\nConfusion Matrix and Statistics\n\n       y\nknn_fit  0  1\n      0 82 13\n      1 18 87\n                                          \n               Accuracy : 0.845           \n                 95% CI : (0.7873, 0.8922)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.69            \n                                          \n Mcnemar's Test P-Value : 0.4725          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8700          \n         Pos Pred Value : 0.8632          \n         Neg Pred Value : 0.8286          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4750          \n      Balanced Accuracy : 0.8450          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nCodeset.seed(2025)\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\nknn_cvfit &lt;- train(y ~ ., method = \"knn\", \n                   data = data.frame(\"x\" = x, \"y\" = as.factor(y)),\n                   tuneGrid = data.frame(k = seq(1, 40, 1)),\n                   trControl = control)\npar(mar = c(4, 4, 0, 0))\nplot(knn_cvfit$results$k, 1 - knn_cvfit$results$Accuracy,\n     xlab = \"K\", ylab = \"Classification Error\", type = \"b\",\n     pch = 19, col = 2)\n\n\n\n\n\n\n\n\nCodezip.train &lt;- read.csv(\"../data/zip.train.csv\")\nzip.test &lt;- read.csv(\"../data/zip.test.csv\")\n# fit 3nn model and calculate the error\nknn.fit &lt;- class::knn(zip.train[, 2:257], zip.test[, 2:257], zip.train[, 1], k = 3)\n# overall prediction error\nmean(knn.fit != zip.test[, 1])\n\n[1] 0.05530643\n\nCode# the confusion matrix\ntable(knn.fit, zip.test[, 1], dnn = c(\"pred\", \"true\"))\n\n    true\npred   0   1   2   3   4   5   6   7   8   9\n   0 355   0   6   2   0   4   3   0   4   1\n   1   0 257   0   0   2   0   0   1   0   0\n   2   2   0 184   2   0   3   1   1   2   0\n   3   0   0   2 153   0   3   0   1   5   0\n   4   0   3   1   0 182   0   2   4   0   3\n   5   0   0   0   6   2 145   1   0   1   1\n   6   0   2   0   0   2   0 163   0   0   0\n   7   1   2   2   1   2   0   0 138   1   4\n   8   0   0   3   0   1   1   0   1 151   0\n   9   1   0   0   2   9   4   0   1   2 168"
  },
  {
    "objectID": "code/11-knn-code.html#python-implementation",
    "href": "code/11-knn-code.html#python-implementation",
    "title": "11 - K Nearest Neighbors Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport rdata\n\n\n\nCode# Load the .rda file into a dictionary\nmixture_example = rdata.read_rda('../data/ESL.mixture.rda')\n\n/Users/chenghanyu/.virtualenvs/r-reticulate/lib/python3.12/site-packages/rdata/conversion/_conversion.py:856: UserWarning: Missing constructor for R class \"matrix\". The underlying R object is returned instead.\n  warnings.warn(\n\nCodex = mixture_example['ESL.mixture']['x']\ny = mixture_example['ESL.mixture']['y']\n\n\n\nCodeknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(x, y)\n\n\n\n\nKNeighborsClassifier(n_neighbors=15)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=15) \n\n\nCodepred = knn.predict(x)\npd.DataFrame(confusion_matrix(y, pred), \n             index=[f\"Actual {int(i)}\" for i in np.unique(y)], \n             columns=[f\"Pred {int(i)}\" for i in np.unique(y)])\n\n          Pred 0  Pred 1\nActual 0      82      18\nActual 1      13      87\n\n\n\nCode# Prepare train/test split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n                                                    random_state=2025)\n\n# Perform 10-fold cross-validation for different k values\nk_values = range(1, 41)\ncv_errors = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Use negative accuracy for error rate\n    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_errors.append(1 - np.mean(scores))  # Classification error = 1 - accuracy\n\n# Plot classification error vs. k\nplt.figure()\nplt.plot(k_values, cv_errors, marker='o', linestyle='-', color='red')\nplt.xlabel('K')\nplt.ylabel('Classification Error')\nplt.title('K vs. Classification Error')\nplt.show()\n\n\n\n\n\n\n\n\nCodezip_train = pd.read_csv(\"../data/zip.train.csv\",).to_numpy()\nzip_test = pd.read_csv(\"../data/zip.test.csv\",).to_numpy()\nx_train = zip_train[:, 1:257]\ny_train = zip_train[:, 0]\nx_test = zip_test[:, 1:257]\ny_test = zip_test[:, 0]\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train, y_train)\n\n\n\n\nKNeighborsClassifier(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\nCodeknn_pred = knn.predict(x_test)\n\nnp.mean(knn_pred != y_test)\n\n0.05530642750373692\n\nCodepd.DataFrame(confusion_matrix(y_test, knn_pred), \n             index=[f\"Actual {int(i)}\" for i in np.unique(y_test)], \n             columns=[f\"Pred {int(i)}\" for i in np.unique(y_test)])\n\n          Pred 0  Pred 1  Pred 2  Pred 3  ...  Pred 6  Pred 7  Pred 8  Pred 9\nActual 0     355       0       3       0  ...       0       0       0       1\nActual 1       0     258       0       0  ...       2       1       0       0\nActual 2       8       0     183       1  ...       0       2       3       0\nActual 3       3       0       2     153  ...       0       1       0       1\nActual 4       0       2       0       0  ...       2       2       1       8\nActual 5       5       0       3       3  ...       0       0       1       4\nActual 6       3       1       1       0  ...     163       0       0       0\nActual 7       0       1       1       1  ...       0     138       1       1\nActual 8       4       0       3       4  ...       0       1     151       2\nActual 9       2       0       0       0  ...       0       4       0     168\n\n[10 rows x 10 columns]"
  },
  {
    "objectID": "code/05-ridge-cv-code.html",
    "href": "code/05-ridge-cv-code.html",
    "title": "05-Ridge Regression Code Demo",
    "section": "",
    "text": "Codelibrary(MASS)\nfit &lt;- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1) ## ridge fit\ncoef(fit) ## original scale\n\n                      cyl         disp           hp         drat           wt \n16.537660830 -0.162402755  0.002333078 -0.014934856  0.924631319 -2.461146015 \n        qsec           vs           am         gear         carb \n 0.492587517  0.374651744  2.308375781  0.685715851 -0.575791252 \n\nCodefit$coef ## standardized scale\n\n       cyl       disp         hp       drat         wt       qsec         vs \n-0.2854708  0.2846046 -1.0078499  0.4865947 -2.3702010  0.8663632  0.1858566 \n        am       gear       carb \n 1.1337179  0.4979561 -0.9153711 \n\n\n\nCodeX &lt;- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)\n\n\n\nCodedf &lt;- data.frame(cbind(mtcars[, 1, drop=FALSE], X))\nridge_fit &lt;- lm.ridge(mpg ~ ., data = df, lambda = 0:40)\nMASS::select(ridge_fit)\n\nmodified HKB estimator is 2.58585 \nmodified L-W estimator is 1.837435 \nsmallest value of GCV  at 15 \n\n\n\nCodepar(mar = c(4, 4, 0, 0))\nplot(ridge_fit$lambda, ridge_fit$GCV, type = \"l\", col = \"darkgreen\", \n     ylab = \"GCV\", xlab = \"Lambda\", lwd = 3)\n\n\n\n\n\n\n\n\nBy defualt, glmnet() standardizes the x variables with standardize = TRUE, and does not standardize the response (standardize.response = FALSE).\nglmnet() always return the coefficients at the original scale.\n\n\nCodelibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nCoderidge_cv_fit &lt;- cv.glmnet(x = X, y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nCodeplot(ridge_cv_fit)\n\n\n\n\n\n\n\n\nCoderidge_cv_fit$lambda.min\n\n[1] 3.014598\n\nCode# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit$lambda.1se \n\n[1] 11.08883\n\nCodecoef(ridge_cv_fit, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) 20.0906250\ncyl         -0.6681360\ndisp        -0.6591218\nhp          -0.7889394\ndrat         0.5644227\nwt          -1.1786358\nqsec         0.2866108\nvs           0.3966957\nam           0.7941621\ngear         0.3997316\ncarb        -0.8635288\n\n\n\n\nCodelibrary(glmnet)\nridge_cv_fit_ori &lt;- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit_ori$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nCodeplot(ridge_cv_fit_ori)\n\n\n\n\n\n\n\n\nCoderidge_cv_fit_ori$lambda.min\n\n[1] 3.014598\n\nCode# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit_ori$lambda.1se \n\n[1] 12.16998\n\nCodecoef(ridge_cv_fit_ori, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 21.051283516\ncyl         -0.374112703\ndisp        -0.005318127\nhp          -0.011506803\ndrat         1.055629523\nwt          -1.204585685\nqsec         0.160391657\nvs           0.787069385\nam           1.591536197\ngear         0.541785546\ncarb        -0.534626533"
  },
  {
    "objectID": "code/05-ridge-cv-code.html#r-implementation",
    "href": "code/05-ridge-cv-code.html#r-implementation",
    "title": "05-Ridge Regression Code Demo",
    "section": "",
    "text": "Codelibrary(MASS)\nfit &lt;- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1) ## ridge fit\ncoef(fit) ## original scale\n\n                      cyl         disp           hp         drat           wt \n16.537660830 -0.162402755  0.002333078 -0.014934856  0.924631319 -2.461146015 \n        qsec           vs           am         gear         carb \n 0.492587517  0.374651744  2.308375781  0.685715851 -0.575791252 \n\nCodefit$coef ## standardized scale\n\n       cyl       disp         hp       drat         wt       qsec         vs \n-0.2854708  0.2846046 -1.0078499  0.4865947 -2.3702010  0.8663632  0.1858566 \n        am       gear       carb \n 1.1337179  0.4979561 -0.9153711 \n\n\n\nCodeX &lt;- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)\n\n\n\nCodedf &lt;- data.frame(cbind(mtcars[, 1, drop=FALSE], X))\nridge_fit &lt;- lm.ridge(mpg ~ ., data = df, lambda = 0:40)\nMASS::select(ridge_fit)\n\nmodified HKB estimator is 2.58585 \nmodified L-W estimator is 1.837435 \nsmallest value of GCV  at 15 \n\n\n\nCodepar(mar = c(4, 4, 0, 0))\nplot(ridge_fit$lambda, ridge_fit$GCV, type = \"l\", col = \"darkgreen\", \n     ylab = \"GCV\", xlab = \"Lambda\", lwd = 3)\n\n\n\n\n\n\n\n\nBy defualt, glmnet() standardizes the x variables with standardize = TRUE, and does not standardize the response (standardize.response = FALSE).\nglmnet() always return the coefficients at the original scale.\n\n\nCodelibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nCoderidge_cv_fit &lt;- cv.glmnet(x = X, y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nCodeplot(ridge_cv_fit)\n\n\n\n\n\n\n\n\nCoderidge_cv_fit$lambda.min\n\n[1] 3.014598\n\nCode# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit$lambda.1se \n\n[1] 11.08883\n\nCodecoef(ridge_cv_fit, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) 20.0906250\ncyl         -0.6681360\ndisp        -0.6591218\nhp          -0.7889394\ndrat         0.5644227\nwt          -1.1786358\nqsec         0.2866108\nvs           0.3966957\nam           0.7941621\ngear         0.3997316\ncarb        -0.8635288\n\n\n\n\nCodelibrary(glmnet)\nridge_cv_fit_ori &lt;- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit_ori$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nCodeplot(ridge_cv_fit_ori)\n\n\n\n\n\n\n\n\nCoderidge_cv_fit_ori$lambda.min\n\n[1] 3.014598\n\nCode# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit_ori$lambda.1se \n\n[1] 12.16998\n\nCodecoef(ridge_cv_fit_ori, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 21.051283516\ncyl         -0.374112703\ndisp        -0.005318127\nhp          -0.011506803\ndrat         1.055629523\nwt          -1.204585685\nqsec         0.160391657\nvs           0.787069385\nam           1.591536197\ngear         0.541785546\ncarb        -0.534626533"
  },
  {
    "objectID": "code/05-ridge-cv-code.html#python-implementation",
    "href": "code/05-ridge-cv-code.html#python-implementation",
    "title": "05-Ridge Regression Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nCodemtcars = pd.read_csv(\"../data/mtcars.csv\")\nX = mtcars.drop(columns=[\"mpg\"])\ny = mtcars[\"mpg\"]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nridge_model = Ridge(alpha=1) ## alpha is the n*lambda\nridge_model.fit(X_scaled, y)\n\n\n\n\nRidge(alpha=1)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRidgeRidge(alpha=1)\n\n\n\nCoderidge_model.coef_\n\narray([-0.28547077,  0.28460463, -1.00784994,  0.4865947 , -2.37020101,\n        0.86636324,  0.18585663,  1.13371791,  0.49795614, -0.91537115])\n\n\n\nCodelambdas = np.arange(1, 41) ## alpha must be &gt; 0\n# Ridge regression with cross-validation to select the best lambda\n# Enable storing CV values (can only use LOOCV)\nridge_cv = RidgeCV(alphas=lambdas, store_cv_values=True)  \nridge_cv.fit(X_scaled, y)\n\n\n\n\nRidgeCV(alphas=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40]),\n        store_cv_values=True)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRidgeCVRidgeCV(alphas=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40]),\n        store_cv_values=True)\n\n\n\nCode# Optimal lambda and corresponding coefficients\noptimal_lambda = ridge_cv.alpha_\noptimal_lambda\n\n13\n\nCodeoptimal_coefficients = ridge_cv.coef_\noptimal_coefficients\n\narray([-0.64745384, -0.62930084, -0.79208923,  0.55274563, -1.2273356 ,\n        0.2909736 ,  0.37094879,  0.81866969,  0.3981575 , -0.89842163])\n\nCoderidge_cv.intercept_\n\n20.090625000000003\n\nCode# Cross-validation mean squared error for each lambda\ncv_mse = np.mean(ridge_cv.cv_values_, axis=0)\n\n# Plot the CV MSE vs Lambda\nplt.plot(lambdas, cv_mse, marker=\"o\", linestyle=\"-\")\nplt.axvline(optimal_lambda, color=\"red\", linestyle=\"--\", \n            label=f\"Optimal Lambda = {optimal_lambda}\")\nplt.xlabel(\"Lambda (Alpha)\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"Cross-Validation MSE vs Lambda\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nHere we transform the coefficients back to the original scale when non-standardized X is used.\n\nCode# Reverse standardization\nstd_devs = scaler.scale_  # Feature standard deviations\nmeans = scaler.mean_      # Feature means\n\ncoef_original = optimal_coefficients / std_devs\ncoef_original\n\narray([-0.36833293, -0.00515876, -0.0117376 ,  1.05033188, -1.27442867,\n        0.16543865,  0.74776248,  1.66690255,  0.54828706, -0.56512958])\n\nCodeintercept_original = ridge_cv.intercept_ - np.sum(optimal_coefficients * means / std_devs)\nintercept_original\n\n21.21467495074396\n\n\n\nCode## the lambda size is matching the size used in cv.glmnet(). Should it be multiplied by 32?\nlambdas = 5 * 10 ** np.linspace(-1, 3, 100)\n\n# RidgeCV with 10-fold cross-validation\nridge_cv = RidgeCV(alphas=lambdas, scoring=\"neg_mean_squared_error\", cv=10)\nridge_cv.fit(X_scaled, y)\n\n\n\n\nRidgeCV(alphas=array([5.00000000e-01, 5.48749383e-01, 6.02251770e-01, 6.60970574e-01,\n       7.25414389e-01, 7.96141397e-01, 8.73764200e-01, 9.58955131e-01,\n       1.05245207e+00, 1.15506485e+00, 1.26768225e+00, 1.39127970e+00,\n       1.52692775e+00, 1.67580133e+00, 1.83918989e+00, 2.01850863e+00,\n       2.21531073e+00, 2.43130079e+00, 2.66834962e+00, 2.92851041e+00,\n       3.21403656e+00, 3.52740116e+0...\n       5.88405976e+02, 6.45774833e+02, 7.08737081e+02, 7.77838072e+02,\n       8.53676324e+02, 9.36908711e+02, 1.02825615e+03, 1.12850986e+03,\n       1.23853818e+03, 1.35929412e+03, 1.49182362e+03, 1.63727458e+03,\n       1.79690683e+03, 1.97210303e+03, 2.16438064e+03, 2.37540508e+03,\n       2.60700414e+03, 2.86118383e+03, 3.14014572e+03, 3.44630605e+03,\n       3.78231664e+03, 4.15108784e+03, 4.55581378e+03, 5.00000000e+03]),\n        cv=10, scoring='neg_mean_squared_error')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRidgeCVRidgeCV(alphas=array([5.00000000e-01, 5.48749383e-01, 6.02251770e-01, 6.60970574e-01,\n       7.25414389e-01, 7.96141397e-01, 8.73764200e-01, 9.58955131e-01,\n       1.05245207e+00, 1.15506485e+00, 1.26768225e+00, 1.39127970e+00,\n       1.52692775e+00, 1.67580133e+00, 1.83918989e+00, 2.01850863e+00,\n       2.21531073e+00, 2.43130079e+00, 2.66834962e+00, 2.92851041e+00,\n       3.21403656e+00, 3.52740116e+0...\n       5.88405976e+02, 6.45774833e+02, 7.08737081e+02, 7.77838072e+02,\n       8.53676324e+02, 9.36908711e+02, 1.02825615e+03, 1.12850986e+03,\n       1.23853818e+03, 1.35929412e+03, 1.49182362e+03, 1.63727458e+03,\n       1.79690683e+03, 1.97210303e+03, 2.16438064e+03, 2.37540508e+03,\n       2.60700414e+03, 2.86118383e+03, 3.14014572e+03, 3.44630605e+03,\n       3.78231664e+03, 4.15108784e+03, 4.55581378e+03, 5.00000000e+03]),\n        cv=10, scoring='neg_mean_squared_error')\n\n\n\nCodebest_lambda = ridge_cv.alpha_\n\ncoefficients = []\nfor lam in lambdas:\n    ridge = Ridge(alpha=lam)\n    ridge.fit(X_scaled, y)\n    coefficients.append(ridge.coef_)\n\n\n\n\nRidge(alpha=5000.0)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRidgeRidge(alpha=5000.0)\n\n\n\nCodecoefficients = np.array(coefficients)\nfor i in range(coefficients.shape[1]):\n    plt.plot(np.log(lambdas), coefficients[:, i])\nplt.xlabel(\"Log(Lambda)\")\nplt.ylabel(\"Coefficients\")\nplt.show()"
  },
  {
    "objectID": "code/07-splines-code.html",
    "href": "code/07-splines-code.html",
    "title": "07- Splines and Generalized Addive Models Code Demo",
    "section": "",
    "text": "Show/Hidebirthrates &lt;- read.csv(\"../data/birthrates.csv\")\n\n\n\n\nShow/Hidelmfit3 &lt;- lm(Birthrate ~ poly(Year-mean(Year), degree = 3), data = birthrates)\nplot(birthrates, pch = 19, col = 4, main = \"degree = 3\")\nlines(birthrates$Year, lmfit3$fitted.values, lty = 1, col = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\n\nShow/Hidelibrary(splines)\n\n\nFor linear splines, change degree = 3 to degree = 1.\n\nShow/Hidecub_sp &lt;- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = c(1936, 1960, 1978)), \n             data = birthrates)\nplot(birthrates, pch = 19, col = 4, main = \"Cubic spline (k = 3) with 3 knots\")\nlines(birthrates$Year, cub_sp$fitted.values, lty = 1, col = 2, lwd = 3)\n\n\n\n\n\n\n\n\n\nShow/Hidefit &lt;- smooth.spline(birthrates$Year, birthrates$Birthrate, df = 15)\nplot(birthrates$Year, birthrates$Birthrate, pch = 19, \n     xlab = \"Year\", ylab = \"BirthRates\", col = 4)\nlines(seq(1917, 2003), predict(fit, seq(1917, 2003))$y, col = 2, lty = 1, lwd = 3)\n\n\n\n\n\n\n\n\n\nShow/Hidelibrary(gam)\n\nLoading required package: foreach\n\n\nLoaded gam 1.22-4\n\n\n\nShow/HideWage &lt;- read.csv(\"../data/Wage.csv\")\nWage$education &lt;- as.factor(Wage$education)\ngam.m3 &lt;- gam(wage ~ s(year, df = 4) + s(age , df = 5) + education, data = Wage)\npar(mfrow = c(1, 3), mar = c(4, 4, 2, 0))\nplot.Gam(gam.m3, se = TRUE, col = 4, lwd = 2, las = 1)"
  },
  {
    "objectID": "code/07-splines-code.html#r-implementation",
    "href": "code/07-splines-code.html#r-implementation",
    "title": "07- Splines and Generalized Addive Models Code Demo",
    "section": "",
    "text": "Show/Hidebirthrates &lt;- read.csv(\"../data/birthrates.csv\")\n\n\n\n\nShow/Hidelmfit3 &lt;- lm(Birthrate ~ poly(Year-mean(Year), degree = 3), data = birthrates)\nplot(birthrates, pch = 19, col = 4, main = \"degree = 3\")\nlines(birthrates$Year, lmfit3$fitted.values, lty = 1, col = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\n\nShow/Hidelibrary(splines)\n\n\nFor linear splines, change degree = 3 to degree = 1.\n\nShow/Hidecub_sp &lt;- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = c(1936, 1960, 1978)), \n             data = birthrates)\nplot(birthrates, pch = 19, col = 4, main = \"Cubic spline (k = 3) with 3 knots\")\nlines(birthrates$Year, cub_sp$fitted.values, lty = 1, col = 2, lwd = 3)\n\n\n\n\n\n\n\n\n\nShow/Hidefit &lt;- smooth.spline(birthrates$Year, birthrates$Birthrate, df = 15)\nplot(birthrates$Year, birthrates$Birthrate, pch = 19, \n     xlab = \"Year\", ylab = \"BirthRates\", col = 4)\nlines(seq(1917, 2003), predict(fit, seq(1917, 2003))$y, col = 2, lty = 1, lwd = 3)\n\n\n\n\n\n\n\n\n\nShow/Hidelibrary(gam)\n\nLoading required package: foreach\n\n\nLoaded gam 1.22-4\n\n\n\nShow/HideWage &lt;- read.csv(\"../data/Wage.csv\")\nWage$education &lt;- as.factor(Wage$education)\ngam.m3 &lt;- gam(wage ~ s(year, df = 4) + s(age , df = 5) + education, data = Wage)\npar(mfrow = c(1, 3), mar = c(4, 4, 2, 0))\nplot.Gam(gam.m3, se = TRUE, col = 4, lwd = 2, las = 1)"
  },
  {
    "objectID": "code/07-splines-code.html#python-implementation",
    "href": "code/07-splines-code.html#python-implementation",
    "title": "07- Splines and Generalized Addive Models Code Demo",
    "section": "Python implementation",
    "text": "Python implementation\n\nShow/Hideimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nShow/Hidebirthrates = pd.read_csv(\"../data/birthrates.csv\")\n\n\nPolynomial regression\n\nShow/Hidefrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\nShow/Hidebirthrates['Year_centered'] = birthrates['Year'] - birthrates['Year'].mean()\n\n# Polynomial regression with degree = 3\npoly = PolynomialFeatures(degree=3, include_bias=False)\nX_poly = poly.fit_transform(birthrates[['Year_centered']])\n\npolyfit3 = LinearRegression().fit(X_poly, birthrates['Birthrate'])\n\nplt.scatter(birthrates['Year'], birthrates['Birthrate'], color='blue')\nplt.plot(birthrates['Year'], polyfit3.predict(X_poly), color='red',\n         linewidth=2)\nplt.title(\"Cubic Polynomial Regression (Degree = 3)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCubic Splines\n\nShow/Hidefrom patsy import dmatrix\nfrom sklearn.linear_model import LinearRegression\n\n\nFor linear splines, change degree=3 to degree=1.\nNOTE:\n\nCan‚Äôt find prediction or extrapolation for cubic splines.\nUse patsy.cr() to fit natural cubic splines.\n\n\nShow/Hideknots = [1936, 1960, 1978]\n# https://patsy.readthedocs.io/en/latest/API-reference.html\n# Generate cubic spline basis functions with specified knots\nspline_basis = dmatrix(\n    \"bs(Year, degree=3, knots=knots, include_intercept=True)\", \n    {\"Year\": birthrates[\"Year\"]}, \n    return_type=\"dataframe\"\n)\n# Fit the cubic spline model\n# import statsmodels.api as sm\n# model = sm.OLS(birthrates[\"Birthrate\"], spline_basis).fit()\n# birthrates[\"Fitted\"] = model.fittedvalues\ncub_sp = LinearRegression().fit(spline_basis, birthrates[\"Birthrate\"])\n\n# Plot the data and the fitted spline\nplt.scatter(birthrates[\"Year\"], birthrates[\"Birthrate\"], color=\"blue\")\nplt.plot(birthrates[\"Year\"], cub_sp.predict(spline_basis), color=\"red\",\n         linewidth=2)\nplt.title(\"Cubic Spline (d=3) with 3 Knots\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\n\nShow/Hide# https://patsy.readthedocs.io/en/latest/API-reference.html\n# Generate cubic spline basis functions with specified knots\nspline_basis = dmatrix(\n    \"bs(Year, degree=3, df=7, include_intercept=True)\", \n    {\"Year\": birthrates[\"Year\"]}, \n    return_type=\"dataframe\"\n)\n# Fit the cubic spline model\n# import statsmodels.api as sm\n# model = sm.OLS(birthrates[\"Birthrate\"], spline_basis).fit()\n# birthrates[\"Fitted\"] = model.fittedvalues\ncub_sp = LinearRegression().fit(spline_basis, birthrates[\"Birthrate\"])\n\n# Plot the data and the fitted spline\nplt.scatter(birthrates[\"Year\"], birthrates[\"Birthrate\"], color=\"blue\")\nplt.plot(birthrates[\"Year\"], cub_sp.predict(spline_basis), color=\"red\",\n         linewidth=2)\nplt.title(\"Cubic Spline (df=6)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\nSmoothing Splines\nWe use scipy.interpolate.make_smoothing_spline.\n\nPython has no functions for smoothing splines that can directly specify the degrees of freedom. Please let me know if you find one.\nTo have similar smoothing results, R and Python would use quite a different size of penalty term \\(\\lambda\\), as well as the degrees of freedom and smoothing factor.\n\n\nShow/Hidefrom scipy.interpolate import make_smoothing_spline\n\n\n\nShow/Hidex = birthrates[\"Year\"].values\ny = birthrates[\"Birthrate\"].values\nspline = make_smoothing_spline(x, y, lam=20)\n# Predict for the range of years\nx_pred = np.linspace(1917, 2003, 500)\ny_pred = spline(x_pred)\n# Plot the original data\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, color='blue', label='Data', s=50)\nplt.plot(x_pred, y_pred, color='red', linewidth=3, label='Smoothing Spline')\nplt.title(\"Smoothing Spline\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\nTo use a smoothing factor, use splrep (make_splrep) and BSpline\nThe smoothing factor is set unresonably high to 4500. Please let me know if you figure out why.\n\nShow/Hidefrom scipy.interpolate import splrep, BSpline\n\n\n\nShow/Hidex = birthrates[\"Year\"].values\ny = birthrates[\"Birthrate\"].values\n\n# Fit the smoothing spline with a smoothing factor\nsmoothing_factor = 4500 # Adjust this for the desired smoothness\ntck = splrep(x, y, s=smoothing_factor)\n\n# Predict for a range of years\nx_pred = np.linspace(1917, 2003, 500)\ny_pred = BSpline(*tck)(x_pred)\n\n# Plot the original data\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, color='blue', label='Data', s=50)\n\n# Plot the fitted smoothing spline\nplt.plot(x_pred, y_pred, color='red', linewidth=3, label='Smoothing Spline')\n\n# Add labels and title\nplt.title(\"Smoothing Spline with splrep\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\nGAM\nhttps://kirenz.github.io/regression/docs/gam.html\nhttps://gist.github.com/josef-pkt/453de603b019143e831fbdd4dfb6aa30\n\nShow/Hidefrom statsmodels.gam.api import BSplines\nfrom statsmodels.gam.api import GLMGam\nfrom statsmodels.tools.eval_measures import mse, rmse\nimport statsmodels.api as sm\nimport patsy\n\n\n\nShow/HideWage = pd.read_csv(\"../data/Wage.csv\")\nWage['education'] = pd.Categorical(Wage['education'], categories=['1. &lt; HS Grad', '2. HS Grad', '3. Some College', '4. College Grad', '5. Advanced Degree'], ordered=True)\n\n# penalization weights are taken from mgcv to match up its results\n# sp = np.array([0.830689464223685, 425.361212061649])\n# s_scale = np.array([2.443955e-06, 0.007945455])\nx_spline = Wage[['year', 'age']].values\nexog = patsy.dmatrix('education', data=Wage)\n\n# TODO: set `include_intercept=True` automatically if constraints='center'\nbs = BSplines(x_spline, df=[4, 5], degree=[3, 3], variable_names=['year', 'age'], \n              constraints='center', include_intercept=True)\n# alpha = 1 / s_scale * sp / 2\ngam_bs = GLMGam(Wage['wage'], exog=exog, smoother=bs)\nres = gam_bs.fit()\n\n\n\nShow/Hidefig, axes = plt.subplots(1, 2, figsize=(12, 6))\nres.plot_partial(0, cpr=False, include_constant=False, ax=axes[0])\naxes[0].set_title(\"Year\")\nres.plot_partial(1, cpr=False, include_constant=False, ax=axes[1])\naxes[1].set_title(\"Age\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nShow/Hideprint(res.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                   wage   No. Observations:                 3000\nModel:                         GLMGam   Df Residuals:                     2988\nModel Family:                Gaussian   Df Model:                        11.00\nLink Function:               Identity   Scale:                          1238.8\nMethod:                         PIRLS   Log-Likelihood:                -14934.\nDate:                Mon, 10 Feb 2025   Deviance:                   3.7014e+06\nTime:                        21:43:15   Pearson chi2:                 3.70e+06\nNo. Iterations:                     3   Pseudo R-squ. (CS):             0.3358\nCovariance Type:            nonrobust                                         \n===================================================================================================\n                                      coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nIntercept                          85.6860      2.156     39.745      0.000      81.461      89.911\neducation[T.2. HS Grad]            10.7413      2.431      4.418      0.000       5.977      15.506\neducation[T.3. Some College]       23.2067      2.563      9.056      0.000      18.184      28.229\neducation[T.4. College Grad]       37.8704      2.547     14.871      0.000      32.879      42.862\neducation[T.5. Advanced Degree]    62.4355      2.764     22.591      0.000      57.019      67.852\nyear_s0                             3.3874      4.257      0.796      0.426      -4.957      11.732\nyear_s1                             1.8170      4.220      0.431      0.667      -6.454      10.088\nyear_s2                             4.4943      1.754      2.563      0.010       1.057       7.931\nage_s0                             10.1360      5.932      1.709      0.087      -1.490      21.762\nage_s1                             47.6380      5.326      8.945      0.000      37.200      58.076\nage_s2                              6.7739      7.296      0.928      0.353      -7.526      21.074\nage_s3                            -10.0472     10.672     -0.941      0.346     -30.963      10.869\n===================================================================================================\n\n\nOne option is to use pygam package.\n\nShow/Hidefrom pygam import LinearGAM, s, f\nfrom pygam.datasets import wage\n\n\n\nShow/HideX, y = wage(return_X_y=True)\n\n## model\ngam = LinearGAM(s(0) + s(1) + f(2)).fit(X, y)\n\nfor i, term in enumerate(gam.terms):\n    if term.isintercept:\n        continue\n\n    XX = gam.generate_X_grid(term=i)\n    pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)\n\n    plt.figure()\n    plt.plot(XX[:, term.feature], pdep)\n    plt.plot(XX[:, term.feature], confi, c='r', ls='--')\n    plt.title(repr(term))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow/Hidegam.summary()\n\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     25.1911\nLink Function:                     IdentityLink Log Likelihood:                                -24118.6847\nNumber of Samples:                         3000 AIC:                                            48289.7516\n                                                AICc:                                           48290.2307\n                                                GCV:                                             1255.6902\n                                                Scale:                                           1236.7251\n                                                Pseudo R-Squared:                                   0.2955\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P &gt; x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           7.1          5.95e-03     **          \ns(1)                              [0.6]                20           14.1         1.11e-16     ***         \nf(2)                              [0.6]                5            4.0          1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\n&lt;string&gt;:3: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n \nPlease do not make inferences based on these values! \n\nCollaborate on a solution, and stay up to date at: \ngithub.com/dswah/pyGAM/issues/163"
  },
  {
    "objectID": "code/04-linear-reg-code.html",
    "href": "code/04-linear-reg-code.html",
    "title": "04-Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hideadvertising_data &lt;- read.csv(\"../data/Advertising.csv\")\nadvertising_data &lt;- advertising_data[, 2:5]\nhead(advertising_data)\n\n     TV radio newspaper sales\n1 230.1  37.8      69.2  22.1\n2  44.5  39.3      45.1  10.4\n3  17.2  45.9      69.3   9.3\n4 151.5  41.3      58.5  18.5\n5 180.8  10.8      58.4  12.9\n6   8.7  48.9      75.0   7.2\n\nShow/Hidelm_out &lt;- lm(advertising_data$sales ~ ., data = advertising_data)\nsummary(lm_out)\n\n\nCall:\nlm(formula = advertising_data$sales ~ ., data = advertising_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nShow/Hideconfint(lm_out)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "code/04-linear-reg-code.html#r-implementation",
    "href": "code/04-linear-reg-code.html#r-implementation",
    "title": "04-Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hideadvertising_data &lt;- read.csv(\"../data/Advertising.csv\")\nadvertising_data &lt;- advertising_data[, 2:5]\nhead(advertising_data)\n\n     TV radio newspaper sales\n1 230.1  37.8      69.2  22.1\n2  44.5  39.3      45.1  10.4\n3  17.2  45.9      69.3   9.3\n4 151.5  41.3      58.5  18.5\n5 180.8  10.8      58.4  12.9\n6   8.7  48.9      75.0   7.2\n\nShow/Hidelm_out &lt;- lm(advertising_data$sales ~ ., data = advertising_data)\nsummary(lm_out)\n\n\nCall:\nlm(formula = advertising_data$sales ~ ., data = advertising_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nShow/Hideconfint(lm_out)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "code/04-linear-reg-code.html#python-implementation",
    "href": "code/04-linear-reg-code.html#python-implementation",
    "title": "04-Linear Regression Code Demo",
    "section": "Python implementation",
    "text": "Python implementation\n\nShow/Hideimport pandas as pd\nimport numpy as np\n\n\n\nShow/Hideadvertising_data = pd.read_csv(\"../data/Advertising.csv\")\nadvertising_data = advertising_data.iloc[:, 1:5]\nX = advertising_data.drop(columns=[\"sales\"])\ny = advertising_data[\"sales\"]\n\n\nscikit-learn\n\nShow/Hidefrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nreg.intercept_\n\n2.9388893694594014\n\nShow/Hidereg.coef_\n\narray([ 0.04576465,  0.18853002, -0.00103749])\n\n\nstatsmodels\n\nShow/Hidefrom statsmodels.formula.api import ols\nols_out = ols(formula='sales ~ TV + radio + newspaper', data=advertising_data).fit()\nols_out.params\n\nIntercept    2.938889\nTV           0.045765\nradio        0.188530\nnewspaper   -0.001037\ndtype: float64\n\nShow/Hideprint(ols_out.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Mon, 13 Jan 2025   Prob (F-statistic):           1.58e-96\nTime:                        11:56:31   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nradio          0.1885      0.009     21.893      0.000       0.172       0.206\nnewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nShow/Hidecoef_summary = ols_out.summary2().tables[1]  # Get the coefficients table\nprint(coef_summary)\n\n              Coef.  Std.Err.          t         P&gt;|t|    [0.025    0.975]\nIntercept  2.938889  0.311908   9.422288  1.267295e-17  2.323762  3.554016\nTV         0.045765  0.001395  32.808624  1.509960e-81  0.043014  0.048516\nradio      0.188530  0.008611  21.893496  1.505339e-54  0.171547  0.205513\nnewspaper -0.001037  0.005871  -0.176715  8.599151e-01 -0.012616  0.010541\n\nShow/Hideconf_intervals = ols_out.conf_int()\nprint(conf_intervals)\n\n                  0         1\nIntercept  2.323762  3.554016\nTV         0.043014  0.048516\nradio      0.171547  0.205513\nnewspaper -0.012616  0.010541"
  },
  {
    "objectID": "code/12-gp-code.html",
    "href": "code/12-gp-code.html",
    "title": "12 - Gaussian Process Regression Code Demo",
    "section": "",
    "text": "Codelibrary(plgp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: tgp\n\n\n\nCodelen &lt;- 100\nn_path &lt;- 5\nX &lt;- seq(0, 5, length = len)\nD &lt;- distance(X)\neps &lt;- sqrt(.Machine$double.eps) \nSigma &lt;- exp(-D/2) + diag(eps, len) \ny &lt;- mvnfast::rmvn(n_path, mu = rep(0, len), sigma = Sigma)\nmatplot(X, t(y), type = \"l\", main = \"Prior\", lwd = 2,\n        ylim = c(-3, 3), las = 1, lty = 1, xlab = \"x\", ylab = \"f(x)\")\n\n\n\n\n\n\nCoden &lt;- 5\nX &lt;- matrix(seq(0, 2*pi, length=n), ncol=1)\ny &lt;- sin(X)\nD &lt;- distance(X) \nSigma &lt;- exp(-D/2) + diag(eps, ncol(D))\nXX &lt;- matrix(seq(-0.5, 2*pi + 0.5, length=100), ncol=1)\nDXX &lt;- distance(XX)\nSXX &lt;- exp(-DXX/2) + diag(eps, ncol(DXX))\nDX &lt;- distance(XX, X)\nSX &lt;- exp(-DX/2) \nSi &lt;- solve(Sigma)\nmup &lt;- SX %*% Si %*% y\nSigmap &lt;- SXX - SX %*% Si %*% t(SX)\nYY &lt;- rmvnorm(100, mup, Sigmap)\nq1 &lt;- mup + qnorm(0.025, 0, sqrt(diag(Sigmap)))\nq2 &lt;- mup + qnorm(0.975, 0, sqrt(diag(Sigmap)))\nmatplot(XX, t(YY)[, 1:5], type=\"l\", col=1:5, lty=1, xlab=\"x\", ylab=\"f(x)\",\n        las = 1, main = \"Posterior\", lwd = 2)\npoints(X, y, pch=20, cex=2)\n\n\n\n\n\n\nCodematplot(XX, t(YY), type=\"l\", col=c(rep(\"grey\", nrow(XX))), \n        lty=1, xlab=\"x\", ylab=\"f(x)\", las = 1, main = \"Prediction with Uncertainty\")\npoints(X, y, pch=20, cex=2)\nlines(XX, t(YY)[, 1], col=3, lty = 1)\nlines(XX, mup, lwd=2, col = \"blue\", lty=2)\nlines(XX, q1, lwd=2, lty=2, col=2)\nlines(XX, q2, lwd=2, lty=2, col=2)"
  },
  {
    "objectID": "code/12-gp-code.html#r-implementation",
    "href": "code/12-gp-code.html#r-implementation",
    "title": "12 - Gaussian Process Regression Code Demo",
    "section": "",
    "text": "Codelibrary(plgp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: tgp\n\n\n\nCodelen &lt;- 100\nn_path &lt;- 5\nX &lt;- seq(0, 5, length = len)\nD &lt;- distance(X)\neps &lt;- sqrt(.Machine$double.eps) \nSigma &lt;- exp(-D/2) + diag(eps, len) \ny &lt;- mvnfast::rmvn(n_path, mu = rep(0, len), sigma = Sigma)\nmatplot(X, t(y), type = \"l\", main = \"Prior\", lwd = 2,\n        ylim = c(-3, 3), las = 1, lty = 1, xlab = \"x\", ylab = \"f(x)\")\n\n\n\n\n\n\nCoden &lt;- 5\nX &lt;- matrix(seq(0, 2*pi, length=n), ncol=1)\ny &lt;- sin(X)\nD &lt;- distance(X) \nSigma &lt;- exp(-D/2) + diag(eps, ncol(D))\nXX &lt;- matrix(seq(-0.5, 2*pi + 0.5, length=100), ncol=1)\nDXX &lt;- distance(XX)\nSXX &lt;- exp(-DXX/2) + diag(eps, ncol(DXX))\nDX &lt;- distance(XX, X)\nSX &lt;- exp(-DX/2) \nSi &lt;- solve(Sigma)\nmup &lt;- SX %*% Si %*% y\nSigmap &lt;- SXX - SX %*% Si %*% t(SX)\nYY &lt;- rmvnorm(100, mup, Sigmap)\nq1 &lt;- mup + qnorm(0.025, 0, sqrt(diag(Sigmap)))\nq2 &lt;- mup + qnorm(0.975, 0, sqrt(diag(Sigmap)))\nmatplot(XX, t(YY)[, 1:5], type=\"l\", col=1:5, lty=1, xlab=\"x\", ylab=\"f(x)\",\n        las = 1, main = \"Posterior\", lwd = 2)\npoints(X, y, pch=20, cex=2)\n\n\n\n\n\n\nCodematplot(XX, t(YY), type=\"l\", col=c(rep(\"grey\", nrow(XX))), \n        lty=1, xlab=\"x\", ylab=\"f(x)\", las = 1, main = \"Prediction with Uncertainty\")\npoints(X, y, pch=20, cex=2)\nlines(XX, t(YY)[, 1], col=3, lty = 1)\nlines(XX, mup, lwd=2, col = \"blue\", lty=2)\nlines(XX, q1, lwd=2, lty=2, col=2)\nlines(XX, q2, lwd=2, lty=2, col=2)"
  },
  {
    "objectID": "code/12-gp-code.html#python-implementation",
    "href": "code/12-gp-code.html#python-implementation",
    "title": "12 - Gaussian Process Regression Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\n\n\nCodedef plot_gpr_samples(gpr_model, n_samples, ax):\n    \"\"\"Plot samples drawn from the Gaussian process model.\n\n    If the Gaussian process model is not trained then the drawn samples are\n    drawn from the prior distribution. Otherwise, the samples are drawn from\n    the posterior distribution. Be aware that a sample here corresponds to a\n    function.\n\n    Parameters\n    ----------\n    gpr_model : `GaussianProcessRegressor`\n        A :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model.\n    n_samples : int\n        The number of samples to draw from the Gaussian process distribution.\n    ax : matplotlib axis\n        The matplotlib axis where to plot the samples.\n    \"\"\"\n    x = np.linspace(0, 2 * np.pi, 100)\n    X = x.reshape(-1, 1)\n\n    y_mean, y_std = gpr_model.predict(X, return_std=True)\n    y_samples = gpr_model.sample_y(X, n_samples)\n\n    for idx, single_prior in enumerate(y_samples.T):\n        ax.plot(\n            x,\n            single_prior,\n            linestyle=\"--\",\n            alpha=0.7,\n            label=f\"Sampled function #{idx + 1}\",\n        )\n    ax.plot(x, y_mean, color=\"black\", label=\"Mean\")\n    ax.fill_between(\n        x,\n        y_mean - y_std,\n        y_mean + y_std,\n        alpha=0.1,\n        color=\"black\",\n        label=r\"$\\pm$ 1 std. dev.\",\n    )\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_ylim([-3, 3])\n\n\n\nCoderng = np.random.RandomState(4)\n\nX_train = np.linspace(0, 2 * np.pi, 5).reshape(-1, 1)\ny_train = np.sin(X_train)\n\nn_samples = 5\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\nkernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\ngpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n\nfig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\n\n\n\n\nGaussianProcessRegressor(kernel=1**2 * RBF(length_scale=1), random_state=0)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n¬†¬†GaussianProcessRegressor?Documentation for GaussianProcessRegressoriFittedGaussianProcessRegressor(kernel=1**2 * RBF(length_scale=1), random_state=0) \n\n\nCodeplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Radial Basis Function kernel\", fontsize=18)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slides/01-syllabus.html",
    "href": "slides/01-syllabus.html",
    "title": "Welcome Aboard üôå",
    "section": "",
    "text": "When you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "slides/01-syllabus.html#code",
    "href": "slides/01-syllabus.html#code",
    "title": "Welcome Aboard üôå",
    "section": "",
    "text": "When you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "slides/01-syllabus.html#my-journey",
    "href": "slides/01-syllabus.html#my-journey",
    "title": "Welcome Aboard üôå",
    "section": "My Journey",
    "text": "My Journey\n\nAssistant Professor (2020/08 - )\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Fellow\n\n\n\n\n\n\n\n\n\n\nPhD in Statistics\n\n\n\n\n\n\n\n\n\n\nMA in Economics/PhD program in Statistics"
  },
  {
    "objectID": "slides/01-syllabus.html#how-to-reach-me",
    "href": "slides/01-syllabus.html#how-to-reach-me",
    "title": "Welcome Aboard üôå",
    "section": "How to Reach Me",
    "text": "How to Reach Me\n\nOffice hours TuTh 4:50 - 5:50 PM and Wed 12 - 1 PM in Cudahy Hall 353.\nüìß cheng-han.yu@marquette.edu\n\nAnswer your question within 24 hours.\nExpect a reply on Monday if shoot me a message on weekends.\nStart your subject line with [math4720] or [mssc5720] followed by a clear description of your question.\n\n\n\n\n\n\n\n\n\n\n\n\nI will NOT reply your e-mail if ‚Ä¶ Check the email policy in the syllabus!"
  },
  {
    "objectID": "slides/01-syllabus.html#ta-information",
    "href": "slides/01-syllabus.html#ta-information",
    "title": "Welcome Aboard üôå",
    "section": "TA Information",
    "text": "TA Information\n\n\n\nStatistics PhD student Qishi Zhan\nüì® qishi.zhan@marquette.edu\nHelp desk hours: To be announced.\nWelcome to set up a meeting with your TA via Teams.\nLet me know if you need any other help! üòÑ"
  },
  {
    "objectID": "slides/01-syllabus.html#course-materials",
    "href": "slides/01-syllabus.html#course-materials",
    "title": "Welcome Aboard üôå",
    "section": "Course Materials",
    "text": "Course Materials\n\nCourse Website - https://math4720-f25.github.io/website/"
  },
  {
    "objectID": "slides/01-syllabus.html#textbook-dr.-yus-online-book",
    "href": "slides/01-syllabus.html#textbook-dr.-yus-online-book",
    "title": "Welcome Aboard üôå",
    "section": "Textbook (Dr.¬†Yu‚Äôs Online Book)",
    "text": "Textbook (Dr.¬†Yu‚Äôs Online Book)\n\nIntroduction to Statistics, by Cheng-Han Yu\nFull detailed explanation of course slides üòé"
  },
  {
    "objectID": "slides/01-syllabus.html#learning-management-system-d2l",
    "href": "slides/01-syllabus.html#learning-management-system-d2l",
    "title": "Welcome Aboard üôå",
    "section": "Learning Management System (D2L)",
    "text": "Learning Management System (D2L)\n\n\n\n\n\n\n\n\n\nAssessments &gt; Grades"
  },
  {
    "objectID": "slides/01-syllabus.html#grading-policy",
    "href": "slides/01-syllabus.html#grading-policy",
    "title": "Welcome Aboard üôå",
    "section": "Grading Policy ‚ú®",
    "text": "Grading Policy ‚ú®\n\nYour final grade is earned out of 1000 total points distributed as follows:\n\nHomework 1 to 8: 200 pts (25 pts each)\nQuiz 1 to 4: 240 pts (60 pts each)\nExam 1 and 2: 320 pts (160 pts each)\nFinal exam: 240 pts\n\nClass participation for extra points!\n\n\n‚ùå No extra credit projects/homework/exam to compensate for a poor grade.\nIndividual grade will NOT be curved.\nThis is not a course that gives most of students grade A. Wanna get a good grade? Study hard. No pain, no gain! ‚úç ‚úç"
  },
  {
    "objectID": "slides/01-syllabus.html#grade-percentage-conversion",
    "href": "slides/01-syllabus.html#grade-percentage-conversion",
    "title": "Welcome Aboard üôå",
    "section": "Grade-Percentage Conversion ‚ú®",
    "text": "Grade-Percentage Conversion ‚ú®\n\nYour final grade is based on your percentage of points earned out of 1000 points.\n\n\\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\).\n\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)"
  },
  {
    "objectID": "slides/01-syllabus.html#homework-200-pts",
    "href": "slides/01-syllabus.html#homework-200-pts",
    "title": "Welcome Aboard üôå",
    "section": "Homework (200 pts)",
    "text": "Homework (200 pts)\n\n\nAssessments &gt; Dropbox and upload your homework in PDF format.\nThere are 8 homework sets.\nYou must submit YOUR OWN work. üôè\n‚ùå No make-up homework.\nEvery homework is due by Friday 11:59 PM  (Don‚Äôt miss it. This is a hard deadline‚ùó). \n\nThe lowest score of the homework sets will not be in your final grade calculation."
  },
  {
    "objectID": "slides/01-syllabus.html#quizzes-240-pts",
    "href": "slides/01-syllabus.html#quizzes-240-pts",
    "title": "Welcome Aboard üôå",
    "section": "Quizzes (240 pts)",
    "text": "Quizzes (240 pts)\n\nThere are 4 in-class quizzes. \n\nüìö Quizzes are individual and in closed-book format.\n‚ùå No cheat sheet is allowed.\n‚ùå No make-up quizzes for any reason unless you got COVID or excused absence. Check the syllabus for more details."
  },
  {
    "objectID": "slides/01-syllabus.html#exams-560-pts",
    "href": "slides/01-syllabus.html#exams-560-pts",
    "title": "Welcome Aboard üôå",
    "section": "Exams (560 pts)",
    "text": "Exams (560 pts)\n\nThere are two midterm exams and one final exam.\nHave in-class and take-home parts. \n\nüìÑ One piece of letter size cheat sheet is allowed. It has to be turned-in with your in-class exam.\n\nAssessments &gt; Dropbox to submit your take-home exams in PDF format. \n\nExam 1 covers Week 1 to 6\n\nExam 2 covers Week 7 to 11\n\nFinal exam is comprehensive and covers the all course materials. üòé\n‚ùå No make-up exams unless you got COVID or excused absence. Check the syllabus for more details."
  },
  {
    "objectID": "slides/01-syllabus.html#what-computing-language-we-use-i-teach",
    "href": "slides/01-syllabus.html#what-computing-language-we-use-i-teach",
    "title": "Welcome Aboard üôå",
    "section": "What Computing Language We Use (I Teach)?",
    "text": "What Computing Language We Use (I Teach)?\n\n\n\nüìà The best language for statistical computing! \n\n‚úÖ You may use other tools or software such as Excel, Python, Minitab, etc to do your work, but I will NOT teach any of them.\n‚ùå Drop/Swap deadline: 09/05/2022. Don‚Äôt miss it!\n\n\n\n\n\n\nhttps://www.r-project.org/"
  },
  {
    "objectID": "slides/01-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "href": "slides/01-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "title": "Welcome Aboard üôå",
    "section": "Generative AI and Sharing/Reusing Code Policy",
    "text": "Generative AI and Sharing/Reusing Code Policy\n\n\nGenerative AI\n\nYou may use generative AI tools such as ChatGPT or DALL-E to generate a first draft of text for your work, provided that this use is documented and cited.\n\n[Example] Data science is an interdisciplinary field that ‚Ä¶ 1\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\nSharing/Reusing Code\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\n\n[Example]\n\nThe code is modified from the GitHub repo https://github.com/chenghanyustats/slam\nThe code is generated from ChatGPT response to ‚ÄúPlease generate Python code for solving the math problem I attach,‚Äù Jan 14, 2025.\n\n\n\nYou are responsible for the content of all work submitted for this course.\nYou can use any code shared online or in books. But please give the authors full credits. Cite their work and let me know whose code you are using to do your homework and project.\nI encourage you to write your own code. This way you learn the most.\n‚ùå Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source. üò±"
  },
  {
    "objectID": "slides/01-syllabus.html#academic-integrity",
    "href": "slides/01-syllabus.html#academic-integrity",
    "title": "Welcome Aboard üôå",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThis course expects all students to follow University and College statements on academic integrity.\n\n\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code."
  },
  {
    "objectID": "slides/01-syllabus.html#footnotes",
    "href": "slides/01-syllabus.html#footnotes",
    "title": "Welcome Aboard üôå",
    "section": "Footnotes",
    "text": "Footnotes\n\nChatGPT, response to ‚ÄúTell me what data science is,‚Äù Jan 14, 2025, https://chat.openai.com.‚Ü©Ô∏é"
  },
  {
    "objectID": "documents/project_slides/group5_Stats ML Project 1 Visualization.html",
    "href": "documents/project_slides/group5_Stats ML Project 1 Visualization.html",
    "title": "MATH 4720 (MSSC 5720) - Fall 2025",
    "section": "",
    "text": "import pandas as pd\n# Load Dataset\ndata = pd.read_csv(\"train.csv\")\n\n# Import necessary libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport numpy as np\n\n# Set plot style\nsns.set_style(\"whitegrid\")\n\n# 1. Distribution of SalePrice (Histogram + KDE)\nplt.figure(figsize=(10, 5))\nsns.histplot(data['SalePrice'], kde=True, bins=30)\nplt.title(\"Distribution of SalePrice\")\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# 2. Boxplot for detecting outliers in GrLivArea\nplt.figure(figsize=(10, 5))\nsns.boxplot(y=data['GrLivArea'])\nplt.title(\"Boxplot of GrLivArea\")\nplt.ylabel(\"GrLivArea\")\nplt.show()\n\n# 3. Scatterplot of GrLivArea vs. SalePrice\nplt.figure(figsize=(10, 5))\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=data)\nplt.title(\"Scatterplot of GrLivArea vs. SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\n# 4. Boxplot for Neighborhood vs. SalePrice\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Neighborhood', y='SalePrice', data=data)\nplt.xticks(rotation=45)\nplt.title(\"SalePrice Distribution Across Neighborhoods\")\nplt.xlabel(\"Neighborhood\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\n# 5. Correlation Heatmap\n# Increase the figure size \nplt.figure(figsize=(20, 15))  \n\n# Generate the heatmap with larger font size for annotations\nsns.heatmap(data.select_dtypes(include=[np.number]).corr(), \n            cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5, \n            annot_kws={\"size\": 10})  # Increase annotation size\n\nplt.title(\"Correlation Heatmap\", fontsize=16)\nplt.xticks(fontsize=12, rotation=45)  # Rotate x-axis labels \nplt.yticks(fontsize=12)  # Adjust y-axis font size\n\nplt.show()\n\n\n# 6. Pairplot for Selected Features\nselected_features = ['SalePrice', 'TotalBsmtSF', 'GrLivArea', 'LotArea']\nsns.pairplot(data[selected_features])\nplt.show()\n\n# 7. Line Plot for SalePrice over Years Sold\nplt.figure(figsize=(10, 5))\nsns.lineplot(x='YrSold', y='SalePrice', data=data)\nplt.title(\"Average SalePrice Over Time\")\nplt.xlabel(\"Year Sold\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\n# 8. ANOVA Test (Checking if Neighborhood affects SalePrice)\nanova_result = stats.f_oneway(\n    data[data['Neighborhood'] == 'NAmes']['SalePrice'],\n    data[data['Neighborhood'] == 'CollgCr']['SalePrice'],\n    data[data['Neighborhood'] == 'OldTown']['SalePrice']\n)\n\n# 9. Chi-Square Test (Dependency between HouseStyle and SaleCondition)\nchi2_stat, p_value, _, _ = stats.chi2_contingency(pd.crosstab(data['HouseStyle'], data['SaleCondition']))\n\nanova_result, chi2_stat, p_value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(F_onewayResult(statistic=95.28410630774302, pvalue=1.2519558385662742e-35),\n 88.43116541471059,\n 1.626510734741343e-06)\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Select only numeric features\nnumeric_data = data.select_dtypes(include=['number'])\n\n# Compute correlation with SalePrice\ncorr_matrix = numeric_data.corr()['SalePrice'].sort_values(ascending=False)\n\n# Plot the top 10 correlated features\ntop_features = corr_matrix[1:11]  # Exclude SalePrice itself\n\nplt.figure(figsize=(10, 5))\ntop_features.plot(kind='bar', color='royalblue')\nplt.title(\"Top 10 Features Correlated with Sale Price\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Correlation with SalePrice\")\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "hw.html",
    "href": "hw.html",
    "title": "Homework",
    "section": "",
    "text": "More homework sets to be added as the semester progresses.\n\n\n\n\n\n\n\n\n\n\nNo.\n\n\n\nTitle\n\n\n\nDue date\n\n\n\n\n\n\n\n\nHomework 1\n\n\nBias-Variance Tradeoff and Linear Regression\n\n\nDue Friday, Feb 14, 11:59 PM\n\n\n\n\n\n\nHomework 2 - Ridge, Lasso, and Splines\n\n\nDue Friday, Feb 28, 11:59 PM\n\n\n\n\n\n\n\n\n\nHomework 3 - Bayesian Statistics, Logistic Regression, Generative Models, and K-Nearest Neighbors\n\n\nDue Friday, April 4, 11:59 PM\n\n\n\n\n\n\n\n\n\nHomework 4 - Support Vector Machines, Tree Methods, Unsupervised Learning\n\n\nDue Friday, May 2 11:59 PM\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download the syllabus.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#time-and-location",
    "href": "course-syllabus.html#time-and-location",
    "title": "Syllabus",
    "section": "Time and location",
    "text": "Time and location\n\n\n\nDay\nTime\nLocation\n\n\n\nLectures\nTu & Th\n3:30 - 4:45 PM\nCuday Hall 120\n\n\nLab\nNone\nNone\nNone",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#office-hours",
    "href": "course-syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\n\nMy in-person office hours are TuTh 4:50 - 5:50 PM, and Wed 12 - 1 PM in Cudahy Hall room 353.\nYou are welcome to schedule an online meeting via Microsoft Teams if you need/prefer.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nMATH 4720 (Intro to Statistics), MATH 3100 (Linear Algebra) and MATH 4780 (Regression Analysis)\nHaving taken MATH 4700 (Probability) and MATH 4710 (Statistical Inference) or more advanced ones is strongly recommended.\nThis course is supposed to be taken in the last semester for the applied statistics (APST) master students. Talk to me if you are not sure whether or not this is the right course for you.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#e-mail-policy",
    "href": "course-syllabus.html#e-mail-policy",
    "title": "Syllabus",
    "section": "E-mail Policy",
    "text": "E-mail Policy\n\nI will attempt to reply your email quickly, at least within 24 hours.\nExpect a reply on Monday if you send a question during weekends. If you do not receive a response from me within two days, re-send your question/comment in case there was a ‚Äúmix-up‚Äù with email communication (Hope this won‚Äôt happen!).\nPlease start your subject line with [mssc6250] followed by a clear description of your question. See an example below.\n\n\n\nEmail Subject Line Example\n\n\nEmail etiquette is important. Please read this article to learn more about email etiquette.\n\nI am more than happy to answer your questions about this course or data science/statistics in general. However, with tons of email messgaes everyday, I may choose NOT to respond to students‚Äô e-mail if\n\nThe student could answer his/her own inquiry by reading the syllabus or information on the course website or D2L.\nThe student is asking for an extra credit opportunity. The answer is ‚Äúno‚Äù.\nThe student is requesting an extension on homework. The answer is ‚Äúno‚Äù.\nThe student is asking for a grade to be raised for no legitimate reason. The answer is ‚Äúno‚Äù.\nThe student is sending an email with no etiquette.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#required-textbook",
    "href": "course-syllabus.html#required-textbook",
    "title": "Syllabus",
    "section": "Required Textbook",
    "text": "Required Textbook\n\n\n(ISL) An Introduction to Statistical Learning, by James et al.¬†Publisher: Springer. (Undergraduate to master level, R and Python code)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#optional-references",
    "href": "course-syllabus.html#optional-references",
    "title": "Syllabus",
    "section": "Optional References",
    "text": "Optional References\n\n(MML) Mathematics for Machine Learning, by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. Publisher: Cambridge University Press. (College level mathematics for machine learning)\n(PML) Probabilistic Machine Learning: An Introduction, by Kevin Murphy. Publisher: MIT Press. (Master to PhD level, lots of mathematics foundations, Python code)\n(PMLA) Probabilistic Machine Learning: Advanced Topics, by Kevin Murphy. Publisher: MIT Press. (PhD level, more probabilistic-based or Bayesian)\n(ESL) The Elements of Statistical Learning, 2nd edition, by Hastie et. al.¬†Publisher: Springer. (PhD level, more frequentist-based)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading-policy",
    "href": "course-syllabus.html#grading-policy",
    "title": "Syllabus",
    "section": "Grading Policy",
    "text": "Grading Policy\n\n\nYour final grade is earned out of 1000 total points distributed as follows:\n\nHomework: 500 pts\nMidterm project presentation: 300 pts\nFinal project: 200 pts\n\n\n\nYou will NOT be allowed any extra credit projects/homework/exam to compensate for a poor average. Everyone must be given the same opportunity to do well in this class. Individual exam will NOT be curved. \nThe final grade is based on your percentage of points earned out of 1000 points and the grade-percentage conversion Table. \\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\). For example, 94.1 is in \\([93, 100]\\) and the grade is A and 92.8 is in \\([90, 94)\\) and the grade is A-.\n\n\n\n\nGrade-Percentage Conversion\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[70, 77)\n\n\nF\n[0, 70)\n\n\n\n\n\n\n\nYou may use your preferred programming language to do your homework and/or your project.\n\nHomework\n\n\nHomework will be assigned through the course website in weekly modules.\nTo submit your homework, please go to D2L &gt; Assessments &gt; Dropbox and upload your homework in PDF format.\nNo late or make-up homework for any reason.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#midterm-project-presentation",
    "href": "course-syllabus.html#midterm-project-presentation",
    "title": "Syllabus",
    "section": "Midterm Project Presentation",
    "text": "Midterm Project Presentation\n\nThere will be 2 in-class mini project presentations\nStudents will learn from each other by presenting and discussing the assigned topics.\nMore details about the mini project presentation will be released later.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#final-project",
    "href": "course-syllabus.html#final-project",
    "title": "Syllabus",
    "section": "Final Project",
    "text": "Final Project\n\nThe final project is submitted as a paper and some relevant work.\nThe project submission deadline is Thursday, 5/8, 10 AM.\nMore details about the final project will be released in April.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "href": "course-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "title": "Syllabus",
    "section": "Generative AI and Sharing/Reusing Code Policy",
    "text": "Generative AI and Sharing/Reusing Code Policy\nGenerative AI \n\n\nYou are responsible for the content of all work submitted for this course. You may use generative AI tools such as ChatGPT or DALL-E to generate a first draft of text for your assignments, provided that this use is appropriately documented and cited.\nRead the articles (MLA and APA) to learn how to cite and document the use of AI in your work. Learn more citing chatbots at https://libguides.marquette.edu/generative_technologies/citing\nSharing/Reusing Code\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#academic-integrity",
    "href": "course-syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\n\nThis course expects all students to follow University and College statements on academic integrity.\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#accommodation",
    "href": "course-syllabus.html#accommodation",
    "title": "Syllabus",
    "section": "Accommodation",
    "text": "Accommodation\nIf you need to request accommodations, or modify existing accommodations that address disability-related needs, please contact Disability Service.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\n\nJan 21: Last day to add/swap/drop\n\nMar 10-16: Spring break\n\nMar 11: Midterm grade submission\n\nApr 11: Withdrawal deadline\n\nApr 17 - Apr 20: Easter break\n\nMay 3: Last day of class\n\nMay 8: Final project submission\n\nMay 13: Final grade submission\n\nClick here for the full Marquette academic calendar.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/01-syllabus-slides.html#code",
    "href": "slides/01-syllabus-slides.html#code",
    "title": "Welcome Aboard üôå",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#my-journey",
    "href": "slides/01-syllabus-slides.html#my-journey",
    "title": "Welcome Aboard üôå",
    "section": "My Journey",
    "text": "My Journey\n\nAssistant Professor (2020/08 - )\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Fellow\n\n\n\n\n\n\n\n\n\n\nPhD in Statistics\n\n\n\n\n\n\n\n\n\n\nMA in Economics/PhD program in Statistics"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#how-to-reach-me",
    "href": "slides/01-syllabus-slides.html#how-to-reach-me",
    "title": "Welcome Aboard üôå",
    "section": "How to Reach Me",
    "text": "How to Reach Me\n\nOffice hours TuTh 4:50 - 5:50 PM and Wed 12 - 1 PM in Cudahy Hall 353.\nüìß cheng-han.yu@marquette.edu\n\nAnswer your question within 24 hours.\nExpect a reply on Monday if shoot me a message on weekends.\nStart your subject line with [math4720] or [mssc5720] followed by a clear description of your question.\n\n\n\n\n\nI will NOT reply your e-mail if ‚Ä¶ Check the email policy in the syllabus!"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#ta-information",
    "href": "slides/01-syllabus-slides.html#ta-information",
    "title": "Welcome Aboard üôå",
    "section": "TA Information",
    "text": "TA Information\n\n\n\nStatistics PhD student Qishi Zhan\nüì® qishi.zhan@marquette.edu\nHelp desk hours: To be announced.\nWelcome to set up a meeting with your TA via Teams.\nLet me know if you need any other help! üòÑ"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#course-materials",
    "href": "slides/01-syllabus-slides.html#course-materials",
    "title": "Welcome Aboard üôå",
    "section": "Course Materials",
    "text": "Course Materials\n\nCourse Website - https://math4720-f25.github.io/website/"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#textbook-dr.-yus-online-book",
    "href": "slides/01-syllabus-slides.html#textbook-dr.-yus-online-book",
    "title": "Welcome Aboard üôå",
    "section": "Textbook (Dr.¬†Yu‚Äôs Online Book)",
    "text": "Textbook (Dr.¬†Yu‚Äôs Online Book)\n\nIntroduction to Statistics, by Cheng-Han Yu\nFull detailed explanation of course slides üòé"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#learning-management-system-d2l",
    "href": "slides/01-syllabus-slides.html#learning-management-system-d2l",
    "title": "Welcome Aboard üôå",
    "section": "Learning Management System (D2L)",
    "text": "Learning Management System (D2L)\n\n\nAssessments &gt; Grades"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#grading-policy",
    "href": "slides/01-syllabus-slides.html#grading-policy",
    "title": "Welcome Aboard üôå",
    "section": "Grading Policy ‚ú®",
    "text": "Grading Policy ‚ú®\n\nYour final grade is earned out of 1000 total points distributed as follows:\n\nHomework 1 to 8: 200 pts (25 pts each)\nQuiz 1 to 4: 240 pts (60 pts each)\nExam 1 and 2: 320 pts (160 pts each)\nFinal exam: 240 pts\n\nClass participation for extra points!\n\n\n‚ùå No extra credit projects/homework/exam to compensate for a poor grade.\nIndividual grade will NOT be curved.\nThis is not a course that gives most of students grade A. Wanna get a good grade? Study hard. No pain, no gain! ‚úç ‚úç"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#grade-percentage-conversion",
    "href": "slides/01-syllabus-slides.html#grade-percentage-conversion",
    "title": "Welcome Aboard üôå",
    "section": "Grade-Percentage Conversion ‚ú®",
    "text": "Grade-Percentage Conversion ‚ú®\n\nYour final grade is based on your percentage of points earned out of 1000 points.\n\n\\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\).\n\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#homework-200-pts",
    "href": "slides/01-syllabus-slides.html#homework-200-pts",
    "title": "Welcome Aboard üôå",
    "section": "Homework (200 pts)",
    "text": "Homework (200 pts)\n\n\nAssessments &gt; Dropbox and upload your homework in PDF format.\nThere are 8 homework sets.\nYou must submit YOUR OWN work. üôè\n‚ùå No make-up homework.\nEvery homework is due by Friday 11:59 PM  (Don‚Äôt miss it. This is a hard deadline‚ùó). \n\nThe lowest score of the homework sets will not be in your final grade calculation."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#quizzes-240-pts",
    "href": "slides/01-syllabus-slides.html#quizzes-240-pts",
    "title": "Welcome Aboard üôå",
    "section": "Quizzes (240 pts)",
    "text": "Quizzes (240 pts)\n\nThere are 4 in-class quizzes. \n\nüìö Quizzes are individual and in closed-book format.\n‚ùå No cheat sheet is allowed.\n‚ùå No make-up quizzes for any reason unless you got COVID or excused absence. Check the syllabus for more details."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#exams-560-pts",
    "href": "slides/01-syllabus-slides.html#exams-560-pts",
    "title": "Welcome Aboard üôå",
    "section": "Exams (560 pts)",
    "text": "Exams (560 pts)\n\nThere are two midterm exams and one final exam.\nHave in-class and take-home parts. \n\nüìÑ One piece of letter size cheat sheet is allowed. It has to be turned-in with your in-class exam.\n\nAssessments &gt; Dropbox to submit your take-home exams in PDF format. \n\nExam 1 covers Week 1 to 6\n\nExam 2 covers Week 7 to 11\n\nFinal exam is comprehensive and covers the all course materials. üòé\n‚ùå No make-up exams unless you got COVID or excused absence. Check the syllabus for more details."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#what-computing-language-we-use-i-teach",
    "href": "slides/01-syllabus-slides.html#what-computing-language-we-use-i-teach",
    "title": "Welcome Aboard üôå",
    "section": "What Computing Language We Use (I Teach)?",
    "text": "What Computing Language We Use (I Teach)?\n\n\n\nüìà The best language for statistical computing! \n\n‚úÖ You may use other tools or software such as Excel, Python, Minitab, etc to do your work, but I will NOT teach any of them.\n‚ùå Drop/Swap deadline: 09/05/2022. Don‚Äôt miss it!\n\n\n\n\n\n\nhttps://www.r-project.org/"
  },
  {
    "objectID": "slides/01-syllabus-slides.html#generative-ai-and-sharingreusing-code-policy",
    "href": "slides/01-syllabus-slides.html#generative-ai-and-sharingreusing-code-policy",
    "title": "Welcome Aboard üôå",
    "section": "Generative AI and Sharing/Reusing Code Policy",
    "text": "Generative AI and Sharing/Reusing Code Policy\n\n\nGenerative AI\n\nYou may use generative AI tools such as ChatGPT or DALL-E to generate a first draft of text for your work, provided that this use is documented and cited.\n\n[Example] Data science is an interdisciplinary field that ‚Ä¶ 1\n\n\n\n\n\n\n\n\n\n\n\nSharing/Reusing Code\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\n\n[Example]\n\nThe code is modified from the GitHub repo https://github.com/chenghanyustats/slam\nThe code is generated from ChatGPT response to ‚ÄúPlease generate Python code for solving the math problem I attach,‚Äù Jan 14, 2025.\n\n\nYou are responsible for the content of all work submitted for this course.\nYou can use any code shared online or in books. But please give the authors full credits. Cite their work and let me know whose code you are using to do your homework and project.\nI encourage you to write your own code. This way you learn the most.\n‚ùå Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source. üò±\n\n\nChatGPT, response to ‚ÄúTell me what data science is,‚Äù Jan 14, 2025, https://chat.openai.com."
  },
  {
    "objectID": "slides/01-syllabus-slides.html#academic-integrity",
    "href": "slides/01-syllabus-slides.html#academic-integrity",
    "title": "Welcome Aboard üôå",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThis course expects all students to follow University and College statements on academic integrity.\n\n\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code."
  }
]